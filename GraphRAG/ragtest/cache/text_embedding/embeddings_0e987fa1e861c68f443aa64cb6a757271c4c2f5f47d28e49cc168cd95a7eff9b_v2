{"result": {"data": [{"embedding": [0.026691873, 0.059653953, -0.13772321, -0.07475118, 0.03605239, -0.03237253, -0.008058847, -0.0098220995, -0.0581971, -0.020549944, -0.01405414, 0.015851622, 0.07459248, 0.004533646, 0.027865387, 0.008972597, 0.025029588, -0.04248821, -0.017741501, 0.0038417976, -0.02017044, 0.012816575, 0.021290807, -0.055888228, 0.0121892365, 0.031051166, -0.06780086, -0.005253865, -0.012986668, -0.003978369, 0.064856894, -0.04849691, -0.02690307, -0.07867524, -0.065911435, -0.01168217, 0.062163252, -0.030853316, -0.013591579, 0.08120913, 0.060459968, 0.0140590435, -0.02283714, -0.041194, 0.032650504, -0.026545884, 0.0011881795, -0.014177999, 0.045054313, -0.03799655, -0.008306925, 0.009968061, 0.017638799, 0.04047563, 0.11464741, 0.072528824, -0.030668449, 0.036972955, 0.030580543, -0.042532638, 0.114909545, 0.010911446, -0.079438075, 0.036364075, 0.010464754, 0.004831269, -0.06992406, 0.06470766, 0.011984819, -0.037834506, 0.0019414612, -0.0097416695, -0.012446101, 0.040482197, -0.024662122, 0.0043325946, 0.019786702, -0.005466143, 0.012642624, 0.056246284, -0.0035316623, -0.000701631, 0.05146536, -0.049806368, 0.028759511, 0.025838448, -0.04117426, -0.014097726, -0.040251195, 0.113939375, 0.00971855, 0.05262532, 0.025325943, 0.0036998682, -0.064602084, 0.024386827, -0.04752816, -0.040002275, -0.027605673, -0.06784978, -0.024225628, -0.0004013609, 0.045576103, -0.013232946, 0.054850347, 0.027443219, 0.024279743, -0.019128062, -0.05554939, 0.0085648345, -0.03616548, 0.0215398, -0.0066423295, 0.0012325718, -0.05349717, -0.07552332, 0.07384391, 0.0016376231, 0.027550638, 0.06539978, 0.050207756, -0.047613204, 0.038964417, -0.024555089, 0.008970055, 0.04140932, -0.06538129, -0.019159092, -0.016805964, -0.057792712, 0.010071459, 0.03573824, -0.031992614, 0.010903365, 0.006074297, 0.11388514, -0.016972812, -0.007361508, 0.08653821, 0.0066314996, 0.04666117, -0.0076981806, -0.009405328, 0.007862219, 0.020105556, -0.0551205, -0.022231312, -0.018460844, -0.02986863, 0.020754516, 0.034765337, 0.041326344, -0.004853174, 0.017763494, 0.020763533, 0.019352144, 0.0074535054, -0.0019925304, -0.0064728227, -0.011201344, 0.036269255, 0.004229001, -0.05328369, 0.011984595, -0.011301015, -0.03212142, 0.021482827, 0.009011524, 0.032617748, 0.03800237, -0.088107556, -0.042490512, -0.019906402, 0.009929625, 0.036660768, 0.0063529066, 0.059895653, -0.043756917, 0.0066478085, -0.0015606916, 0.030493014, -0.07279589, 0.026560629, -0.021268344, -0.043448813, -0.021213943, -0.0059506223, 0.017369794, 0.01855992, -0.05059648, -0.048588492, 0.055110108, -0.03517772, -0.02802535, -0.025140181, -0.04676735, 0.041482896, -0.048968453, 0.01886532, -0.025894541, 0.002034309, -0.015866358, -0.04685662, 0.020013534, -0.035888556, 0.07801904, 0.018039143, 0.038221262, -0.046906754, 0.0065199444, 0.043287825, 0.008079806, 0.027538937, -0.022072919, 0.014713617, 0.004483983, -0.0052812938, 0.007924432, 0.002075482, 0.012237, 0.0312553, 0.008081187, 0.018717133, 0.018303737, -0.0005797497, 0.002979565, -0.030009985, -0.01157919, -0.066896334, 0.053903583, -0.029863711, -0.029944398, 0.07145945, 0.0007909321, 0.019726437, -0.018670542, 0.010248544, 0.05117356, -0.009774182, 0.043933306, 0.015639342, 0.0043702107, -0.036983617, 0.01203141, -0.08438979, -0.025000136, -0.01896744, -0.051766712, -0.009060768, 0.061757673, -0.010210635, -0.0049192514, 0.027787382, -0.016763633, 0.027812399, 0.0023847143, -0.049109552, 0.032980178, 0.03950216, -0.034723464, 0.026815848, 0.0058616945, -0.0005228716, -0.033736985, -0.0034500426, -0.050691687, 0.005383029, 0.008711549, 0.011345773, -0.010549355, 0.036406484, 0.039404705, 0.03961113, 0.0122142015, 0.010733636, 0.04291421, -0.05503426, -0.017765518, 0.008942978, 0.015822481, -0.023904052, -0.05081787, 0.002509641, -0.026791627, 0.005139087, 0.018702915, 0.036099594, -0.028237911, 0.028514687, -0.0058226744, 0.010307142, 0.024573427, -0.01359761, -0.032816544, -0.025252234, 0.024024704, 0.08208125, 0.013564786, 0.033222128, -0.035612382, 0.012853808, -0.006384495, 0.045510318, 0.052551, 0.010456383, 0.004509047, -0.013892192, 0.023585686, 0.054659676, -0.016556654, -0.046685163, -0.014945814, -0.022731762, -0.008017846, -0.04292117, 0.029714653, 0.042191464, 0.03671069, 0.045064636, -0.03181127, 0.001287881, -0.06480688, -0.0321985, -0.066581875, -0.009970034, 0.0042650253, 0.018972201, -0.0009780867, -0.06075507, -0.012172941, 0.062254056, 0.040446505, 0.043068472, 0.009862435, -0.005645857, -0.009743556, -0.031858806, -0.029422183, 0.039533693, -0.015459367, 0.033897523, -0.017658789, 0.050217096, 0.011211499, 0.011280072, 0.0060542547, -0.06478528, -0.040691428, 0.014644443, 0.0049352, -0.018442506, -0.011589381, -0.020826261, -0.018303063, 0.05124827, 0.0041041304, 0.049346697, 0.021861477, 0.0045633153, -0.012306402, 0.03731357, 0.012634825, 0.01270729, -0.006971253, 0.004780437, 0.013493533, 0.052360967, 0.032081626, 0.049535155, 0.006493215, 0.04289272, -0.004497864, -0.0189991, 0.056971442, -0.05644223, 0.015735654, -0.065631166, -0.012820161, -0.011291482, 0.00029744202, 0.0022340256, -0.033582408, 0.0040203677, -0.004347416, 0.013845169, 0.0039156587, 0.04568967, -0.027474707, 0.00317861, 0.022438047, 0.0011041911, -0.059256073, -0.053513363, 0.014005422, 0.017135834, 0.0021937373, 0.041112907, 0.02016076, -0.022581154, 0.012752005, -0.050163627, -0.059892632, -0.006261814, -0.032074768, -0.01681767, 0.0020503853, -0.03597773, -0.02820738, 0.015067003, 0.0035501625, 0.02352171, 0.06996354, -0.0068827197, -0.025481204, 0.020301709, 0.00091285014, 0.009649878, -0.008654593, -0.010917541, -0.010990956, 0.016625706, 0.024715915, 0.0015602758, -0.017949907, -0.005955254, -0.006917833, 0.04424129, 0.068959996, -0.005374927, -0.04070906, 0.04353472, 0.021464467, 0.04382322, -0.012645466, 0.014953341, 0.0042286078, -0.008238151, 0.0074961022, -0.0021730124, 0.016827902, 0.03287198, -0.060889624, 0.004563504, -0.021863468, 0.04318042, 0.062477607, 0.020121, 0.012441426, -0.0927925, 0.018793738, -0.022797517, 0.028757691, 0.022885889, -0.008031632, 0.09967423, -0.02942432, 0.05029617, -0.011405607, 0.046179324, 0.017871141, 0.056993034, 0.003571851, -0.07230263, -0.025665665, 0.00039989583, 0.01265089, -0.019758262, -0.023023333, 0.024066526, 0.005002744, -0.032208808, 0.02028329, 0.047879204, -0.0021561093, -0.02964984, -0.0052751238, -0.016929219, -0.021310924, 0.030184008, 0.015707254, 0.0013846149, -0.02902998, -0.035894345, -0.04750503, -0.032179117, 0.07094387, -0.0012745807, -0.026649427, -0.008746436, 0.026621813, 0.0634692, -0.0024019713, 0.0014403774, 0.019073322, -0.05676311, -0.036863077, -0.032702174, 0.04257455, 0.04433737, 0.028737644, 0.027743025, 0.07320571, -0.019453041, -0.031461418, -0.0126055945, -0.04249732, 0.009959957, -0.06485916, -0.08388503, 0.038390048, 0.022031043, -0.0009962417, -0.034063935, 0.030905247, 0.04928845, -0.07201179, 0.044169758, 0.020900564, -0.043954097, 0.037108243, 0.007974269, -0.014712717, 0.040213823, -0.03804152, -0.06269986, -0.01393466, 0.012959775, -0.047712546, 0.0025234767, -0.008235732, 0.01544605, 0.02523985, -0.01846257, -0.010255927, 0.011864518, -0.0023272976, 0.006715906, -0.0004568009, 0.029142156, 0.057907205, 0.023690298, 0.02187584, -0.0025403379, 0.0054962207, 0.025423454, 0.00391957, 0.0048845843, 0.021222347, -0.0014402433, -0.057165556, 0.00030046297, -0.07110977, 0.0059713833, -0.014074637, 0.04368798, -0.03761385, -0.0720618, -0.028526641, -0.012987661, -0.0036714862, -0.0055711917, 0.024100097, 0.091657825, 0.0063585797, -0.034725014, 0.0037221566, -0.03354592, -0.006753576, -0.0047404463, 0.078384735, 0.060226344, -0.06019107, -0.005753758, 0.040138923, 0.013357022, -0.050975192, 0.03085164, -0.04759667, -0.038839683, -0.06865362, 0.012934966, -0.009650682, -0.014713708, -0.00033817263, -0.014239226, 0.028185666, -0.07487788, -0.019809945, 0.010492346, -0.009325343, 0.012989424, -0.039086856, 0.0380025, -0.04321505, -0.0027637368, -0.018035887, -0.003469344, -0.022743158, -0.022846516, -0.068790235, -0.018624559, 0.025044613, 0.03713121, -0.062134746, 0.046217177, 0.050101526, -0.0333072, 0.07397853, 0.011306077, -0.002158975, -0.0015572615, 0.014210112, -0.022838784, 0.00279637, 0.021666596, -0.032601092, 0.084421255, -0.021637825, -0.073498555, 0.002483968, -0.023734827, -0.04185376, 0.028379953, -0.061150454, -0.028301263, -0.021445127, -0.034096878, -0.037113972, -0.003765995, 0.05186432, -0.04269513, 0.009206608, -0.006994418, -0.021970114, -0.049135555, 0.008145232, -0.05766709, -0.023127247, 0.0137373, 0.038183495, 0.033960596, -0.03393762, -0.011590274, 0.026844824, 0.045494992, -0.03201875, 0.028814463, 0.06223147, -0.0069948197, 0.014296076, 0.076290905, 0.091690905, 0.02034142, -0.004785335, -0.024116734, 0.015605689, 0.02219673, -0.02708128, 0.014399453, 0.025890741, 0.033845365, -0.011286822, -0.011410724, -0.025785068, 0.014129017, -0.03774635, -0.024267163, -0.0042205025, -0.04175569, -0.04224247, 0.033575464, -0.020207604, 0.009748101, -0.023199856, 0.012973842, 0.04585256, 0.008891352, 0.020945627, 0.0010366653, 0.008336647, -0.011188081, 0.0041170307, -0.016261416, -0.030809231, 0.01628687, -0.036612254, 0.026292339, -0.03258257, -0.058840085, -0.026181038, -0.0010782694, -0.05879729, -0.007358954, 0.024473377, 0.0014795698, 0.019699572, 0.017496957, 0.033255536, 0.018580854, 0.09138245, -0.016772728, 0.03275102, -0.007987701, 0.04228524, 0.017423574, -0.019480566, 0.010360555, 0.022266278, 0.018105293, -0.0603561, -0.056605034, 0.027194094, -0.01171029, 0.04576167, 0.002144596, 0.055019617, 0.01150269, -0.00726804, 0.029759625, 0.050103836, 0.051121473, -0.0056649353, -0.04461034, 0.01864687, -0.014919331, -0.025686683, 0.011388442, -0.033697482, -0.016801463, 0.013981073, 0.009725317, -0.07501625, -0.050911836, 0.021935556, -0.014284385, -0.0114689, -0.030753227, 0.014531803, -0.040873524, -0.011139362, -0.02985201, 0.009158057, 0.02043132, 0.021404168, -0.009523587, -0.024180157, -0.033923205, 0.047418583, 0.011057327, -0.052311838, -0.026025366, -0.023600647, -0.023206774, 0.019485235, -0.028127816, -0.013560133, -0.027302127, 0.046639133, 0.12441547, -0.04054052, 0.026915958, -0.077870555, 0.08361124, -0.0045965407, -0.032430902, 0.009460668, -0.07664196, -0.031083947], "index": 0, "object": "embedding"}, {"embedding": [0.019705135, 0.048633773, -0.1577462, -0.03333761, 0.058226697, -0.0075477418, 0.096308924, 0.009504269, -0.009238706, -0.039953884, -0.016872002, -0.0021120596, 0.10895875, 0.045374382, 0.0069489037, -1.5058857e-05, 0.00063070137, -0.06226845, -0.0021775865, 0.047318354, 0.043684717, -0.04927175, 0.042626478, -0.037264604, -0.024003146, 0.019566022, -0.04010454, -0.0014678001, 0.0032326833, 0.015107351, 0.0621945, -0.014397036, -0.022965342, -0.040725738, -0.052876838, -0.034765787, 0.010377619, 0.04997798, 0.029864378, 0.04981474, 0.035906687, 0.013328752, 0.0014547154, -0.019409942, 0.012461855, -0.035800498, 0.05296564, -0.02145601, 0.05085435, -0.07159163, 0.048555713, 0.020745201, -0.016100718, -0.015791768, 0.08953709, 0.039197817, -0.024433373, 0.05873326, 0.015256362, -0.08702554, 0.069390126, 0.07103143, -0.09544997, 0.03327763, -0.019298581, -0.02171712, -0.042718366, 0.037283145, -0.001271148, -0.041562155, -0.015144984, 0.014839948, -0.016384996, 0.012776407, -0.06995694, 0.0008031829, -0.038282536, -0.01275064, -0.032883372, -0.014949077, 0.032419086, 0.03591785, 0.023565125, -0.042360667, 0.031098329, 0.0195591, -0.04440231, 0.00676284, -0.044607397, 0.098340884, 0.0063615227, 0.019886412, 0.035041425, 0.040981688, -0.022310516, 0.010813127, -0.04062085, -0.004209302, 0.0076189036, -0.024850342, -0.012005289, -0.022800706, 0.0023015544, 0.0012977744, -0.009178159, 0.030283129, 0.014739666, 0.008240333, -0.01347732, 0.0027236533, -0.018296912, 0.041048776, -0.040069055, 0.021031864, -0.08547769, -0.0354415, 0.0574492, -0.015413554, 0.0070883636, 0.017934654, -0.038753998, -0.030884743, -0.019267298, -0.008763669, 0.012377208, 0.017164024, -0.03935534, 0.005899738, -0.05043476, -0.0012897537, 0.008554982, -0.04333829, -0.062690265, 0.030625194, 0.03075641, 0.08841343, -0.059774928, -0.00094294996, 0.039790433, 0.026942885, 0.032333188, 0.01233933, -0.019200886, -0.03673765, -0.014536581, -0.040265005, -0.025829934, 0.020131113, -0.03985003, 0.015766514, 0.017474746, 0.049514085, -0.0035284958, 0.030285401, 0.03700608, -0.011505978, -0.004225664, -0.021618076, 0.012982891, -0.031673882, 0.025716173, -0.017516933, -0.051584132, 0.06786968, -0.0055968612, 0.0036134229, 0.0020560645, -0.00032540943, -0.0074995714, 0.029672606, -0.0566365, -0.038484402, -0.0021337413, -0.008724971, 0.03646534, 0.016785244, 0.101234905, -0.039719604, 0.0014044001, -0.010021258, 0.01667248, -0.08141341, 0.0025091635, 0.029706279, -0.017423496, -0.010290044, -0.006361885, -0.012798152, 0.022366863, -0.040204085, 0.035621326, 0.037474383, -0.047412455, -0.02563275, -0.03382359, -0.04452052, 0.04963678, -0.037489437, 0.0010989582, 0.0033157293, -0.057921592, -0.03551941, -0.02856176, 0.02243592, -0.009349151, 0.07691177, 0.015334161, 0.039323263, 0.011837957, -0.006061345, 0.056799594, -0.0014418276, 0.062728755, 0.014229859, 0.016535845, -0.018448412, -0.0086549, 0.0024837817, -0.0025836711, -0.0027195138, 0.0043645883, -0.023596158, 0.015259269, 0.045552503, 0.0263168, 0.006390589, -0.013674158, 0.022270128, -0.03534431, 0.0136648, 0.014761617, -0.038977027, 0.043630783, -0.012281073, -0.0048069386, 0.017893145, -0.010592005, 0.029195443, 0.004531742, 0.029673109, 0.014594524, 0.024864806, -0.02301544, -0.06121289, -0.049358446, -0.044989634, 0.00029045992, -0.02864362, -0.04490399, 0.017346898, -0.02897697, 0.039027426, 0.054112587, -0.015292331, 0.035363387, -0.038375344, -0.009175517, 0.022199005, 0.051606763, 0.034496125, -0.028785102, -0.013907138, -0.013210041, -0.0068616956, -0.018390607, -0.009466577, -0.014767416, -0.005582603, 0.028398408, -0.043390088, 0.052673604, 0.0065953187, 0.031205118, 0.04805339, -0.020007638, 0.03215442, -0.027326519, -0.0046842555, -0.0061487905, 0.0075914753, -0.007893681, -0.10379473, -0.029873755, -0.02033555, 0.03409187, 0.045014363, 0.0067478167, -0.013366672, 0.020559506, -0.01982792, -0.006257009, 0.018783744, -0.011294419, 0.004292942, -0.019294813, 0.026103942, 0.031831544, -0.019135006, 0.0055314773, -0.015367152, 0.013518713, 0.058412347, 0.044961248, 0.040147558, -0.026093246, -0.0042800675, -0.0072903596, -0.024831856, -0.04069209, 0.0022015576, -0.0353914, 0.007705484, -0.083024986, 0.029964719, -0.05526599, 0.00839801, 0.010928533, 0.01312206, 0.04879146, -0.0074076327, 0.018369874, -0.034128524, 0.005917263, -0.084979266, 0.012457851, 0.07503725, -0.0071071694, -0.030344948, -0.018286688, 0.012822725, 0.09630234, 0.015850622, 0.00038905288, -0.04585693, -0.06621663, 0.020252267, 0.013679013, 0.0052925567, 0.029755233, -0.026717171, 0.051197696, -0.027190346, -0.0013359211, -0.03402291, -0.015231403, 0.010003947, -0.065576576, -0.033435557, 0.059224967, 0.040234003, -0.027226929, 0.009881934, -0.049207117, -0.0396572, 0.032130055, -0.0016473018, 0.020379229, 0.024730075, -0.0161681, 0.00046612366, 0.06323032, 0.006188735, 0.006545853, -0.031722497, -0.0024295335, 0.025682084, 0.054283045, -0.0058302856, 0.0027536268, 0.002282717, 0.001391148, -0.01880737, -0.055831518, 0.020344155, -0.011206874, 0.030969806, -0.036198635, 0.014657639, -0.03010035, -0.020437751, -0.0052146385, -0.009501687, -0.025188776, -0.008356824, 0.013110949, 0.0036892917, 0.021927223, -0.035646107, 0.0063569997, 0.049511824, -0.0051831235, -0.05489535, -0.0053172805, 0.00867877, 0.021363039, -0.0037592812, 0.041838452, -0.003775518, -0.012307561, 0.06605157, -0.044014137, -0.0449005, 0.015748508, 0.021570686, -0.008246546, -0.017581396, -0.06617228, -0.019127185, -0.037297804, -0.005308544, 0.011847392, 0.04512272, -0.008068367, -0.03539411, 0.029472338, 0.0075496174, 0.025596624, 0.008419664, -0.042337112, -0.0016504186, 0.023671754, 0.018323403, 0.0055095907, 6.7753084e-05, 0.010459202, 0.051682133, 0.007125673, 0.035071064, 0.004537622, -0.06171483, 0.06265732, 0.010253042, 0.04315808, -0.014343374, -0.0055424804, 0.012624817, -0.00056920986, 0.012729279, -0.0013423462, 0.046368107, 0.010659586, -0.0070417034, -0.03901039, -0.028714387, -0.0056387666, 0.012124049, 0.022751993, 0.012651195, -0.06950994, -0.007846451, -0.002488155, 0.05227898, 0.07871542, 0.03654836, 0.09692303, -0.018938096, 0.0034063715, 0.008970666, 0.06718495, 0.0031776272, 0.02287068, 0.0246718, -0.05241686, 0.021438075, 0.0038115429, -0.020505263, -0.016545478, -0.043501496, -0.006741804, 0.0569698, -0.044663236, -0.014861531, 0.036403526, -0.01482753, -0.008799431, 0.0061374754, -0.020929268, 0.0047061746, 0.04826576, 0.04558536, 0.0022830814, -0.038313292, -0.026747212, -0.024790423, -0.008037862, 0.03627042, 0.03839538, -0.027454453, 0.012095484, -0.007849539, 0.074859306, 0.008602731, 0.020173151, 0.0024588348, -0.028598405, -0.007568841, -0.027644116, 0.051040754, 0.053150445, -0.00056701334, 0.03943608, 0.04605416, -0.023590975, -0.026836947, -0.031771444, -0.0058215293, 0.048906565, -0.10973258, -0.08783959, 0.037758052, 0.01336011, -0.013392369, -0.010879701, 0.02346396, 0.037823994, -0.085543096, -0.008131203, 0.04452721, -0.0147850225, -0.023005508, 0.032964725, -0.0052885246, -0.012294866, -0.04140408, -0.08655551, -0.0063756844, 0.0009952965, -0.05321792, 0.017284887, 0.0073349103, 0.056991573, 0.08398125, -0.056977767, 0.03891387, 0.022949351, 0.03327832, -0.054625567, 0.002737032, 0.024366675, -0.009953388, -0.0018088322, 0.021169286, 0.032734286, 0.003914323, 0.027037568, -0.018376054, 0.025133012, 0.08069081, 0.015830753, -0.07662218, 0.009441282, -0.08519676, 0.025540933, -0.03827695, 0.025368342, 0.038059387, -0.015387954, -0.07432376, -0.006454517, -0.057836022, -0.0539625, 0.038367867, 0.07039019, 0.007421931, -0.012448968, 0.01301285, -0.025436344, 0.021013426, 0.009945321, 0.04805269, -0.009681014, -0.05972199, 0.021381482, 0.007308976, 0.011862264, -0.026267078, 0.013783858, -0.00096760335, -0.078996316, -0.04986617, 0.026219087, -0.031653084, 0.047375966, 0.04728047, -0.008990699, 0.0520238, -0.07784959, -0.027956896, 0.05261413, -0.01388296, 0.031236859, -0.0034668297, 0.04197217, -0.031119013, 0.020800637, -0.017216254, 0.023614516, -0.011160152, -0.08509074, -0.061480466, -0.005290383, 0.01586812, 0.048739623, -0.047789734, 0.010623125, 0.07524574, -0.001270602, 0.043816257, 0.0034011698, -0.005260087, -0.00097625307, -0.009803922, 0.0011978266, -0.013301827, -0.0064269854, -0.043895308, 0.035106864, 0.011078151, 0.0032178517, 0.009011027, -0.010961623, -0.04468767, 0.045709867, -0.06399567, -0.019532716, -0.023444349, -0.031508606, -0.015157455, 0.044850327, 0.016798366, -0.059300218, -0.01406638, -0.048387446, 0.007343179, -0.04489128, 0.034155965, -0.026897935, -0.0041987267, 0.024823554, 0.02049683, 0.014540977, -0.012550907, -0.02817991, 0.004931251, 0.046665683, -0.039646186, 0.044818647, 0.06597325, 0.04952752, 0.013454317, 0.066606835, 0.07267949, -0.018178428, -0.04394722, -0.0037954184, -0.004641098, 0.022543829, 0.0037952524, -0.011959863, -0.0054005412, -0.0014484419, 0.01640055, 0.01703846, 0.02218052, 0.059946258, 0.007921852, -0.043302864, 0.019906644, -0.017632943, -0.018431252, 0.024683451, 0.03377323, 0.010916369, -0.0344663, 0.048337378, 0.017239623, 0.005710608, 0.031244626, 0.020099586, 0.0010837052, 0.03382657, 0.002322921, -0.0006001492, -0.03338079, -0.03143236, -0.035663966, 0.034429293, -0.032341413, -0.04859775, -0.0010232076, -0.052787624, -0.0053324285, -0.016731622, 0.029736573, -0.031906698, 0.009337828, 0.023814572, -0.041414082, 0.031312574, 0.028288193, -0.0074006966, 0.011533981, 0.006853693, 0.0172014, -0.01875689, 0.0095083965, 0.03203226, -0.029446907, 0.0052588666, -0.050076347, -0.040261533, 0.025458207, -0.011024132, 0.04077637, -0.015958004, 0.037362482, -0.03617729, -0.043395, -0.044168644, -0.0068897502, 0.0097664315, -0.042854052, -0.0031694486, 0.005627184, 0.044651646, -0.023188153, 0.021846673, -0.08131188, 0.016635967, 0.026672866, -0.051448744, -0.0677325, -0.0562452, 0.03288406, -0.021094633, 0.005329333, -0.011450638, -0.029624106, -0.08196603, -0.009701128, -0.08097319, 0.05642207, -0.026596582, 0.052381143, -0.01860114, 0.0023889483, 0.014038514, 0.067640714, 0.0031030164, -0.042523008, -0.004895639, -0.0012529875, -0.09137317, -0.027375564, -0.061644297, 0.020177817, 0.025793342, 0.016237436, 0.12075618, 0.024176465, -0.007516323, 0.013632484, 0.06874746, 0.016830847, -0.06420398, -0.03271616, 0.03466609, -0.01582431], "index": 1, "object": "embedding"}, {"embedding": [0.016426979, 0.053491347, -0.19611079, -0.08917866, 0.0697581, -0.034512, 0.07212199, 0.042325076, 0.0087267775, -0.04062063, 0.025149116, 0.0044533852, 0.080649085, 0.052218746, -0.03547304, -0.019439962, -0.0010038693, -0.04031822, -0.043340463, 0.026134815, -0.013012371, -0.041704893, 0.019450873, 0.003798909, 0.024174374, 0.024614217, -0.0440396, -0.031914145, -0.008696301, 0.022969678, 0.05158719, -0.03197671, -0.0017270503, -0.054114822, -0.030819194, -0.0023135792, 0.043133065, -0.00040013023, 0.036212977, 0.059234407, 0.027381824, 0.042015217, 0.026996149, -0.040552847, 0.004990647, 0.0221723, 0.08246712, -0.020265374, 0.041121725, -0.03326609, 0.056833714, -0.030658895, -0.0006163972, 0.03719216, 0.0928954, -0.0051169237, 0.001250644, 0.016632523, 0.0063392385, -0.07640028, 0.06374727, 0.062394716, -0.07273555, 0.017589398, -0.0068303198, 0.033812996, -0.04913011, 0.026498962, 0.0062422277, -0.048185356, 0.042509627, 0.005388064, -0.008247633, 0.022176964, -0.029544564, 0.017847082, -0.0320181, -0.02690255, -0.0370602, 0.040532544, 0.022340331, -0.01390255, 0.013415511, -0.002062635, 0.023304766, 0.04703141, -0.0067095743, 0.036626693, -0.025086917, 0.080223106, 0.013728336, -0.021637907, 0.036253817, 0.027269846, -0.07006454, 0.06714803, -0.024511343, 0.013493629, -0.01043698, 0.016757343, -0.057766333, -0.010277668, -0.01336334, -0.019986564, 0.016384229, 0.01934548, 0.009758394, -0.020821761, -0.0036139635, -0.013904149, -0.051228788, -0.009187779, 0.013984726, 0.0017933103, -0.027319703, -0.043018725, 0.084429115, -0.004813468, -0.035739385, 0.038875964, -0.02399107, -0.015370804, -0.023266837, 0.018750645, -0.011821973, 0.010942853, -0.023488363, 0.0011224591, 0.025151009, 0.02059891, -0.041599456, -0.051834762, -0.036605958, -0.008555659, 0.060174644, 0.05514543, -0.008083128, -0.02192132, 0.038739167, 0.045714363, 0.06038258, -0.010418866, -0.01118422, -0.0036511498, -0.017858798, -0.04047043, 0.028327392, 0.00095469056, -0.007629679, -0.0026424718, 0.021873666, 0.022336341, -0.013674749, 0.014267108, 0.047812566, -0.040821422, -0.017645016, 0.055285316, 0.020655306, -0.00951034, 0.042157263, -0.02554467, -0.009848814, 0.04513554, -0.0064169765, -0.03199678, -0.017260889, 0.039360017, 0.015303744, 0.0015240961, -0.050697025, -0.01608571, -0.0010041926, -0.0017682964, 0.049382556, -0.019602627, 0.086025745, -0.054000687, 0.020018978, -0.08107518, 0.04271982, -0.055920247, 0.013584463, 0.015774129, 0.041067697, -0.013937484, 0.027667008, 0.0016540046, 0.006615701, -0.07780699, 0.025383774, 0.010867939, -0.07209545, -0.011453678, -0.04733568, -0.025475008, 0.031833254, -0.034395814, 0.018987402, 0.0021446315, -0.027767861, -0.02394255, -0.0313928, -0.010347577, -0.042855073, 0.024531927, 0.0050194953, 0.029176999, -0.022211099, -0.014283288, 0.044046566, 0.016186947, 0.050431505, 0.044870876, -0.012517919, -0.017423235, 0.029590778, 0.0050969063, 0.009528286, 0.0141513245, 0.03029416, -0.02252956, 0.03503804, -0.020756098, 0.02068856, -0.0468988, -0.0075716274, -0.010384561, -0.042068344, 0.04479194, 0.01026811, -0.08780585, 0.070640266, 0.001960596, 0.007030947, 0.05650496, 0.025067654, -0.031112047, 0.037498802, 0.01579439, -0.02217205, 0.007057621, -0.013337078, -0.043347366, -0.02870717, 0.0046895654, 0.008720339, -0.038811684, -0.0067892177, 0.05322441, -0.0013513564, 0.05145404, 0.048960965, -0.054344535, 0.05596837, -0.07354667, -0.02836529, -0.0021570115, 0.0367171, 0.04450198, 0.040277727, 0.011908096, -0.01922522, 0.027860533, -0.013614362, -0.054996006, 0.012054954, -0.030918008, -0.014599825, -0.01811836, 0.00660326, -0.00096293224, 0.019041803, 0.024995612, -0.005161961, 0.052210342, -0.017984817, -0.015436568, -0.028231686, 0.006357583, -0.022498395, -0.06415941, -0.02400383, -0.017614275, 0.02018586, 0.03339057, 0.01019938, -0.00057226385, 0.0019603532, 0.015160026, -0.026932357, 0.037579756, -0.020102253, -0.0009259528, -0.029688297, 0.035508692, 0.04838761, -0.039311416, 0.024430556, -7.212613e-05, 0.025432846, 0.049943376, 0.027505709, 0.061262656, -0.036901228, -0.0078066946, -0.025739588, -0.0016508385, 0.04995928, 0.003223726, -0.025263539, 0.0078160465, -0.10710499, 0.03647971, -0.004038162, 0.040721167, 0.030153388, 0.038892414, 0.03918332, 0.0037060836, 0.013922056, -0.0009382312, -0.037130047, -0.07041945, 0.019828307, 0.044638548, -0.05327554, -0.022703882, -0.053557955, -0.011403099, 0.07080762, 0.02956508, 0.023055717, -0.038435817, -0.08174152, 0.005025663, -0.0016188362, -0.0014988685, 0.018571684, 0.019946653, 0.027961215, -0.048922475, 0.060907118, -0.07375287, -0.04791587, 0.0123888375, -0.04624429, -0.03687441, 0.048794325, 0.012149002, -0.014474048, -0.0045621456, -0.03269851, -0.042639364, -0.0061781495, -0.02729711, 0.022739924, 0.034638647, 0.013153269, 0.0063800327, 0.015585022, 0.0011802969, -0.01115537, -0.08526576, 0.019802222, 0.037898693, 0.030802602, 0.0004591931, -0.005497938, -0.012713919, -0.048323598, 0.00899654, -0.027622469, 0.021468282, 0.030842569, 0.050296303, -0.043910682, 0.0017619218, -0.014115627, -0.004766534, 0.05033142, -0.015845886, -0.017201884, 0.008773843, -0.009120471, 0.0041352697, 0.049915966, -0.04011602, -0.00977306, 0.035024967, 0.058766425, -0.031110214, -0.047817644, 0.033020645, 0.02185556, -0.044353206, 0.0006911412, -0.009597024, -0.019249093, 0.03434841, -0.066725925, -0.02399449, 0.0022067362, 0.0106978165, -0.01387732, 0.030144725, -0.09196821, -0.022612054, -0.014741013, -0.011516579, 0.008698098, 0.06523173, 0.035879534, -0.04835121, 0.017097292, 0.018099625, 0.04763397, 0.0096252, 0.00598593, 0.0029222004, 0.030568803, 0.016074011, -0.0032133795, -0.008311627, -0.01584128, 0.01907034, 0.045515824, 0.053465284, 0.012139215, -0.08377399, 0.0793192, -0.00034334304, 0.05796611, -0.030150203, -0.0108620785, 0.023234034, -0.015529712, -0.0067011737, 0.016421385, 0.035290994, 0.050383594, -0.046869222, 0.011350059, -0.037916426, -0.035420906, 0.07543551, 0.055679988, 0.026809784, -0.0674469, 0.03831088, -0.023067998, 0.022166723, 0.026658198, -0.011804601, 0.08694171, -0.03255906, 0.0036455838, -0.003999333, 0.008255391, 0.037171233, 0.038824655, 0.050719395, -0.08852458, 0.019107707, 0.0147692645, 0.008311753, 0.017493272, -0.031000791, 0.026045991, 0.021979814, -0.050112046, -0.015072277, 0.024264917, -0.00035860334, -0.017950933, 0.0084113935, -0.0011132568, -0.006187009, 0.03802682, 0.06379016, -0.029846348, -0.02013295, 0.0054455535, -0.017546294, -0.015459897, 0.039376266, 0.051221017, -0.015790278, 0.025056621, 0.01353681, 0.06559427, 0.027455438, 0.013777982, -0.0037467927, -0.012174396, -0.04566106, 0.005993694, -0.0042891414, -0.017119043, 0.0038781445, 0.056779552, 0.05671837, -0.028724348, -0.018479753, -0.020642342, -2.5745121e-05, 0.054779116, -0.08492405, -0.030062191, 0.0077932714, 0.018976184, 0.0149955265, 0.013806929, 0.03986167, 0.023022722, -0.052031748, -0.008354624, 0.070831455, -0.004833082, -0.009625585, -0.0030631819, 0.0023739696, -0.030390233, -0.035354502, -0.11104275, 0.026690021, 0.03644422, -0.041205764, 0.019745318, -0.002512154, 0.05082758, 0.040175173, -0.03006183, 0.046329547, 0.027849635, 0.024007993, -0.04788897, -0.0029994573, 0.015914846, 0.029549068, 0.02036391, 0.0339585, 0.018415304, -0.024155123, 0.0009978242, -0.028586868, -0.0018740768, 0.009389885, 0.020344742, -0.05759447, 0.005089431, -0.051872145, 0.041857943, -0.0107226195, 0.019986678, 1.0163026e-05, -0.0266176, -0.032902967, -0.026188947, 0.020379433, -0.016417569, -0.0108089, 0.08025923, 0.02513234, -0.0026752627, 0.0014367446, -0.032399606, -0.00036845167, -0.023204604, 0.017231055, 0.0074259667, -0.07403949, 0.052004408, 0.017514246, -0.0002548388, -0.01930634, -0.029100504, -0.024544924, -0.022206772, -0.063704014, -0.024673754, -0.0400939, 0.025273813, 0.026375761, -0.002418386, -0.0067220186, -0.057371635, -0.005211232, 0.031591274, -0.020243473, -0.00755934, -0.00012053872, 0.048408512, -0.019687645, 0.021810079, -0.0343903, -0.028895238, -0.023956269, -0.08470399, -0.068308674, -0.022026129, -0.025562549, 0.03447612, -0.045768905, -0.0024254972, 0.012291095, -0.0068358453, 0.042912014, -0.031749394, 0.0056773373, 0.003446033, -0.009431567, -0.011590614, -0.010877408, 0.031537026, -0.045429196, 0.0315962, 0.045012064, -0.01087124, -0.018171616, -0.04100321, -0.04014535, 0.068526566, -0.061360333, 0.023934197, -0.041403126, -0.087629125, -0.01255903, 0.037247203, 0.024635976, -0.010780544, -0.024382161, -0.036019385, 0.026900336, -0.03966605, 0.023457598, -0.060421646, 0.036529806, 0.017049931, -0.0137957875, 0.023794191, 0.022030286, 0.008872926, 0.042454023, 0.028850783, -0.016152881, 0.06300803, 0.06608809, 0.039173864, -0.016478337, 0.09695304, 0.054287814, 0.0112531725, -0.02595301, -0.01114534, 0.01425871, 0.008773467, -0.0018664745, 0.023279766, -0.025589183, 0.016382363, -0.007464346, -0.031331033, 0.0095314365, -0.0014988827, -0.03732596, -0.0033601886, -0.014066755, 0.00072941644, 0.001837271, 0.06967086, -0.015903838, 0.010231371, -0.04044283, 0.03437743, 0.0076745385, 0.023842214, 0.0014073403, 0.037948847, 0.0075548585, 0.015859252, -0.0041477396, 0.03759034, -0.004956967, -0.039568033, -0.0007074713, 0.022542778, -0.014738636, -0.06844783, -0.045913387, -0.052081794, -0.023494978, -0.029153086, 0.029239492, -0.026641421, -0.0031602322, 0.023761325, -0.050275277, 0.0022114303, 0.016096747, -0.033433888, 0.033687443, 0.025594411, 0.002022943, -0.026288893, -0.0519235, 0.029535959, -0.024643049, 0.0120866485, -0.036083538, -0.04342939, 0.0184222, -0.02376534, 0.056405984, -0.040096942, 0.022854935, -0.05027074, -0.03534815, -0.05010666, -0.0024218266, 0.062908635, 0.0060344334, -0.06733596, -0.024574121, 0.026123056, -0.023709653, 0.06276571, -0.09038913, -0.0038688332, -0.03188433, -0.019422544, -0.009345397, -0.059852444, 0.009628658, 0.00885876, 0.037956525, -0.058508825, 0.006309014, -0.07350446, -0.03776107, -0.07189196, -0.0012232016, 0.0049571055, 0.00956412, -0.012525668, 0.007221603, -0.0068168123, 0.042147644, -0.02440407, -0.054597046, 0.002378795, -0.030093815, -0.024117166, 0.0128834965, -0.009993424, 0.07283297, 0.021783499, 0.030000791, 0.098411605, 0.046200026, 0.010682366, -0.023861466, 0.028190015, -0.0067478106, -0.018276332, -0.056911383, -0.030446636, -0.004960953], "index": 2, "object": "embedding"}, {"embedding": [0.04441668, -0.0035543812, -0.20124762, -0.044643547, 0.04516398, -0.03462006, 0.052019317, 0.019370146, 0.021327075, -0.022044891, -0.042753093, 0.017976688, 0.07930047, 0.033267505, 0.00483136, 0.025131434, 0.037715882, -0.05043691, 0.010274557, -0.028789176, 0.014013554, -0.027676431, -0.0016980035, -0.032252975, -0.024972064, 0.008974455, -0.01367239, 0.014920254, 0.025789618, 0.01584965, 0.044889767, -0.007047377, -0.006486554, -0.062441334, -0.01571034, -0.029347684, 0.056824185, 0.014499251, 0.011922875, 0.04986446, 0.01933484, -0.024362693, 0.022029579, -0.023099622, 0.044817306, -0.021131676, 0.049073678, -0.02299411, 0.02559819, -0.024047043, 0.04015381, 0.029893307, -0.015676478, 0.0075238855, 0.10909838, 0.06971524, -0.035550054, 0.06350495, 0.027117182, -0.08866666, 0.091000594, 0.029628076, -0.10727081, 0.037004434, -0.0401393, 0.004572155, -0.039313506, 0.030073615, 0.0061354274, -0.06339246, 0.01358756, 0.051634055, -0.011856155, 0.021899767, -0.04550422, -0.005681501, -0.042100508, -0.030177379, -0.01874303, -0.029770102, 0.008760106, 0.0337089, 0.08193085, -0.030412493, 0.011750957, 0.027873876, -0.050460007, 0.015550819, -0.042095482, 0.098753765, -0.027741775, 0.047682773, 0.028079644, -0.0047178273, -0.04290301, 0.012674847, -0.036723763, 0.019521795, -0.0021100927, -0.030779097, -0.012279883, -0.0095766485, -0.012589648, 0.005174673, -0.014901155, 0.058359787, 0.026412044, -0.023736753, 0.0028185896, 0.003479882, -0.060086805, 0.033132475, -0.04559614, -0.00819161, -0.047058444, -0.013037717, 0.069359675, -0.018599525, 0.0057941056, 0.021318804, -0.011090757, -0.044386137, 0.042759653, -0.006878828, 0.028209483, 0.02798096, -0.07435699, -0.0019674154, -0.0325115, -0.007214323, 0.010221568, -0.020320771, -0.03273307, 0.01611744, 0.03763371, 0.07684503, -0.0012036402, -0.010895047, 0.022100544, 0.009973168, -0.00044915493, -0.030675916, 0.004832171, 0.007624846, 0.026452495, -0.0011477424, -0.008744162, -0.018464046, -0.03378722, 0.027933102, 0.018091561, 0.05917108, 0.013080924, 0.026462948, 0.026633222, -0.033658706, 0.0072284327, -0.013630023, 0.013339367, 5.286649e-05, 0.038905676, -0.037471883, -0.032386553, 0.035249718, -0.003333871, 0.015457396, -0.005708936, 0.036284886, 0.041304782, 0.03869319, -0.058410574, -0.0907644, 0.005174569, -0.00628111, 0.0516891, 0.008515923, 0.09605206, -0.032012627, -0.0035363615, -0.0047910726, 0.06507034, -0.08250223, 0.022993138, 0.012807359, -0.03718829, -0.021724729, 0.025690084, -0.029432936, -0.00078911654, -0.03667554, 0.0106366435, 0.035300594, -0.08816042, -0.0427188, -0.0368806, -0.024739137, 0.044527434, -0.052676078, 0.009856425, -0.023553101, 0.004369404, -0.034296557, -0.032114986, 0.056867376, -0.024001587, 0.07271911, 0.0101598045, 0.04013234, -0.022787947, 0.01584985, 0.045747377, 0.023729699, 0.04771498, 0.015332298, 0.023711704, -0.004140317, -0.002198936, 0.027357217, -5.9686696e-05, 0.012807889, 0.032395102, -0.0017152393, 0.027881403, -0.0054597543, 0.012351595, -0.0008542204, -0.00086175743, 0.022972334, -0.022164103, 0.04564676, 0.013889058, -0.05008665, 0.04351198, 0.0096105635, -0.0067297174, 0.010412229, -0.038660295, 0.013125858, -0.019524202, 0.03673038, 0.020865453, 0.030881003, -0.018012986, -0.024679175, -0.07277856, -0.0022824213, 0.014158481, -0.043658547, -0.03326011, 0.03412318, -0.010440449, 0.012998919, 0.049645007, -0.009448933, 0.015597762, -0.020284694, -0.06686934, 0.03576328, 0.022741584, 0.009352477, 0.04212721, 0.0035523975, 0.012611934, -0.0077503985, -0.013863437, -0.03504094, -0.015852356, -0.008375345, -0.012047642, -0.032385256, 0.040312182, -0.0063122907, 0.0669537, 0.08821845, -0.010995492, 0.031735748, -0.066732116, -0.025641711, -0.011001128, 0.01562675, -0.0065158317, -0.11974231, -0.022076543, -0.010250129, 0.008536365, 0.03425995, 0.04098975, -0.028930832, -0.0012936869, 0.0020721306, -0.022616602, 0.027180016, -0.01475297, 0.010585619, -0.022833573, 0.004433933, 0.09000675, -0.023356095, -0.0110812755, 0.01321298, 0.038150076, 0.036113862, 0.0246681, 0.053568106, -0.0087973755, 0.005601103, -0.018312832, -0.008231023, 0.03260476, 0.017754184, -0.06202252, -0.021575667, -0.09185716, 0.0406768, -0.026477545, 0.011822516, 0.015930884, 0.0006090636, 0.042136136, -0.027690973, 0.021387164, -0.027171096, -0.0006015709, -0.057848882, 0.017072696, 0.044140298, 0.018518552, -0.024831649, -0.04418174, -0.023068804, 0.087116815, 0.03303294, 0.028151564, -0.047698226, -0.02088852, 0.025196766, -0.037305, -0.016451137, 0.0335975, -0.009845101, 0.056130435, -0.02695132, 0.02705769, -0.0043271272, -0.019302826, -0.039684504, -0.038775742, -0.040937398, 0.00924637, 0.02456794, -0.03352679, -0.0026549844, -0.07334785, -0.027572261, 0.040952288, -0.019767309, 0.0530923, 0.058415066, 0.0152255185, 0.011259514, 0.031260956, -0.015953867, 0.01891225, -0.013576148, -0.019141892, 0.02588285, 0.05339547, -0.013182031, 0.016878778, 0.0029165081, 0.0415719, -0.023979519, -0.03402465, 0.048938252, -0.023171883, 0.039848868, -0.026666824, -0.0018628907, -0.042426918, 0.0053526843, 0.006828848, -0.004922943, -0.006817435, -0.015253054, 0.021142837, 0.011054207, 0.056395724, -0.027642876, -0.0041707777, 0.061420843, 0.022465043, -0.05135771, -0.028755158, 0.005974757, 0.0013922752, -0.0011225021, 0.05772878, -0.0038300094, 0.012076717, 0.040541843, -0.025143307, -0.07337223, -0.0019511314, -0.001094659, -0.030712474, 0.008167848, -0.06285723, -0.03603516, -0.00173823, 0.022847902, 0.011453375, 0.045104828, -0.015044707, -0.046516854, 0.022971133, -0.019954737, 0.0136651285, 0.007052174, -0.013741125, 0.0055670734, 0.014792294, 0.007963074, -0.0036158036, -0.028430773, 0.022074653, 0.023018798, 0.030477187, 0.04881887, 0.038265046, -0.102898605, 0.018762972, -7.632603e-05, 0.05776559, -0.011736579, -0.003416181, -0.01955297, -0.005907715, 0.002926757, -0.033214692, -0.0014345123, 0.035032537, -0.031407263, -0.043377604, -0.03281734, -0.022618808, 0.051061045, 0.02987343, -0.022051692, -0.014957087, -0.008282643, -0.0076356903, 0.022933526, 0.07458577, 0.06166895, 0.082314424, -0.025234506, 0.048399556, 0.02854894, 0.016144626, 0.019364562, 0.048989866, 0.008226846, -0.07460162, -0.01792666, -0.0031085995, -0.03441921, -0.03471416, -0.038885884, 0.0074170395, 0.04991441, -0.017581446, -0.05448539, 0.043244522, 0.007637976, 0.0015781961, -0.039422713, -0.009392761, 0.0096015865, 0.04951689, 0.020887587, 0.023334218, -0.031144526, -0.022465073, -0.060622446, -0.00221366, 0.02070876, 0.030089457, -0.031609047, 0.02195296, 0.012121265, 0.053058207, 0.05156608, 0.030384611, 0.005591421, -0.018980887, 0.0129038105, -0.006952733, 0.018996255, 0.035262704, -0.0041798, 0.031785186, 0.073351, -0.0011435244, -0.035810295, -0.00048509557, -0.03339725, 0.03293801, -0.08382145, -0.061084352, -0.023855627, -0.027701018, 0.045270227, -0.023019379, 0.021541659, 0.034773722, -0.07441201, 0.032683074, 0.04703131, -0.01737638, 0.0076666763, 0.0015712196, -0.022238657, -0.013424539, -0.030431751, -0.06525809, -0.0003328347, -0.01960938, -0.0465399, 0.014477559, 0.010597405, 0.039289758, 0.07188625, -0.036810778, -0.0074690557, -0.0067664622, 0.016759953, -0.05640254, 0.01082015, 0.008385891, 0.035032842, -0.006101136, 0.003085382, 0.032057285, -0.012339353, 0.006612608, -0.037485722, 0.006181646, 0.06336292, 0.0045152847, -0.07159424, 0.012445431, -0.08124471, 0.014377375, -0.037554212, 0.025997654, -0.02008716, -0.017307268, -0.07892859, -0.009650115, -0.029969083, 0.0075862985, 0.012859846, 0.09432203, 0.030631475, -0.009479825, 0.006666807, -0.01666623, 0.032939434, 0.018936861, 0.04944369, -0.013508075, -0.10900276, -0.009353013, 0.0016707762, -0.031554237, -0.027357444, 0.0060949265, 0.008836911, -0.04469658, -0.039937332, -0.008770673, -0.032294396, 0.014594993, 0.068695806, -0.030059937, 0.04189419, -0.07649026, -0.056774154, 0.045382116, -0.006846585, -0.022315685, 0.0049129296, 0.0043057664, 0.018418325, 0.043949474, 0.049189508, 0.008901614, -0.04711236, -0.03628201, -0.044215348, -0.013023677, -0.027151296, 0.0026642117, -0.073801935, 0.02698256, 0.042542588, -0.008259831, 0.046238244, 0.03898998, -0.001511062, 0.0063406774, -0.035973154, 0.01037504, -0.0014714078, -0.037316687, -0.0160811, 0.0528459, 0.0186095, -0.03638564, 0.014577828, -0.02131315, -0.027851366, 0.05245377, -0.060433775, -0.039751068, -0.024784379, -0.0017742153, -0.008052061, 0.025342481, -0.0037656366, -0.063559696, -0.012901464, -0.047348563, -0.028989295, -0.057462487, 0.04522956, -0.01849758, -0.016388452, 0.013503004, 0.03222546, 0.05571676, -0.015087335, -0.03352947, 0.019502882, 0.019604951, -0.05146237, 0.029427053, 0.03662335, 0.04151085, 0.0030915253, 0.0643994, 0.06251555, -0.019052433, -0.019337464, -0.02765814, 0.042400002, -0.011651609, 0.01588539, -0.02353484, -0.009940821, -0.008461789, 0.021219596, 0.016997376, -0.012799124, 0.01874629, -0.022068318, -0.012972941, -0.012711661, -0.016764663, -0.036946543, 0.015484308, 0.009762172, 0.013136123, 0.007974124, 0.052181125, 0.027845405, 0.0341524, 0.062182937, -0.005175263, 0.012864229, 0.012003205, 0.010230593, 0.018937035, -0.034460258, -0.037601884, -0.045870814, 0.026813079, -0.033830103, -0.0145007055, -0.002472526, -0.018445073, 0.021619229, -0.028769806, 0.026911955, -0.06960814, 0.05461261, 0.01948633, -0.051569123, 0.021752844, 0.03917306, -0.013910314, 0.011448628, -0.00063654577, 0.012538524, -0.009770353, -0.011877915, -0.021211786, -0.009853941, 0.011858722, -0.04329116, -0.025407815, 0.037763756, -0.011177658, 0.06816202, -0.0054403036, 0.01638009, -0.00060040795, -0.03604663, -0.0064156055, -0.007888113, 0.03862746, -0.0046019778, -0.043646287, -0.0061732805, 0.041874122, -0.029189998, -0.0113836685, -0.067990385, 0.022026971, -0.018646548, -0.005533464, -0.07966258, -0.06267434, 0.020733483, -0.0007897571, -0.02179166, -0.018683301, -0.004166769, -0.03675074, -0.007363242, -0.06892607, 0.019715207, 0.006459389, 0.048447147, 0.019987937, -0.003952113, -0.003939615, 0.060184512, -0.025262812, -0.020854564, -0.02532848, 0.0037678948, -0.045305658, -0.010924445, -0.04889715, 0.026876494, -0.03579133, -0.005123355, 0.117870875, 0.010063178, 0.02222626, -0.043274622, 0.097324766, 0.036356777, -0.039023805, -0.031984467, 0.007960494, 0.007091825], "index": 3, "object": "embedding"}, {"embedding": [-0.021018974, 0.051332753, -0.11359385, -0.05416892, 0.05257369, -0.038534403, -0.0092310775, 0.005799421, 0.0043979012, -0.032691758, -0.029155046, 0.011183732, 0.09263055, -0.057954226, -0.012276342, 0.021198012, -0.006851015, -0.054870985, 0.01592494, 0.05932566, 0.0305991, -0.06739468, -0.00392501, -0.00070336065, 0.037126705, 0.010285552, -0.022609929, 0.012115788, 0.024546763, -0.0006414898, -0.0044685053, 0.0455876, -0.04669964, -0.029952222, -0.044416834, -0.10134782, 0.072236545, -0.003410899, -0.01056474, 0.05947834, 0.032535367, -0.035951506, 0.0433947, 0.0173956, 0.0357668, -0.03440035, 0.013739767, -0.025343778, 0.018494533, -0.0746555, 0.02821476, -0.017515725, -0.012164174, 0.0046991287, 0.056586534, 0.016894843, 0.017216021, -0.00943768, 0.0012415892, -0.05986355, 0.094514914, 0.017658979, -0.015435943, 0.047012623, 0.06478956, 0.007744172, -0.012392123, 0.06485987, -0.008065256, -0.039242435, 0.08380325, 0.012068597, 0.004606348, 0.012536886, -0.028579304, 0.0026767836, -0.065587826, -0.039076686, -0.004967545, 0.087105095, 0.030156117, 0.0326281, -0.003869808, -0.019214531, 0.011228164, -0.023403702, -0.014508766, -0.025116704, 0.017710647, 0.052223254, -0.043662086, 0.031402946, 0.009411046, 0.0019240408, -0.06717339, 0.050637446, -0.01671212, 0.025699712, -0.06463728, -0.007346206, -0.060472563, -0.044439483, 0.035349116, 0.003651476, 0.041737255, 0.030837303, 0.022081845, -0.03745293, -0.0019599535, -0.017384328, -0.018756198, 0.025667604, -0.02038083, -0.022491183, -0.0342101, -0.010338086, 0.06254209, 0.014075391, 0.06637058, 0.0388884, 0.04149296, -0.014138816, 0.01587972, -0.011562947, -0.005373783, 0.029142337, -0.06647392, -0.00834371, 0.027499113, -0.021130655, 0.035287015, 0.0044701938, -0.081117, 0.033863876, -0.018877748, 0.04535481, -0.062038, -0.013634761, 0.044053894, 0.044157915, 0.07067743, 0.066121235, -0.015635204, 0.034768198, 0.016284216, -0.03636352, 0.01008497, -0.021740908, -0.008602846, 0.042550858, 0.024617469, 0.0066924384, 0.00197076, 0.017479012, 0.021312047, 0.009611456, -0.029949605, -0.004916758, 0.043885026, 0.034281664, 0.043058984, 0.00016952633, -0.043178916, 0.04292665, -0.014624059, -0.036474533, 0.040864665, 0.012340251, 0.012126848, -0.0071974373, -0.055520043, -0.04845202, -0.014695591, 0.010726844, 0.007086431, -0.020762213, 0.039772186, -0.018250166, 0.058716927, -0.009401228, 0.032132246, -0.0034834903, 0.016299933, 0.016941758, -0.025340466, 0.037500978, 0.012243165, -0.013973242, -0.07591275, -0.009891626, 0.059034977, 0.0075196945, -0.04929728, -0.035768937, -0.051504776, -0.025501842, 0.046501335, 0.00049787894, 0.062020734, -0.07507346, -0.00710394, -0.026997814, -0.10482832, 0.04164529, 0.00068114523, 0.01562308, 0.029600557, -0.0062896614, -0.010151766, 0.022744857, 0.07395808, 0.0060573635, -0.002880219, -0.03231717, 0.005159823, -0.05377698, -0.012760193, -0.006256022, -0.0182771, 0.028798442, -0.0017972146, -0.016783169, -0.006476447, 0.01820948, 0.045912955, -0.011954348, -0.04983923, -0.001584058, -0.0593908, 0.006793927, -0.004066465, -0.067436256, 0.0230033, -0.0135483425, 0.053182606, 0.025302432, 0.02638945, 0.01617855, -0.036287572, 0.008382259, 0.021713084, -0.009310465, 0.019272633, 0.015753698, -0.028028114, -0.010205721, 0.021244373, -0.048609573, 0.009717748, 0.056776475, 0.021948176, -0.030680565, 0.023788955, 0.0039937953, 0.0523772, 0.037292246, 0.024815964, 0.034920856, 0.039886974, -0.026603991, 0.043613333, -0.018272767, 0.019121438, -0.038904693, -0.002000291, -0.01333681, -0.03465461, -0.035994533, 0.008301608, 0.013202103, 0.016143138, -0.0036437216, 0.027643671, 0.0014425259, -0.007825404, 0.035057034, 0.014989224, 0.0029210732, -0.016884869, -0.002041141, -0.0032402528, -0.10277356, -0.021717992, 0.036326226, -0.0138065405, 0.058011167, -0.035095777, 0.011722236, 0.015302629, -0.022679266, 0.07639482, 0.003475684, -0.058267508, -0.059742842, 0.00572358, 0.018205313, 0.058707107, 0.029422294, 0.033754047, 0.025882475, 0.047952816, 0.02975185, 0.087615065, 0.0324346, 0.021223234, -0.022352306, -0.033175424, 0.013573265, 0.037683196, 0.006477548, -0.025459936, -0.010969508, -0.061543267, 0.056104485, -0.033156604, 0.051665954, 0.018975712, 0.007469293, 0.03886233, -0.029518303, -0.03513711, -0.082226396, 0.0017909099, -0.060468517, -0.0067266384, 0.051961225, -0.01975278, 0.0034568012, -0.030216673, -0.040002014, -0.005498588, 0.008425766, 0.02398948, -0.05696289, -0.08152436, 0.003928075, 0.02529423, 0.010447488, 0.042015254, 0.0049296943, 0.08933547, -0.021696554, -0.009312475, -0.066324905, -0.0780837, -0.031022348, -0.008394544, -0.018868728, 0.034813005, 0.053742547, 0.008998806, 0.01431665, -0.0010406214, 0.0031417434, -0.026112614, -0.035051133, 0.027413547, 0.040282607, 0.011834284, -0.021626022, 0.020789204, 0.005089773, 0.030764142, -0.055436265, 0.011025402, 0.025882907, 0.03311945, -0.020328937, 0.038342077, 0.018728932, 0.013264117, -0.017977098, -0.010664038, 0.042661257, -0.02008807, -0.01377361, -0.06771381, -0.019877683, -0.015893308, 0.02037836, 0.026948264, -0.011708443, -0.003643314, 0.0058382633, -0.01802519, -0.018761825, 0.055014875, -0.03608739, -0.023601796, -0.0002983141, -0.037961792, -0.0167175, -0.016702954, -0.013226227, 0.06381164, -0.0023911956, 0.008896032, 0.016246887, -0.064894, 0.0948341, -0.067487955, -0.10754077, -0.03614324, -0.049625717, -0.015194408, 0.00047633375, -0.0036759714, -0.0031620096, -0.0037538337, -0.017044308, 0.018617522, 0.02172217, -0.0434751, -0.03889604, -0.012075532, -0.010729736, 0.04518121, 0.018052064, -0.06883887, -0.025682818, 0.0051785083, -0.0038978257, 0.03052615, 0.0041969963, 0.0052882144, -0.03195896, 0.043929428, 0.047722258, -0.042398944, -0.047442473, 0.027382968, 0.024434574, 0.058596354, 0.03450553, 0.00180553, -0.014045558, 0.03527593, 0.021413058, -0.018814899, 0.01740446, 0.02921876, -0.0070126387, 0.019447485, -0.026877038, -0.036859643, 0.05146635, 0.03715775, 0.006320812, 0.005594063, 0.010993893, 0.037239715, 0.03647802, 0.009062142, 0.019946227, 0.0006890491, 0.034040686, 0.04243522, 0.021148179, -0.0056927376, 0.016195416, -0.014385446, 0.018339535, -0.025736997, -0.006202856, -0.009550825, -0.008766409, 0.016998695, -0.0007653374, 0.01020806, 0.021878915, -0.04021446, -0.023059856, 0.039521333, -0.0068091783, -0.010846535, -0.013339679, 0.010625797, -0.031612705, 0.019087711, 0.06446194, 0.016933002, -0.046011135, -0.04613558, -0.056118134, -0.02589002, 0.0007603057, 0.020630524, -0.034459755, 0.0039221686, 0.006013229, 0.024738448, 0.032643408, 0.072782524, 0.016808148, -0.04738458, -0.035055015, 0.005056439, 0.036785483, 0.024402158, 0.018029189, 0.05732763, 0.04106553, 0.0012430901, -0.005600108, 0.030177504, -0.04857699, 0.030020488, 0.010180609, 0.01680544, -0.007348671, 0.010789691, 0.07282762, -0.009059519, 0.04237495, 0.030729134, -0.036229275, 0.057686463, 0.004590424, -0.08593208, 0.016100142, -0.026310625, -0.030636119, 0.05957648, -0.04940259, -0.026818315, 0.03266229, 0.021114409, -0.005029006, 0.027942047, 0.022829298, -0.0074161934, -0.005061, -0.03067795, -0.04270312, -0.019020105, -0.0069849314, -0.08170525, 0.035638675, 0.07804242, 0.0041790702, 0.02437412, -0.05988438, 0.0020847758, -0.04118893, 0.014064264, -0.032882746, 0.0028010048, 0.014718119, 0.03453101, -0.050986495, 0.055473797, -0.04826334, -0.009727298, -0.045942903, 0.017213736, -0.04469582, -0.008897419, -0.011185968, -0.0280203, 0.005565284, -0.016573736, -0.023126082, 0.08365712, 0.0056814025, -0.0035361177, -0.035952043, -0.03784688, -0.038521294, 0.0043893186, 0.10507003, -0.016201546, -0.04125667, -0.011947582, 0.024870938, 0.03566282, -0.036751643, 0.01320229, -0.021736912, -0.0028588085, -0.07451891, 0.04523553, -0.041115575, 0.08372637, 0.021134159, 0.039955176, 0.011676949, -0.008289593, 0.028535876, -0.037209895, -0.07646548, 0.027166644, 0.021455582, -0.004938156, -0.05455283, -0.021990074, 0.040048454, -0.03751374, 0.026153678, -0.03366277, -0.03101463, -0.063268356, -0.03869211, 0.06191247, -0.007938471, -0.0035953762, 0.041721728, -0.011976508, 0.025541514, -0.006903107, -0.009194143, -0.0056304485, -0.0299426, -0.054931313, -0.0025325862, 0.04435446, -0.011236195, 0.056356546, -0.012091858, 0.018273583, 0.00891119, -0.03703973, -0.023252286, 0.02334385, 0.026149493, -0.023783082, -0.05293672, -0.027113002, -0.007766072, -0.016149966, 0.04650572, -0.059630718, 0.0042876927, -0.031429667, -0.01412431, -0.01821952, 0.023247225, -0.061353143, -0.03060952, 0.020228053, 0.09149102, 0.037487295, -0.049597397, -0.0229128, 0.03996293, -0.022273628, 0.022647625, 0.037679993, 0.08436003, 0.04047854, -0.02118041, 0.04980222, 0.08987712, 0.06259597, -0.002294331, 0.052401043, -0.0031743052, 0.01696231, -0.03206836, -0.07957096, -0.001458858, -0.0029577573, -0.026484922, -0.0014393915, -0.021171205, 0.030789172, 0.040977478, -0.03883559, -0.018937701, -0.031435654, -0.05066885, 0.02188463, 0.0359158, -0.0038272613, -0.056559436, 0.012917739, 0.041743275, 0.063461386, -0.02654269, -0.03713737, 0.0047825826, -0.037957944, -0.030541312, 0.012806541, 0.0016545148, -0.009965287, -0.035483707, 0.0015980722, -0.029993262, -0.020705653, -0.054143492, -0.031187164, -0.030937817, 0.0003479184, -0.016956521, -0.00747317, 0.006364715, -0.025814842, -0.04472815, -0.00943726, 0.031057777, -0.041894075, -0.009760313, -0.0012742716, 0.016097514, -0.002917242, 0.020397, 0.02270855, 0.08524565, 0.015256992, -0.05763265, -0.028215494, 0.004530044, -0.020025136, 0.02139962, -0.016893387, 0.03583756, -0.023101663, -0.054726236, -0.015799532, 0.0074812644, -0.0006507771, 0.01662149, -0.032602802, 0.012672912, -0.016630404, -0.006543261, 0.009680758, -0.04525146, 0.02925856, -0.02130365, 0.04656584, 0.009203885, -0.061645545, 0.05335463, -0.009315405, 0.050715778, -0.051368512, -0.04520389, -0.04757586, 0.019474212, -0.026744986, 0.006504403, -0.06500136, -0.014909006, -0.008545999, -0.02000323, -0.0061344192, 0.013829218, 0.051493485, -0.04892515, -0.0037622019, -0.0920161, -0.016381241, 0.015515344, -0.0063986564, 0.0005441228, 0.013517718, 0.04201718, 0.102116495, 0.042105716, 0.010417758, -0.028982379, 0.02337377, 0.011840699, -0.009086776, -0.040296666, -0.021008637, 0.0007939679], "index": 4, "object": "embedding"}, {"embedding": [0.018109482, 0.075962536, -0.13344125, -0.0758743, 0.029311813, 0.009785278, 0.0023403391, 0.06347073, 0.008113987, 0.005805996, -0.0025450129, 0.001147894, 0.083088614, 0.013511421, 0.011750011, -0.018267939, -0.020864017, -0.081776135, 0.007158829, 0.043766342, 0.051638857, -0.025953002, 0.035692703, -0.038475562, 0.00972361, -0.06681032, -0.028624149, 0.0017775291, -0.053547822, 0.032989863, 0.02719165, -0.03037019, -0.0035876923, 0.02091463, 0.005182144, -0.028981775, 0.052536022, 0.041163333, -0.026165092, 0.06867413, 0.07395716, 0.01339617, 0.019619767, 0.01949938, -0.029009866, -0.03780141, 0.011215557, -0.052498434, 0.045658104, -0.026493372, 0.032033537, 0.018150367, 0.0018843499, 0.040373147, 0.10806848, 0.009245847, -0.01505499, -0.00036926565, 0.014900736, -0.058713373, 0.09640662, 0.07960527, -0.068462715, 0.01899526, 0.021116797, -0.014239639, -0.0072701205, 0.05713487, 0.026550962, -0.034323752, 0.03240242, 0.023169322, -0.009982846, 0.06763507, -0.048897654, -0.026306393, -0.039540682, -0.005275132, -0.0025472466, 0.06403883, 0.04473415, -0.008027234, 0.05341232, -0.045444965, 0.04580048, -0.017609028, -0.041995287, 0.007209539, -0.02159523, 0.10560222, -0.002641834, 0.003506822, 0.02677092, 0.032395013, -0.033500545, 0.03357422, -6.5263516e-06, -0.0018867998, -0.051825117, -0.04508012, -0.061540246, 0.02197796, 0.054853417, 0.0077995225, 0.060158882, 0.035781898, 0.035761587, -0.023262141, -0.015841808, 0.0019117635, -0.028157689, 0.029078947, -0.019892761, -0.0067871697, -0.0744025, -0.033449385, 0.03302611, -0.0010770531, -0.027001215, 0.04211092, 0.044984967, -0.002458361, -0.045006573, -0.018843735, 0.0063305763, 0.010799094, -0.05614293, -0.016464824, -0.043601766, -0.033930175, 0.005921228, -0.008253487, -0.0063095847, 0.017162764, 0.009429539, 0.08903689, -0.03609781, 0.008037937, 0.040382367, 0.00994432, 0.041384567, 0.022359634, 0.0009947609, 0.049833536, 0.0114469705, -0.032861754, -0.02959024, -0.0015399959, -0.05402549, 0.05530177, 0.015710391, 0.03595735, 0.027289024, 0.029965771, 0.04055756, 0.0029503899, -0.005266243, 0.0030028776, 0.0071634348, 0.04538762, 0.049347904, -0.015955772, -0.062601514, 0.067511946, 0.029928654, 0.03064819, 0.0029126976, 0.03057243, 0.00075364724, 0.042338096, -0.056809414, -0.039979856, 0.0105437655, 0.0017514509, -0.0028917133, 0.031634856, 0.056851525, -0.02545506, 0.022183077, -0.04016901, 0.04203518, -0.04567386, 0.0187635, -0.0044601383, -0.0016370454, -0.035022903, -0.041969806, -0.019490797, 0.0013107033, -0.03515651, 0.04041892, 0.041211855, -0.028916428, 0.0097486675, -0.074606314, -0.0124454, 0.052547466, -0.03559879, 0.020091891, -0.022724787, -0.038114015, 0.0031245106, -0.0774851, -0.015679006, -0.010800067, 0.06883592, 0.03027149, 0.01054037, -0.027531167, 0.05569403, 0.02445075, 0.006778526, 0.032070346, 0.024406744, -0.026388343, 0.026256952, 0.013657911, 0.0033166653, -0.011956921, -0.008001697, 0.0012398846, 0.008664794, 0.00026364232, 0.032249846, 0.025271378, 0.020545678, -0.05027954, 0.016883511, -0.05055751, 0.021683617, 0.0028808007, -0.055349946, 0.06090184, -0.030928887, 0.028687576, -0.010402127, 0.010436579, 0.056511607, -0.024572268, 0.039187185, 0.023705121, 0.0103588905, -0.049194593, -0.017138315, -0.021975255, -0.012991552, -0.0144973695, -0.007273385, -0.010951911, 0.05306574, -0.028992975, 0.04490458, 0.0706627, -0.021468844, 0.08384227, 0.0024464747, -0.047695883, 0.04412856, 0.0395926, 0.011753302, 0.04267763, -0.035176005, 0.005087352, -0.032833457, -0.043140657, -0.014503534, -0.027333384, -0.013390489, 0.009989366, 0.0062535023, -0.0008539462, 0.0033367546, 0.035294898, -0.016552482, -0.016621163, 0.0451201, 0.019906132, 0.032477338, -0.008088608, 0.0037248645, -0.039254855, -0.05092676, -0.008456008, 0.022503003, -0.02749856, 0.051825646, 0.0050135306, -0.04318845, 0.0127239665, -0.01635061, 0.018468944, -0.028020699, -0.034837008, -0.027591588, 0.02043802, 0.029905833, 0.015565619, -0.0016084538, 0.031613346, -0.050971173, 0.026760068, 0.0031544203, 0.08156607, 0.065894134, 0.002605813, -0.04328571, -0.0074962457, -0.052665424, 0.024504554, -0.009008196, -0.01755105, -0.01810863, -0.023332309, 0.025014546, -0.025429988, 0.033203058, 0.03552511, 0.009785979, 0.036505137, -0.024241969, 0.051161505, -0.061698135, -0.0011223572, -0.060553398, 0.019704241, 0.052603528, 0.0027479103, -0.024393141, -0.045974184, -0.008040994, 0.052381705, 0.022661056, -0.007445666, -0.069105916, -0.052636035, 0.017420067, -0.013224186, -0.016087066, 0.032283142, 0.027829677, 0.057646375, -0.0039420864, 0.00826999, -0.014620759, -0.01051261, 0.01030656, -0.019519096, -0.038592838, 0.029118076, 0.045507446, 0.0015663528, -0.002924109, -0.0038470838, -0.051037088, 0.02003933, -0.0025676736, 0.07203921, -0.016591543, -0.023216303, 0.0030184796, 0.059916485, 0.021575691, 0.026520768, -0.040951476, -0.026574256, -0.010098182, 0.05037309, 0.017468335, 0.030730415, 0.006693321, 0.028419703, -0.03292437, 0.004449603, 0.03376725, -0.0556572, 0.022465045, -0.05717204, 0.017095873, 0.037882444, -0.0028529079, 0.0054775225, -0.022074105, 0.010838474, -0.020322071, -0.013137039, -0.03960725, 0.036805443, 0.0021463328, 0.022376683, -0.021660281, -0.0039297203, -0.035549138, -0.028352516, -0.02650382, 0.012722275, -0.034348402, -0.008140685, -0.0057261297, -0.020385774, 0.06632164, -0.08914319, -0.03736262, 0.02971116, 0.039381716, -0.014523927, -0.020059872, -0.027284306, -0.03282579, 0.016997563, -0.009938584, 0.007927991, 0.036247615, -7.1026974e-05, -0.03088833, 0.017336147, 0.013314982, 0.088005826, -0.02536887, -0.046423346, 2.2012364e-05, -0.037245605, 0.021996215, 0.052802585, 0.022015918, 0.0003847738, 0.049799547, 0.03087886, 0.036661543, 0.00396316, -0.032243915, 0.03523323, -0.021871312, 0.033458102, -0.03721156, 0.016681066, 0.002004322, 0.03110206, 0.048020765, 0.05999553, 0.026081514, 0.013381014, -0.018970365, 0.009346412, -0.021851026, -0.0028419138, 0.04654381, 0.053420138, 0.004900875, -0.046853367, 0.07042363, -0.016378341, -0.021075714, 0.06120991, -0.012891498, 0.10367749, 0.0053197355, 0.005364929, -0.0047573135, -0.0016308035, 0.010284356, 0.014277921, -0.0027932723, -0.04946853, 0.016846694, -0.021916704, -0.042728573, -0.0029173642, -0.04047885, 0.01264742, 0.045968823, -0.062334716, 0.01753235, 0.018991588, -0.027581563, 0.014126079, -0.038485296, -0.003980714, 0.0032353722, -0.00063856045, 0.0043773204, 0.026311364, -0.08313322, -0.057053432, -0.042288996, -0.022141954, 0.05191648, -0.009650679, -0.0067128763, -0.01891372, -0.0037798195, 0.034956135, -0.015250012, 0.004362612, -0.049305707, -0.109354265, 0.012233742, -0.04974947, 0.029045207, 0.06946122, -0.0019696085, 0.028771816, 0.05290161, -0.008681634, -0.030088965, 0.018187704, -0.03648611, 0.03172905, -0.01644292, -0.093974434, 0.04369892, -0.00063096965, 0.02117522, -0.053216707, 0.025666805, 0.037473064, 0.0018728205, 0.025437701, 0.061561726, -0.05740715, -0.004681616, 0.005718122, 0.0024691715, 0.016273847, 0.0049560335, -0.037791464, -0.023783974, 0.00975221, -0.05281188, 0.04494464, -0.018584007, 0.00890194, 0.042308655, -0.021861587, -0.01882029, 0.008761653, -0.0154734785, -0.07433141, -0.020642603, 0.053181738, 0.00905685, 0.052382857, -0.0107909795, 0.013598955, 0.035115343, -0.030893216, -0.021208933, 0.0473682, 0.028403902, 0.0030819124, -0.07055047, -0.004091761, -0.0700075, 0.043648686, -0.00569325, 0.01323642, -0.050381765, -0.062038526, -0.054168686, -0.0369089, -0.084655754, -0.060018055, 0.03850448, 0.08915763, 0.010320593, -0.035815652, -0.02048647, -0.013801284, -0.01401505, -0.005511624, 0.043202206, 0.039107025, -0.050761707, 0.008217404, -0.0042036, 0.022098405, -0.049585354, 0.058750533, 0.025634563, -0.0472218, -0.058874674, -0.026083719, -0.017103303, 0.023111524, 0.029295905, -0.02972747, 0.0153967235, -0.08047701, 0.0020412377, -0.004193135, -0.016969819, -0.0077392138, -0.037386872, 0.045509823, -0.029035436, -0.012458722, -0.037968017, -0.008912333, 0.032542955, -0.032878987, -0.070001215, -0.04834059, -0.004776309, 0.059418235, -0.04361501, 0.0029637758, 0.04949833, -0.0037189098, 0.031845562, -0.021980481, -0.013673352, -0.019943312, -0.025416812, -0.04636924, 0.034326185, -0.0018501651, -0.043898337, 0.047680072, -0.0037687449, -0.0075373044, 0.04883902, -0.018265998, -0.045600362, 0.060689937, -0.018567514, -0.041640326, 0.0013874022, -0.08135599, 0.007033196, 0.011508154, 0.0005527275, 0.013837514, -0.0021204273, -0.07753571, 0.0059175193, -0.06989017, 0.017112106, 0.0051033744, 0.036275297, -0.024784762, 0.034532078, -0.0016155073, -0.059399422, -0.0007402882, 0.014979668, 0.006241688, -0.023298565, 0.047170613, 0.05997057, -0.004926975, -0.004793708, 0.043646727, 0.09513702, -0.0024915482, -0.03489377, -0.009550457, 0.033953186, -0.011540411, -0.011295288, -0.058169305, 0.00968308, 0.026453093, 0.008764442, -0.03323504, -0.011604743, 0.031192856, -0.014880704, -0.044828787, 0.008504293, -0.03208451, -0.014863761, 0.04842829, 0.013421447, 0.012526397, -0.029346233, 0.04589306, 0.04200934, 0.059606113, -0.007713268, -0.037214242, -0.019959847, 0.005825206, -0.00544509, 0.0060450207, -0.023222782, -0.015858553, -0.032759614, 0.021087913, -0.0009628974, -0.05705563, -0.020704227, -0.005781664, -0.062192846, -0.028165083, 0.039718132, -0.024059981, -0.027256196, 0.03271964, -0.046061907, -0.031475503, 0.021125574, 0.009584549, 0.035364226, 0.025721218, -0.015655976, -0.0047753192, 0.023043545, 0.044197924, -0.0015952247, -0.00042510135, -0.06531354, -0.031048073, 0.0050663184, 0.003719242, 0.058673803, -0.021438899, 0.030909436, 0.016254451, -0.031744502, -0.04438506, 0.02042203, 0.022463718, -0.00017499768, -0.048456267, 0.0030261767, 0.0029350722, -0.044522017, 0.020217692, -0.034195103, 0.013272755, -0.010884235, 0.018661242, -0.02707772, -0.038878534, 0.0031282916, -0.026155647, -0.017204281, -0.031588905, -0.039097626, -0.049950115, -0.03154943, -0.009163923, 0.009620372, -0.00062157825, 0.00044926623, -0.0093606645, -0.011749718, -0.036037426, 0.07405514, 0.02673053, -0.0514461, -0.0057400377, -0.028339798, -0.006282165, -0.021388648, -0.07895443, 0.043562513, -0.0119749205, 0.03589391, 0.116681345, -0.0063590626, 0.050228223, -0.030876128, 0.06656666, -0.02925906, -0.008811422, 0.0028009356, -0.042714633, -0.024262495], "index": 5, "object": "embedding"}], "model": "nomic-embed-text:v1.5", "object": "list", "usage": {"prompt_tokens": 7055, "total_tokens": 7055}}, "input": {"input": ["Chapter 1\nIntroduction\n1.1\nBackground of Large Language Models (LLMs)\nLarge Language Models (LLMs) represent a significant leap in computational systems capable of under-\nstanding and generating human language. Building on traditional language models (LMs) like N-gram\nmodels [1], LLMs address limitations such as rare word handling, overfitting, and capturing complex\nlinguistic patterns. Notable examples, such as GPT-3 and GPT-4 [2], leverage the self-attention mecha-\nnism within Transformer architectures to efficiently manage sequential data and understand long-range\ndependencies. Key advancements include in-context learning for generating coherent text from prompts\nand Reinforcement Learning from Human Feedback (RLHF) [3] for refining models using human re-\nsponses. Techniques like prompt engineering, question-answering, and conversational interactions have\nsignificantly advanced the field of natural language processing (NLP) [4].\n1.2\nHistorical Development and Key Milestones\nLanguage models are fundamental to natural language processing (NLP), leveraging mathematical tech-\nniques to generalise linguistic rules and knowledge for tasks involving prediction and generation. Over\nseveral decades, language modelling has evolved from early statistical language models (SLMs) to to-\ndays advanced large language models (LLMs). This rapid advancement has enabled LLMs to process,\ncomprehend, and generate text at a level comparable to human capabilities [5, 6].\nFigure 1.1 shows the evolution of large language models from early statistical approaches to current\nadvanced models.\n1.3\nEvolution from Traditional NLP Models to State-of-the-Art\nLLMs\nUnderstanding LLMs requires tracing the development of language models through stages such as Statis-\ntical Language Models (SLMs), Neural Language Models (NLMs), Pre-trained Language Models (PLMs),\nand LLMs.\n1.3.1\nStatistical Language Models (SLMs)\nEmerging in the 1990s, SLMs analyse natural language using probabilistic methods to determine the\nlikelihood of sentences within texts.\nFor instance, the probability P(S) of the sentence I am very\nhappy is given by:\nP(S) = P(1, 2, 3, 4) = P(I, am, very, happy)\n(1.1)\nThis probability can be calculated using conditional probabilities:\nP(I, am, very, happy) = P(I)  P(am | I)  P(very | I, am)  P(happy | I, am, very)\n(1.2)\nConditional probabilities are estimated using Maximum Likelihood Estimation (MLE):\n6\nFigure 1.1: A chronological timeline showcasing the evolution of Large Language Models (LLMs) from\n1990 to 2023. This progression begins with early statistical models such as N-grams, transitions through\nneural language models like Word2Vec and RNN/LSTM, and advances into the era of pre-trained mod-\nels with the introduction of transformers and attention mechanisms. The figure highlights significant\nmilestones, including the development of BERT, GPT series, and recent innovations such as GPT-4 and\nChatGPT, demonstrating the rapid advancements in LLM technology over time. (adapted from [6])\nP(i | 12    i1) =\nC(12    i)\nC(12    i1)\n(1.3)\n1.3.2\nNeural Language Models (NLMs)\nNLMs leverage neural networks to predict word sequences, overcoming SLM limitations. Word vectors\nenable computers to understand word meanings. Tools like Word2Vec [7] represent words in a vector\nspace where semantic relationships are reflected in vector angles. NLMs consist of interconnected neurons\norganised into layers, resembling the human brains structure. The input layer concatenates word vectors,\nthe hidden layer applies a non-linear activation function, and the output layer predicts subsequent words\nusing the Softmax function to transform values into a probability distribution.\nFigure 1.2 illustrates the structure of Neural Language Models, highlighting the layers and connections\nused to predict subsequent words.\n1.3.3\nPre-trained Language Models (PLMs)\nPLMs are initially trained on extensive volumes of unlabelled text to understand fundamental language\nstructures (pre-training). They are then fine-tuned on a smaller, task-specific dataset. This pre-training\nand fine-tuning paradigm, exemplified by GPT-2 [8] and BERT [9], has led to diverse and effective model\narchitectures.\n1.3.4\nLarge Language Models (LLMs)\nLLMs like GPT-3, GPT-4, PaLM [10], and LLaMA [11] are trained on massive text corpora with tens of\nbillions of parameters. LLMs undergo a two-stage process: initial pre-training on a vast corpus followed\n7\nFigure 1.2: A schematic representation of Neural Language Models, showcasing the layered architecture\nwhere the input layer processes sequential data, the hidden layer captures dependencies, and the output\nlayer generates predictions. The figure emphasises the flow of information through concatenation and\nmatrix multiplications, culminating in a probability distribution via the softmax function. (adopted from\n[6])\nby alignment with human values. This approach enables LLMs to understand human commands and\nvalues better.\n1.4\nOverview of Current Leading LLMs\nLLMs are powerful tools in NLP, capable of performing tasks such as translation, summarisation, and\nconversational interaction. Advances in", " predictions. The figure emphasises the flow of information through concatenation and\nmatrix multiplications, culminating in a probability distribution via the softmax function. (adopted from\n[6])\nby alignment with human values. This approach enables LLMs to understand human commands and\nvalues better.\n1.4\nOverview of Current Leading LLMs\nLLMs are powerful tools in NLP, capable of performing tasks such as translation, summarisation, and\nconversational interaction. Advances in transformer architectures, computational power, and extensive\ndatasets have driven their success. These models approximate human-level performance, making them\ninvaluable for research and practical implementations. LLMs rapid development has spurred research\ninto architectural innovations, training strategies, extending context lengths, fine-tuning techniques, and\nintegrating multi-modal data. Their applications extend beyond NLP, aiding in human-robot interactions\nand creating intuitive AI systems. This highlights the importance of comprehensive reviews consolidating\nthe latest developments [12].\nFigure 1.3 provides an overview of current leading LLMs, highlighting their capabilities and applications.\n1.5\nWhat is Fine-Tuning?\nFine-tuning uses a pre-trained model, such as OpenAIs GPT series, as a foundation.\nThe process\ninvolves further training on a smaller, domain-specific dataset. This approach builds upon the models\npre-existing knowledge, enhancing performance on specific tasks with reduced data and computational\nrequirements.\nFine-tuning transfers the pre-trained models learned patterns and features to new tasks, improving\nperformance and reducing training data needs. It has become popular in NLP for tasks like text classi-\nfication, sentiment analysis, and question-answering.\n8\nFigure 1.3: Mind map depicting various dimensions of Large Language Models (LLMs), covering aspects\nfrom pre-training and fine-tuning methodologies to efficiency, evaluation, inference, and application do-\nmains. Each dimension is linked to specific techniques, challenges, and examples of models that exemplify\nthe discussed characteristics. This diagram serves as an overview of the multifaceted considerations in\nthe development and deployment of LLMs. (adapted from [13])\n1.6\nTypes of LLM Fine-Tuning\n1.6.1\nUnsupervised Fine-Tuning\nThis method does not require labelled data. Instead, the LLM is exposed to a large corpus of unla-\nbelled text from the target domain, refining its understanding of language. This approach is useful for\nnew domains like legal or medical fields but is less precise for specific tasks such as classification or\nsummarisation.\n1.6.2\nSupervised Fine-Tuning (SFT)\nSFT involves providing the LLM with labelled data tailored to the target task. For example, fine-tuning\nan LLM for text classification in a business context uses a dataset of text snippets with class labels.\nWhile effective, this method requires substantial labelled data, which can be costly and time-consuming\nto obtain.\n9\n1.6.3\nInstruction Fine-Tuning via Prompt Engineering\nThis method relies on providing the LLM with natural language instructions, useful for creating spe-\ncialised assistants. It reduces the need for vast amounts of labelled data but depends heavily on the\nquality of the prompts.\n1.7\nPre-training vs Fine-tuning\nTable 1.1 provides a comparison between pre-training and fine-tuning, highlighting their respective char-\nacteristics and processes.\nAspect\nPre-training\nFine-tuning\nDefinition\nTraining on a vast amount of\nunlabelled text data\nAdapting a pre-trained model to\nspecific tasks\nData Requirement\nExtensive\nand\ndiverse\nunla-\nbelled text data\nSmaller,\ntask-specific labelled\ndata\nObjective\nBuild general linguistic knowl-\nedge\nSpecialise\nmodel\nfor\nspecific\ntasks\nProcess\nData\ncollection,\ntraining\non\nlarge\ndataset,\npredict\nnext\nword/sequence\nTask-specific\ndata\ncollection,\nmodify last layer for task, train\non new dataset, generate output\nbased on tasks\nModel Modification\nEntire model trained\nLast layers adapted for new task\nComputational Cost\nHigh (large dataset,\ncomplex\nmodel)\nLower (smaller dataset,\nfine-\ntuning layers)\nTraining Duration\nWeeks to months\nDays to weeks\nPurpose\nGeneral language understand-\ning\nTask-specific performance im-\nprovement\nExamples\nGPT, LLaMA 3\nFine-tuning LLaMA 3 for sum-\nmarisation\nTable 1.1: A Comparative Overview of Pre-training and Fine-tuning in Large Language Models (LLMs).\nThe table outlines key differences between the pre-training and fine-tuning phases across various aspects\nsuch as definition, data requirements, objectives, processes, model modification, computational costs,\ntraining duration, and their respective purposes, with examples highlighting specific models and tasks.\nPre-training involves extensive training on vast amounts of unlabelled data to build general linguistic\nknowledge, while fine-tuning adapts the pre-trained models to specialised tasks using smaller, labelled\ndatasets, focusing on task-specific performance improvements.\n1.8\nImportance of Fine-Tuning LLMs\n1. Transfer Learning: Fine-tuning leverages the knowledge acquired during pre-training, adapting\nit to specific tasks with reduced computation time and resources.\n2. Reduced Data Requirements: Fine-tuning requires less labelled data, focusing on tailoring\npre-trained features to the target task.\n3. Improved Generalisation: Fine-tuning enhances the models ability to generalise to specific\ntasks or domains, capturing general language features and customising them.\n4. Efficient Model Deployment: Fine-tuned models are more efficient for real-world applications,\nbeing computationally efficient and well-suited for specific tasks.\n5.", " tasks with reduced computation time and resources.\n2. Reduced Data Requirements: Fine-tuning requires less labelled data, focusing on tailoring\npre-trained features to the target task.\n3. Improved Generalisation: Fine-tuning enhances the models ability to generalise to specific\ntasks or domains, capturing general language features and customising them.\n4. Efficient Model Deployment: Fine-tuned models are more efficient for real-world applications,\nbeing computationally efficient and well-suited for specific tasks.\n5. Adaptability to Various Tasks: Fine-tuned LLMs can adapt to a broad range of tasks, per-\nforming well across various applications without task-specific architectures.\n6. Domain-Specific Performance: Fine-tuning allows models to excel in domain-specific tasks by\nadjusting to the nuances and vocabulary of the target domain.\n10\n7. Faster Convergence: Fine-tuning usually achieves faster convergence, starting with weights that\nalready capture general language features.\n1.9\nRetrieval Augmented Generation (RAG)\nA popular method to utilise your own data is by incorporating it into the prompt when querying the LLM\nmodel. This approach, known as Retrieval-Augmented Generation (RAG), involves retrieving relevant\ndata and using it as additional context for the LLM. Instead of depending solely on knowledge from the\ntraining data, a RAG workflow pulls pertinent information, connecting static LLMs with real-time data\nretrieval. With RAG architecture, organisations can deploy any LLM model and enhance it to return\nrelevant results by providing a small amount of their own data (see Figure1.4 for visual workflow). This\nprocess avoids the costs and time associated with fine-tuning or pre-training the model.\nFigure 1.4: An illustration of the Traditional Retrieval-Augmented Generation (RAG) pipeline steps,\ndepicting the sequential process from client query to response generation.\nThe pipeline starts with\nthe clients question, followed by semantic search in a vector database, contextually enriching the data\nbefore generating a prompt for the large language model (LLM). The final response is post-processed\nand returned to the client.\n1.9.1\nTraditional RAG Pipeline and Steps\n1. Data Indexing: Organise data efficiently for quick retrieval. This involves processing, chunking,\nand storing data in a vector database using indexing strategies like search indexing, vector indexing,\nand hybrid indexing.\n2. Input Query Processing: Refine user queries to improve compatibility with indexed data. This\ncan include simplification or vector transformation of queries for enhanced search efficiency.\n3. Searching and Ranking: Retrieve and rank data based on relevance using search algorithms\nsuch as TF-IDF, BM25, and deep learning models like BERT to interpret the querys intent and\ncontext.\n4. Prompt Augmentation: Incorporate relevant information from the search results into the origi-\nnal query to provide the LLM with additional context, enhancing response accuracy and relevance.\n11\n5. Response Generation: Use the augmented prompt to generate responses that combine the LLMs\nknowledge with current, specific data, ensuring high-quality, contextually grounded answers.\n1.9.2\nBenefits of Using RAG\n Up-to-Date and Accurate Responses: Enhances the LLMs responses with current external\ndata, improving accuracy and relevance.\n Reducing Inaccurate Responses: Grounds the LLMs output in relevant knowledge, reducing\nthe risk of generating incorrect information.\n Domain-Specific Responses: Delivers contextually relevant responses tailored to an organisa-\ntions proprietary data.\n Efficiency and Cost-Effectiveness: Offers a cost-effective method for customising LLMs without\nextensive model fine-tuning.\n1.9.3\nChallenges and Considerations in Serving RAG\n1. User Experience: Ensuring rapid response times suitable for real-time applications.\n2. Cost Efficiency: Managing the costs associated with serving millions of responses.\n3. Accuracy: Ensuring outputs are accurate to avoid misinformation.\n4. Recency and Relevance: Keeping responses and content current with the latest data.\n5. Business Context Awareness: Aligning LLM responses with specific business contexts.\n6. Service Scalability: Managing increased capacity while controlling costs.\n7. Security and Governance: Implementing protocols for data security, privacy, and governance.\n1.9.4\nUse Cases and Examples\n1. Question and Answer Chatbots: Integrate LLMs with chatbots to generate accurate answers\nfrom company documents, enhancing customer support.\n2. Search Augmentation: Enhance search engines with LLM-generated answers for more accurate\ninformational queries.\n3. Knowledge Engine: Use LLMs to answer questions related to internal functions, such as HR\nand compliance, using company data.\n1.9.5\nConsiderations for Choosing Between RAG and Fine-Tuning\nWhen considering external data access, RAG is likely a superior option for applications needing to access\nexternal data sources. Fine-tuning, on the other hand, is more suitable if you require the model to ad-\njust its behaviour, and writing style, or incorporate domain-specific knowledge. In terms of suppressing\nhallucinations and ensuring accuracy, RAG systems tend to perform better as they are less prone to gen-\nerating incorrect information. If you have ample domain-specific, labelled training data, fine-tuning can\nresult in a more tailored model behaviour, whereas RAG systems are robust alternatives when such data\nis scarce. RAG systems provide an advantage with dynamic data retrieval capabilities for environments\nwhere data frequently updates or changes. Additionally, it is crucial to ensure the transparency and\ninterpret ability of the models decision-making process. In that case, RAG systems offer insight that is\ntypically not available in models that are solely fine-tuned. Figure1.5 illustrates the visual representation\nalong", " can\nresult in a more tailored model behaviour, whereas RAG systems are robust alternatives when such data\nis scarce. RAG systems provide an advantage with dynamic data retrieval capabilities for environments\nwhere data frequently updates or changes. Additionally, it is crucial to ensure the transparency and\ninterpret ability of the models decision-making process. In that case, RAG systems offer insight that is\ntypically not available in models that are solely fine-tuned. Figure1.5 illustrates the visual representation\nalongside example use cases.\n12\nFigure 1.5: Graph comparing the model adaptation required versus the level of external knowledge needed\nacross different scenarios, highlighting the roles of Retrieval-Augmented Generation (RAG), Fine-Tuning,\nand their hybrid applications in various contexts such as Q&A systems, customer support automation,\nand summarisation tasks. (adapted from [14])\n1.10\nObjectives of the Report\n1.10.1\nGoals and Scope\nThe primary goal of this report is to conduct a comprehensive analysis of fine-tuning techniques for LLMs.\nThis involves exploring theoretical foundations, practical implementation strategies, and challenges. The\nreport examines various fine-tuning methodologies, their applications, and recent advancements.\n1.10.2\nKey Questions and Issues Addressed\nThis report addresses critical questions surrounding fine-tuning LLMs, starting with foundational in-\nsights into LLMs, their evolution, and significance in NLP. It defines fine-tuning, distinguishes it from\npre-training, and emphasises its role in adapting models for specific tasks. Key objectives include en-\nhancing model performance for targeted applications and domains.\nThe report outlines a structured fine-tuning process, featuring a high-level pipeline with visual rep-\nresentations and detailed stage explanations. It covers practical implementation strategies, including\nmodel initialisation, hyperparameter definition, and fine-tuning techniques such as Parameter-Efficient\nFine-Tuning (PEFT) and Retrieval-Augmented Generation (RAG). Industry applications, evaluation\nmethods, deployment challenges, and recent advancements are also explored.\n1.10.3\nOverview of the Report Structure\nThe rest of the report provides a comprehensive understanding of fine-tuning LLMs. The main chapters\ninclude an in-depth look at the fine-tuning pipeline, practical applications, model alignment, evaluation\nmetrics, and challenges. The concluding sections discuss the evolution of fine-tuning techniques, highlight\nongoing research challenges, and provide insights for researchers and practitioners.\n13\nChapter 2\nSeven Stage Fine-Tuning Pipeline\nfor LLM\nFine-tuning a Large Language Model (LLM) is a comprehensive process divided into seven distinct\nstages, each essential for adapting the pre-trained model to specific tasks and ensuring optimal per-\nformance. These stages encompass everything from initial dataset preparation to the final deployment\nand maintenance of the fine-tuned model. By following these stages systematically, the model is refined\nand tailored to meet precise requirements, ultimately enhancing its ability to generate accurate and\ncontextually appropriate responses. The seven stages include Dataset Preparation, Model Initialisation,\nTraining Environment Setup, Fine-Tuning, Evaluation and Validation, Deployment, and Monitoring and\nMaintenance.\nFigure 2.1 illustrates the comprehensive pipeline for fine-tuning LLMs, encompassing all necessary stages\nfrom dataset preparation to monitoring and maintenance.\n2.1\nStage 1: Dataset Preparation\nFine-tuning a Large Language Model (LLM) starts with adapting the pre-trained model for specific tasks\nby updating its parameters using a new dataset. This involves cleaning and formatting the dataset to\nmatch the target task, such as instruction tuning, sentiment analysis, or topic mapping. The dataset is\ncomposed of < input, output > pairs, demonstrating the desired behaviour for the model.\nFor example, in instruction tuning, the dataset may look like:\n###Human: $<Input Query>$\n###Assistant: $<Generated Output>$\nHere, the Input Query is what the user asks, and the Generated Output is the models response. The\nstructure and style of these pairs can be adjusted based on the specific needs of the task.\n2.2\nStage 2: Model Initialisation\nModel initialisation is the process of setting up the initial parameters and configurations of the LLM\nbefore training or deploying it. This step is crucial for ensuring the model performs optimally, trains\nefficiently, and avoids issues such as vanishing or exploding gradients.\n2.3\nStage 3: Training Environment Setup\nSetting up the training environment for LLM fine-tuning involves configuring the necessary infrastructure\nto adapt a pre-existing model for specific tasks. This includes selecting relevant training data, defining the\nmodels architecture and hyperparameters, and running training iterations to adjust the models weights\nand biases.\nThe aim is to enhance the LLMs performance in generating accurate and contextually\nappropriate outputs tailored to specific applications, like content creation, translation, or sentiment\nanalysis. Successful fine-tuning relies on careful preparation and rigorous experimentation.\n14\nFigure 2.1: A comprehensive pipeline for fine-tuning Large Language Models (LLMs), illustrating the\nseven essential stages: Dataset Preparation, Model Initialisation, Training Environment Setup, Fine-\nTuning, Evaluation and Validation, Deployment, and Monitoring and Maintenance. Each stage plays\na crucial role in adapting the pre-trained model to specific tasks and ensuring optimal performance\nthroughout its lifecycle.\n2.4\nStage 4: Partial or Full Fine-Tuning\nThis stage involves updating the parameters of the LLM using a task-specific dataset. Full fine-tuning up-\ndates all parameters of the model, ensuring comprehensive adaptation to the new task. Alternatively, Half\nfine-tuning (HFT) [15] or Parameter-Efficient Fine-Tuning (PEFT) approaches, such as using adapter\nlayers", " model to specific tasks and ensuring optimal performance\nthroughout its lifecycle.\n2.4\nStage 4: Partial or Full Fine-Tuning\nThis stage involves updating the parameters of the LLM using a task-specific dataset. Full fine-tuning up-\ndates all parameters of the model, ensuring comprehensive adaptation to the new task. Alternatively, Half\nfine-tuning (HFT) [15] or Parameter-Efficient Fine-Tuning (PEFT) approaches, such as using adapter\nlayers, can be employed to partially fine-tune the model. This method attaches additional layers to the\npre-trained model, allowing for efficient fine-tuning with fewer parameters, which can address challenges\nrelated to computational efficiency, overfitting, and optimisation.\n2.5\nStage 5: Evaluation and Validation\nEvaluation and validation involve assessing the fine-tuned LLMs performance on unseen data to ensure\nit generalises well and meets the desired objectives. Evaluation metrics, such as cross-entropy, measure\nprediction errors, while validation monitors loss curves and other performance indicators to detect issues\nlike overfitting or underfitting.\nThis stage helps guide further fine-tuning to achieve optimal model\nperformance.\n15\n2.6\nStage 6: Deployment\nDeploying an LLM means making it operational and accessible for specific applications. This involves\nconfiguring the model to run efficiently on designated hardware or software platforms, ensuring it can\nhandle tasks like natural language processing, text generation, or user query understanding. Deployment\nalso includes setting up integration, security measures, and monitoring systems to ensure reliable and\nsecure performance in real-world applications.\n2.7\nStage 7: Monitoring and Maintenance\nMonitoring and maintaining an LLM after deployment is crucial to ensure ongoing performance and\nreliability.\nThis involves continuously tracking the models performance, addressing any issues that\narise, and updating the model as needed to adapt to new data or changing requirements.\nEffective\nmonitoring and maintenance help sustain the models accuracy and effectiveness over time.\n16\nChapter 3\nStage 1: Data Preparation\n3.1\nSteps Involved in Data Preparation\n3.1.1\nData Collection\nThe first step in data preparation is to collect data from various sources. These sources can be in any\nformat such as CSV, web pages, SQL databases, S3 storage, etc. Python provides several libraries to\ngather the data efficiently and accurately. Table 3.1 presents a selection of commonly used data formats\nalong with the corresponding Python libraries used for data collection.\n3.1.2\nData Preprocessing and Formatting\nData preprocessing and formatting are crucial for ensuring high-quality data for fine-tuning. This step\ninvolves tasks such as cleaning the data, handling missing values, and formatting the data to match the\nspecific requirements of the task. Several libraries assist with text data processing and Table 3.2 contains\nsome of the most commonly used data preprocessing libraries in python.\n3.1.3\nHandling Data Imbalance\nHandling imbalanced datasets is crucial for ensuring balanced performance across all classes. Several\ntechniques and strategies are employed:\n1. Over-sampling and Under-sampling:\nTechniques like SMOTE (Synthetic Minority Over-\nsampling Technique) generate synthetic examples to achieve balance.\nPython Library: imbalanced-learn\nDescription: imbalanced-learn provides various methods to deal with imbalanced datasets, in-\ncluding oversampling techniques like SMOTE.\n2. Adjusting Loss Function: Modify the loss function to give more weight to the minority class,\nsetting class weights inversely proportional to the class frequencies.\n3. Focal Loss: A variant of cross-entropy loss that adds a factor to down-weight easy examples and\nfocus training on hard negatives.\nPython Library: focal loss\nDescription: The focal loss package provides robust implementations of various focal loss func-\ntions, including BinaryFocalLoss and SparseCategoricalFocalLoss.\n4. Cost-sensitive Learning: Incorporating the cost of misclassifications directly into the learning\nalgorithm, assigning a higher cost to misclassifying minority class samples.\n5. Ensemble Methods: Using techniques like bagging and boosting to combine multiple models\nand handle class imbalance.\nPython Library: sklearn.ensemble\nDescription: scikit-learn provides robust implementations of various ensemble methods, including\nbagging and boosting.\n17\nData Format\nPython\nLi-\nbrary\nDescription\nLibrary Link\nCSV Files\npandas\npandas is a powerful library for data ma-\nnipulation and analysis. It provides the\nread csv function for easy and efficient\nreading of CSV files into DataFrame ob-\njects.\nIt also supports reading data in\nExcel, JSON, and more.\npandas documenta-\ntion\nWeb Pages\nBeautifulSoup\nand requests\nBeautifulSoup is a library for parsing\nHTML and XML documents. Combined\nwith requests for sending HTTP re-\nquests, it enables data extraction from\nweb pages, essential for web scraping\ntasks.\nBeautifulSoup\ndocumentation,\nrequests documen-\ntation\nSQL Databases\nSQLAlchemy\nSQLAlchemy\nis\na\nSQL\ntoolkit\nand\nObject-Relational Mapping (ORM) li-\nbrary for Python, providing a full suite\nof enterprise-level persistence patterns.\nSQLAlchemy docu-\nmentation\nS3 Storage\nboto3\nboto3 is the Amazon Web Services\n(AWS) SDK for Python, allowing devel-\nopers to use services like Amazon S3 and\nEC2. It enables interaction with AWS\nservices, including uploading, download-\ning, and managing S3 bucket files.\nboto3\ndocumenta-\ntion\nData\nIntegra-\ntion\nRapidMiner\nRapidMiner is a comprehensive envi-\nronment for data preparation, machine\nlearning, and predictive analytics, allow-\ning efficient processing and transforma", " Web Services\n(AWS) SDK for Python, allowing devel-\nopers to use services like Amazon S3 and\nEC2. It enables interaction with AWS\nservices, including uploading, download-\ning, and managing S3 bucket files.\nboto3\ndocumenta-\ntion\nData\nIntegra-\ntion\nRapidMiner\nRapidMiner is a comprehensive envi-\nronment for data preparation, machine\nlearning, and predictive analytics, allow-\ning efficient processing and transforma-\ntion of raw data into actionable insights.\nRapidMiner\ndocu-\nmentation\nData Cleaning\nTrifacta\nWran-\ngler\nTrifacta Wrangler focuses on simplify-\ning and automating data wrangling pro-\ncesses, transforming raw data into clean\nand structured formats.\nTrifacta\nWrangler\ndocumentation\nTable 3.1: Python libraries and tools for data collection and integration in various formats, providing\nan overview of commonly used libraries, their functions, and links to their official documentation for\nefficient data management and processing.\n6. Stratified Sampling: Ensuring that each mini-batch during training contains an equal or pro-\nportional representation of each class.\nPython Library: sklearn.model selection.StratifiedShuffleSplit\nDescription: scikit-learn offers tools for stratified sampling, ensuring balanced representation\nacross classes.\n7. Data Cleaning: Removing noisy and mislabelled data, which can disproportionately affect the\nminority class.\nPython Library: pandas.DataFrame.sample\nDescription: pandas provides methods for sampling data from DataFrames, useful for data clean-\ning and preprocessing.\n8. Using Appropriate Metrics: Metrics like Precision-Recall AUC, F1-score, and Cohens Kappa\nare more informative than accuracy when dealing with imbalanced datasets.\nPython Library: sklearn.metrics\nDescription: scikit-learn offers a comprehensive set of tools for evaluating the performance of\nclassification models, particularly with imbalanced datasets.\n18\nLibrary Name\nData Preprocessing Options\nLink\nspaCy\nspaCy provides robust capabilities for text prepro-\ncessing, including tokenization, lemmatization, and\nefficient sentence boundary detection.\nspaCy documentation\nNLTK\nNLTK offers a comprehensive set of tools for data\npreprocessing, such as tokenization, stemming, and\nstop word removal.\nNLTK documentation\nHuggingFace\nHuggingFace provides extensive capabilities for\ntext preprocessing through its transformers library,\nincluding functionalities for tokenization and sup-\nport for various pre-trained models.\nHuggingFace documentation\nKNIME\nKNIME Analytics Platform allows visual workflow\ndesign for data integration, preprocessing, and ad-\nvanced manipulations like text mining and image\nanalysis.\nKNIME documentation\nTable 3.2: Outline of Python libraries commonly used for text data preprocessing, including spaCy,\nNLTK, HuggingFace, and KNIME. It details the specific preprocessing options offered by each library\nand provides links to their official documentation for users seeking more in-depth guidance on their use.\n3.1.4\nSplitting Dataset\nSplitting the dataset for fine-tuning involves dividing it into training and validation sets, typically using\nan 80:20 ratio. Different techniques include:\n1. Random Sampling: Selecting a subset of data randomly to create a representative sample.\nPython Library: sklearn.model selection.train test split\n2. Stratified Sampling: Dividing the dataset into subgroups and sampling from each to maintain\nclass balance.\nPython Library: sklearn.model selection.StratifiedShuffleSplit\n3. K-Fold Cross Validation: Splitting the dataset into K folds and performing training and vali-\ndation K times.\nPython Library: sklearn.model selection.KFold\n4. Leave-One-Out Cross Validation: Using a single data point as the validation set and the rest\nfor training, repeated for each data point.\nPython Library: sklearn.model selection.LeaveOneOut\nFurther details can be found in scikit-learns documentation on model selection.\n3.2\nExisting and Potential Research Methodologies\n3.2.1\nData Annotation\nData annotation involves labelling or tagging textual data with specific attributes relevant to the models\ntraining objectives.\nThis process is crucial for supervised learning tasks and greatly influences the\nperformance of the fine-tuned model. Recent research highlights various approaches to data annotation:\n Human Annotation: Manual annotation by human experts remains a gold standard due to its\naccuracy and context understanding. However, it is time-consuming and costly for large datasets\n[16]. Tools like Excel, Prodigy1, and Innodata2 facilitate this process.\n Semi-automatic Annotation: Combining machine learning algorithms with human review to\ncreate labelled datasets more efficiently. This approach balances efficiency and accuracy. Tools\nlike Snorkel3 use weak supervision to generate initial labels, which are then refined by human\nannotators [17].\n1https://prodi.gy\n2https://innodata.com/\n3https://snorkel.ai/\n19\n Automatic Annotation: Fully automated annotation leverages machine learning algorithms to\nlabel data without human intervention, offering scalability and cost-effectiveness.\nServices like\nAmazon SageMaker Ground Truth4 utilise machine learning to automate data labelling, al-\nthough the accuracy may vary depending on the complexity of the task [18].\n3.2.2\nData Augmentation\nData Augmentation (DA) techniques expand training datasets artificially to address data scarcity and\nimprove model performance. Advanced techniques often used in NLP include:\n Word Embeddings: Using word embeddings like Word2Vec and GloVe to replace words with\ntheir semantic equivalents, thereby generating new data instances [19, 20].\n Back Translation: Translating text to another language and then back to the original language\nto create paraphrased data. This technique helps in generating diverse training samples [21]. Tools\nlike"], "parameters": {"model": "nomic-embed-text:v1.5"}}, "key": "embeddings_0e987fa1e861c68f443aa64cb6a757271c4c2f5f47d28e49cc168cd95a7eff9b_v2"}