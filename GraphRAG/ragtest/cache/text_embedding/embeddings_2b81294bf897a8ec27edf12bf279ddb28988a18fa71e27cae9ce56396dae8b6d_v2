{"result": {"data": [{"embedding": [0.020792764, 0.04533378, -0.1611064, -0.07129945, 0.10327246, -0.04934232, 0.022460561, 0.027694978, -0.057889804, 0.02002441, 0.0017295491, 0.023268119, 0.055601228, 0.04372132, 0.029456364, 0.026371723, -0.032312676, -0.079695724, 0.03207812, -0.0014337227, 0.01080838, -0.030168649, -0.033921037, -0.014933192, 0.05304818, -0.0097976215, -0.05875602, 0.037237514, 0.0008167155, 0.06690249, 0.032447923, -0.056400973, -0.013357469, -0.04846596, -0.042690895, -0.084253184, 0.05643608, 0.022875497, 0.005937382, -0.034853596, 0.048846263, -0.048400044, 0.026116688, -0.037281144, 0.017726377, 0.009502978, 0.070803106, -0.036237966, 0.03807087, -0.060504712, 0.01870737, 0.018868534, 0.01463153, -0.01927581, 0.09641633, 0.041562594, -0.059315503, 0.06720344, -0.0030529113, -0.030158237, 0.056325775, 0.040795222, -0.073302895, 0.06318198, -0.024853067, -0.019888012, -0.034633435, 0.0045925193, -0.029650267, -0.034450304, 0.008655077, -0.0329447, 0.006355774, -0.0045674024, -0.04308447, -0.016104784, -0.04078111, -0.017797453, -0.045721818, -0.0144195855, 0.017976351, -0.020357318, 0.05258171, -0.055030968, 0.0023048478, 0.014926428, -0.024861312, -0.009438149, -0.011419599, 0.06880117, -0.038188983, 0.017499873, 0.080730245, 0.018690236, -0.074735954, 0.062571086, 0.0113210855, 0.05927186, 0.0013412931, -0.025866993, 0.0069834623, -0.006058552, -0.0104059875, -0.034283686, -0.007851417, 0.05598977, 0.023712961, -0.00032679617, 0.0067292345, -0.006145499, -0.062673554, 0.01615668, -0.03017274, -0.023781354, -0.07156249, -0.039768483, 0.047369566, -0.004111242, -0.01298003, 0.02732382, 0.0012884097, -0.01478533, -0.025905488, -0.004288672, 0.018228192, 0.040895794, -0.10723301, 0.012691185, -0.0010152069, -0.023698812, 0.0102848075, -0.0069486434, -0.048368067, 0.0112110125, 0.027026089, 0.107017934, -0.031511486, -0.016315615, 0.043654244, -0.016636668, 0.0046374225, 0.0043964125, -0.033450875, 0.0032537964, 0.024104815, -0.07775035, 0.019959308, 0.0056606536, -0.030319832, 0.022018919, -0.024336422, -0.012650611, 0.028821854, 0.050588034, 0.013455595, -0.02247871, 0.031506233, -0.023960423, 0.029600943, -0.023338629, 0.037206154, -0.027115794, -0.02905065, 0.021967081, -0.0029229836, -0.033063103, 0.04785872, 0.006990309, 0.02893169, 0.029120853, -0.06888467, -0.052579716, 0.0024405604, 0.011146843, 0.043493852, 0.004945686, 0.023039084, -0.024945568, -0.022233628, -0.019979285, 0.026339421, -0.04707266, 0.03224862, -0.005623849, -0.028932076, -0.016120877, 0.050243616, -0.0017916013, 0.03688309, -0.0063036727, 0.007998589, 0.02976465, -0.07129934, -0.04964172, -0.05870216, 0.014114399, 0.09860991, -0.00653977, 0.008501788, -0.023082798, -0.02329104, -0.030337036, -0.027412731, 0.029546145, -0.0119418865, 0.056830697, 0.00593794, 0.06881879, -0.0008042821, 0.00585201, 0.043249108, -0.0034844151, 0.04382389, 0.022823391, 0.0028008025, -0.016851904, -0.018736752, -0.010670534, -0.020335242, 0.025269175, 0.06220393, -0.018561998, 0.058555365, 0.03370372, 0.0159507, -0.011528521, -0.019995613, 0.023248715, -0.053080954, -0.0029396547, -0.016582822, -0.05591202, 0.031595707, 0.016479887, 0.007921211, 0.044473927, -0.032016188, 0.035530236, -0.01108395, 0.03689822, 0.02312382, 0.058742452, -0.047966965, -0.0122861685, -0.07128725, 0.0065400205, -0.008451262, -0.028700212, -0.0020183958, 0.030280542, 0.0049456456, 0.01714634, 0.03928351, -0.041979503, 0.02318738, -0.027500363, -0.030924093, 0.027938595, 0.024301657, 0.018626193, 0.005434645, -0.035145197, 0.018377684, 0.0017277361, -0.03924629, 0.020371899, -0.032845743, -0.025637316, 0.009294582, 0.005908132, 0.03416543, 0.021032464, 0.022059368, 0.044442073, 0.037377495, 0.0065459097, 0.03951544, 0.013900846, -0.001742263, 0.042415984, -0.022417948, -0.076215185, -0.03610833, -0.03584371, -0.007983703, 0.06556617, 0.020102803, 0.009201486, 0.022237455, 0.012543053, 0.017517768, -0.00554142, -0.02395745, -0.028482484, -0.029214323, 0.027185496, 0.0460484, -0.0035645652, 0.012352354, -0.035858043, 0.009767954, 0.0063215722, 0.033182178, 0.07093872, 0.033604655, 0.0041390825, -0.01918931, -0.018582482, 0.010200561, -0.019101333, 0.0028526608, 0.013513898, -0.06087478, 0.03246896, -0.017653732, -0.014222404, 0.020317523, -0.033579312, 0.022363996, -0.036418773, -0.016353667, -0.03041109, -0.015792672, -0.07091041, 0.04666751, 0.038251467, 0.0034101156, 0.0076607205, -0.028472193, 0.0073896735, 0.071628824, 0.06533872, -0.003297521, -0.013923785, -0.09638968, -0.028947003, -0.018529413, 0.013218949, 0.04119849, 0.023204897, 0.030463042, -0.0037367062, 0.045591842, -0.043617006, -0.020713408, -0.0048377556, -0.030911554, -0.029335976, 0.048785154, 0.009045785, -0.03143831, -0.01565006, -0.038847458, -0.01615289, 0.05415696, -0.0054417606, 0.017211033, 0.049119942, 0.019559368, -0.006110101, 0.034255233, -0.011074177, 0.023863647, -0.030772906, -0.0035327459, -0.025099365, 0.032053813, 0.01166854, -0.010118673, -0.0026500965, -0.008317944, 0.008951009, -0.006007997, 0.01163256, -0.023118079, 0.02738937, -0.02741515, -0.027683564, -0.014392392, 0.013703074, 0.0024875612, -0.026556948, -0.014990291, -0.020011367, -0.0038501925, -0.0052771964, 0.006773636, -0.027529173, -0.009278269, 0.0016237305, -0.0014206201, -0.03722244, -0.035865363, 0.028270043, -0.0169384, -0.027240345, 0.0176963, -0.017742652, -0.037215274, 0.06052031, -0.037460744, -0.0700238, 0.021763463, 0.023751149, -0.006995179, -0.023592357, -0.03380081, -0.015163167, 0.009330067, 0.02863475, -0.0007676979, 0.06304378, 0.019950358, -0.06493352, 0.011734541, 0.030266378, -0.0066565406, -0.048982587, 0.0036332782, 0.009636704, 0.013042784, 0.058042932, -0.017253784, 0.017410267, -0.0059853834, 0.029752148, 0.04884288, 0.03168767, 0.028739303, -0.0659293, 0.024863204, -0.025426023, 0.061906476, -0.0037192334, -0.00261323, -0.0034030313, -0.004444272, 0.051877856, -0.008129709, 0.046354212, 0.011743444, -0.034379948, -0.04639699, -0.030043868, 0.01659624, 0.06831212, 0.037084766, -0.019634565, -0.07844512, 0.020961603, 0.0071218335, -0.0045649074, 0.0017489014, 0.00849444, 0.062166456, -0.04693472, -0.004190694, 0.037125133, 0.0706129, 0.033857685, 0.014052742, 0.056525346, -0.086868025, 0.053844202, -0.0056445263, -0.0055517307, -0.041969977, -0.02236505, 0.020103006, 0.07097662, -0.047386542, 0.0136410855, 0.074219294, -0.0018609943, -0.0017244044, -0.0020799844, -0.04153769, 0.0023929235, 0.066843055, 0.06227467, 0.008001634, -0.026566586, -0.018896608, -0.013598433, 0.014390915, 0.035645947, 0.030042326, -0.043838628, -0.0056627137, 0.0014236167, 0.045626294, 0.032214712, 0.031386893, -0.0016554745, -0.0056316876, -0.005584575, 0.0043153833, 0.035550587, 0.012513442, 0.016283885, 0.024646824, 0.029567206, -0.0082920045, -0.0010975979, -0.023949254, -0.05533771, 0.043729637, -0.06593225, -0.03260171, 0.038076326, -0.016912563, 0.039291676, -0.015203957, 0.003023458, 0.019210951, -0.040894292, 0.07176418, 0.0090341605, -0.060342487, 0.005480552, 0.03085111, -0.05285375, -0.0073578903, -0.0067042583, -0.058444243, 0.014946363, 0.00074622215, -0.065209, 0.05849612, -0.07935186, 0.034024175, 0.0558482, -0.040008508, 0.022868492, -0.030866398, 0.027084418, -0.041695513, 0.028711677, 0.04313049, 0.011869217, 0.0052002035, 0.031927176, 0.029285528, 0.047796186, 0.075902104, -0.014726572, -0.035411917, 0.06304967, -0.030728363, -0.09514022, 0.011279179, -0.06434227, 0.010169631, -0.053901166, 0.07728575, -0.04985022, -0.027595928, -0.053597633, 0.01415888, 0.01037828, 0.011528198, 0.038274873, 0.04758661, -0.008803759, -0.0031722356, -0.00039685535, 0.018429926, 0.033660717, -0.04998322, 0.057229, 0.0012760763, -0.06517856, 0.020017982, 0.022398815, 0.032513995, -0.018435478, 0.013246017, 0.007636899, -0.067964315, 0.006664224, -0.002373012, -0.008445582, 0.010241181, 0.043481473, -0.009599189, 0.051370878, -0.035061944, -0.016457556, 0.021265628, -0.032672413, -0.0034788712, 0.0044142897, 0.008046635, -0.0734306, 0.01893777, -0.013544992, -0.029135158, -0.055002928, -0.07333638, -0.03577064, 0.019557409, -0.0077949814, 0.043019515, -0.06147395, 0.00974488, 0.050769847, -0.005346821, 0.045791727, -0.026795356, -0.030591015, -0.0018881226, -0.0290928, -0.014776096, -0.036866773, 0.02805498, -0.049148206, 0.03920782, -0.02349121, -0.06406891, -0.0051557445, -0.009668208, -0.030343715, 0.03944476, -0.07573481, 0.004023073, 0.023177477, 0.011582075, -0.032435812, 0.024345035, -0.00082402164, -0.022331534, 0.05089086, -0.011902911, -0.018540004, -0.02847282, 0.06765418, -0.017080259, -0.02925876, -0.017572742, 0.06710366, 0.03560351, 0.0067894855, 0.0043412982, 0.025654968, 0.019063598, 0.010267739, 0.04219634, 0.035942536, 0.042822875, 0.015591185, 0.04654896, 0.074120045, -0.000541454, -0.0065788827, -0.0041080224, 0.02205779, -0.0111548295, -0.015049424, -0.0100604, -0.04542661, -0.0074847084, -0.017499983, -0.0031065252, -0.0062580197, 0.05416062, -0.020893889, -0.026958503, 0.013488791, -0.056628127, -0.03196149, 0.01199559, 0.021313658, -0.038980927, -0.017920032, 0.04409933, 0.013264015, -0.0012597251, 0.035765696, -0.009260887, -0.027144674, 0.021744346, -0.035246104, -0.0037015043, -0.05901552, -0.047198664, -0.04799161, -0.0071655, 0.014207111, -0.017400226, -0.05755669, -0.023459706, -0.009246708, -0.006918572, 0.029912708, -0.01297127, 0.084820904, -0.030458327, -0.015632195, 0.03581233, 0.065253586, 0.007569854, 0.017338598, 0.045881893, 0.06636374, -0.027357271, -0.0022988305, 0.00932971, 0.0331114, -0.005037433, -0.046327136, 0.02341688, -0.035563465, -0.052041344, 0.02928185, 0.019826334, 0.0024212326, -0.06388238, -0.013422371, -0.019361392, 0.0035901521, 0.048722874, -0.017825175, -0.046373628, -0.018684547, -0.004169347, -0.057007473, 0.021927314, -0.034865677, -0.024018386, -0.020893084, -0.0046516154, -0.031270426, -0.06961773, 0.06393837, -0.021801203, 0.009707274, -0.04880957, -0.0060018683, -0.06990289, 0.040807575, -0.050348904, -0.017540189, -0.015129476, 0.0028741944, 0.01661487, -0.008163321, -0.02119309, 0.018395279, 0.009242444, -0.04119411, 0.010975331, -0.010770371, -0.06361568, 0.012285162, -0.013684325, 0.014479555, 0.08141247, 0.027384797, 0.109073706, -0.013322471, 0.022644725, -0.005824879, 0.06766173, 0.035079498, -0.010928482, -0.02154343, 0.015476239, -0.032231968], "index": 0, "object": "embedding"}, {"embedding": [0.008582758, 0.060582828, -0.1870656, -0.051415365, 0.07571324, -0.01484235, 0.038138323, 0.021184003, 0.033706497, -0.020456351, -0.012257289, -0.038547218, 0.10883162, 0.018729717, -0.014630809, 0.027015772, 0.049499128, -0.038742296, 0.0001760522, 0.004007809, 0.0030746448, -0.09544057, -0.020421727, -0.01054279, 0.0070704394, 0.009481879, -0.02034906, 0.021923367, 0.045989472, -0.031437114, 0.06510899, 0.0056332606, -0.01629758, -0.027676832, -0.041064266, -0.052446358, 0.015680885, -0.008527968, -0.07937215, -0.010459145, 0.039417654, -0.066698305, 0.023241285, -0.0050675487, 0.0017041656, -0.019273657, 0.08019669, 0.004274208, 0.05614019, -0.061739262, 0.0118812, 0.018160515, -0.021883493, -0.0066183317, 0.036054377, 0.0019559187, 0.008103691, 0.05075369, -0.01029389, -0.09008832, 0.07291855, 0.019710768, -0.029426113, 0.022519201, -0.027185407, 0.037363667, -0.056747105, 0.0053763255, 0.009995864, -0.058480885, 0.029634438, -0.017886784, 0.013419931, -0.011369343, -0.019946886, -0.0028089827, -0.015743222, -0.046856742, -0.06427331, 0.023413813, 0.044642944, 0.06292898, 0.05235304, -0.0759324, 0.032937147, 0.049237892, -0.032120258, -0.041633192, -0.03485956, 0.057755154, -0.03188032, 0.041291162, -0.0014574472, 0.016509559, -0.03876113, 0.02012238, -0.034087338, -0.040478867, -0.02363335, -0.0026501422, -0.042295504, -0.030390678, -0.027409896, -0.001136642, -0.0012979131, 0.05631551, -0.00023360521, 0.0036658975, 0.011590033, -0.04166283, -0.055843037, 0.013465541, -0.03174773, -0.018842325, -0.060744274, -0.030812861, 0.07870438, -0.008775611, 0.060276773, 0.034650885, 0.030503048, -0.010463173, -0.005389102, -0.02010393, -0.0022241755, -0.02528848, -0.0721431, -0.0046121897, -0.06671314, -0.014142191, 0.032521427, -0.034896277, -0.06586312, 0.03329422, 0.044993598, 0.08531541, -0.0258804, -0.016268896, 0.04325451, 0.09985279, 0.043152776, 0.051813208, -0.0036290267, 0.011230372, 0.009597189, -0.0045666997, 0.038169418, -0.018061265, -0.006644098, 0.0045301113, 0.022694174, 0.048188124, -0.056620535, 0.050195936, 0.020192966, -0.0048650512, -0.005880018, -0.027585959, 0.026107945, -0.019956356, 0.033940114, -0.017107274, -0.040307183, 0.039078567, 0.03298134, -0.033929374, 0.006658737, 0.035931505, 0.022735067, -0.00020802177, -0.012934719, -0.032769635, 0.005039381, 0.03263273, -0.0014503226, -0.0071839397, 0.098829046, -0.030836448, 0.031535283, -0.015727717, 0.024759667, -0.050030038, -0.0118136015, 0.005031009, -0.017625473, -0.0033083097, 0.04634475, -0.047313236, 0.01888425, -0.050967745, 0.027663207, 0.024251357, -0.043459814, -0.020686567, -0.06065278, -0.043266784, 0.04725792, -0.012800353, 0.074654974, -0.0024793027, 0.0027249209, 0.0041537164, -0.05590691, 0.05670057, -0.018225076, 0.05722031, 0.017776564, 0.037167333, 0.013402825, 0.010954702, 0.036606, 0.0096748965, 0.028489646, 0.004952479, 0.035706874, 0.00022540968, 0.025356386, -0.019849002, -0.0005477979, 0.006951223, -0.010785331, -0.038402498, -0.011899763, -0.004761958, 0.045149766, 0.002924776, -0.024815086, -0.015275224, -0.011268227, -0.002082538, -0.015505584, -0.025589691, -0.01334617, -0.012986671, 0.009244, 0.028938008, -0.010453604, 0.010155283, 0.0095102815, 0.01985505, 0.04158531, 0.009792131, -0.004665133, -0.01351741, -0.06357847, -0.013919607, -0.015028823, -0.049535837, 0.007880735, 0.056699026, 0.030877767, 0.013378631, 0.04109634, -0.008787002, 0.031820446, -0.04186203, 0.0044131516, 0.02564336, 0.011913522, -0.054592986, -0.0008062894, -0.05894678, 0.053224884, -0.0089671, 0.004989313, -0.011441422, -0.04969249, -0.008606593, 0.017616514, -0.034101933, 0.043262135, -0.02457124, 0.007988529, 0.043534894, -0.020929525, 0.042980175, 0.03458532, 0.003012101, -0.011041609, 0.014087132, -0.041810907, -0.11405458, 0.011607943, 0.022570278, -0.0044271243, 0.0069982293, -0.01351191, 0.024018839, -0.011260069, -0.018813653, -0.000945029, 0.030476222, -0.0038559325, -0.030114256, -0.020896915, 0.013022537, 0.047250446, -0.028217856, 0.0076247803, -0.047754504, 0.038642727, 0.044349663, 0.07069816, 0.054374013, 0.0048473803, 0.016177917, 0.002691201, 0.037457615, 0.023161298, 0.009624279, 0.0012861196, 0.020700654, -0.061366785, 0.0011721144, -0.02222996, -0.014981402, 0.019665314, 0.016432669, 0.055417363, -0.014739958, -0.007720047, -0.078012116, -0.022112422, -0.044099063, 0.0026961386, 0.086399086, -0.033844046, 0.0046699285, -0.023057703, -0.0038706437, 0.023483329, 0.008775492, 0.07119663, -0.06903856, -0.075238414, 0.013150875, 0.01882605, 0.044942256, 0.02607214, 0.002730058, 0.060887706, -0.0034535457, 0.013098539, -0.018162109, -0.05078309, -0.031467743, -0.05263531, -0.046601493, 0.051649973, 0.026593635, -0.0011627331, -0.026365358, -0.019873183, 0.002154114, -0.011415198, -0.010198272, 0.023472575, 0.040687315, -0.016921613, -0.016560936, 0.03886443, -0.002452543, 0.05727361, -0.03621326, 0.026651904, 0.030123342, 0.054072216, 4.8169106e-05, 0.052915398, -0.005618635, 0.035300896, -0.021718921, -0.045445308, 0.050732035, 0.029168915, 0.011950531, -0.048779693, -0.0022744641, 0.0125930635, 0.010192936, 0.041949715, 0.0031466773, -0.015080683, 0.011586088, -0.0015228731, 0.028623609, 0.047169946, -0.019648163, -0.03089419, -0.014214881, -0.0015577025, -0.0562699, -0.011281052, 0.0089136865, 0.06208601, 0.016082844, 0.033588104, 0.006078696, 0.0039542, 0.07077448, -0.05476084, -0.03665457, -0.005004279, -0.014675317, 0.0066399346, -0.014009987, -0.04194991, -0.025008764, 0.017003406, 0.012832104, 0.042454407, 0.06901372, -0.043971136, -0.053551313, 0.03756023, -0.017030222, 0.067171626, 0.0104895765, -0.05108193, -0.012518321, 0.015897332, -0.011766506, 0.0045884117, 0.040790115, 0.01169369, 0.020879216, 0.013500378, 0.0147852525, -0.03467456, -0.07769192, 0.037543505, 0.0103702415, 0.021030707, 0.004967587, -0.001227292, -0.0089171035, 0.00030833326, 0.0070773503, -0.02195634, -0.003732881, -0.008232603, -0.00021951995, 0.015374677, -0.013827263, -0.0202574, 0.09162734, 0.06418005, -0.013696735, 0.016868422, 0.012065423, 0.03595481, 0.023802934, 0.0559019, 0.05315582, 0.044060286, 0.017010706, 0.031129595, 0.04637452, 0.003551379, -0.027865022, -0.02041253, 0.0013095682, -0.0452494, 0.0064489967, 0.008632828, -0.02430989, -0.04073586, -0.04605587, -0.017873634, 0.07086597, -0.048838727, -0.032163378, -0.00087764644, -0.0051354007, -0.018448845, -0.020985596, 0.036538955, -0.01584091, 0.050278343, 0.042073928, 0.0075253197, -0.0012385667, -0.06701106, -0.035337593, 0.01675498, 0.028169952, 0.07016384, -0.034807317, 0.013974688, 0.03910526, 0.048000306, -0.00071916723, -0.023769028, 0.02632808, -0.0023852917, 0.006063384, -0.0038874059, -0.0006713339, 0.01970653, 0.033563673, 0.026092907, 0.048882406, 0.030827519, 0.027379509, -0.048891995, -0.02167327, 0.008151532, -0.016986564, -0.0020031724, -0.033703443, 0.00036774858, 0.040302016, -0.031791758, 0.044507597, 0.0583009, -0.053456187, -0.0024104726, 0.011439326, -0.05094039, 0.015951416, -0.016408399, -0.041661616, 6.0298687e-05, -0.042257942, -0.05885865, -0.01268234, 0.015199282, -0.020045597, 0.021637805, 0.019493654, 0.007545217, 0.06207815, -0.033431444, 0.0022114739, 0.046586744, 0.0053002927, -0.06890537, 0.043159664, 0.070815064, -0.038583472, -0.023183314, -0.053827815, 0.025681322, -0.03200894, -0.0052986084, -0.026259154, 0.008298892, 0.022204693, 0.029524595, -0.069226995, 0.030423064, -0.032523226, 0.015065144, 0.006186769, 0.03701646, -0.009929071, -0.04499846, -0.034909718, -0.046937097, -0.0054803565, -0.0106521435, 0.017381083, 0.1074649, -0.013185428, -0.033447504, -0.026435932, 0.0068973554, 0.02225173, 0.013283115, 0.059467472, 0.017620858, -0.055038784, 0.0100404825, 0.02358731, -0.025494333, -0.016497675, -0.0012341286, 0.0070726112, -0.041407008, -0.02105803, 0.041186765, -0.028732467, 0.09225192, 0.034332998, -0.0064552804, 0.035273448, -0.06360591, -0.021503588, 0.016264385, -0.047276713, 0.009110195, -0.0062327003, 0.06263892, -0.03080316, 0.004721481, 0.03630152, 0.01638902, 0.057343896, -0.035636608, -0.06561005, -0.04172538, -5.0771163e-05, 0.039090615, -0.0692357, 0.07059237, 0.05508327, -0.0089104865, 0.015328022, 0.05358239, 0.011077156, 0.008533355, -0.041852113, -0.010976266, -0.014980693, 0.011958715, -0.01808056, 0.018769844, -0.0070563303, -0.035502538, 0.03129367, -0.05217652, 0.0033845669, 0.054171186, -0.00020624735, -0.05758393, -0.045623384, -0.010977824, 0.011676788, -0.012371702, 0.0046706297, -0.053848024, -0.006378886, -0.08596735, -0.031781893, -0.02029974, 0.02820026, -0.003508275, -0.025070606, 0.018324519, 0.023144675, 0.02160238, -0.040084038, -0.0087949205, 0.023873195, -0.014762355, 0.006219868, 0.0034321581, 0.040850013, 0.072393194, -0.003304649, 0.06987682, 0.047147114, 0.011781749, -0.044568203, 0.026937721, 0.011337381, -0.035494495, -0.018252753, -0.053461157, 0.009901136, 0.008013746, -0.009106109, 0.019361345, -0.014570072, 0.020647906, -0.0015285091, -0.05221075, 0.017948136, -0.03950992, -0.025410088, 0.016033476, 0.031029802, -0.019636536, -0.014912482, 0.03984868, 0.0134158945, 0.076798394, 0.0058817505, 0.024192749, -0.015813924, 0.031717546, 0.009770903, -0.009773914, -0.045358524, 0.04789162, -0.03574283, 0.013190868, -0.0018935297, -0.03259952, -0.0104099, -0.03280719, -0.017390687, -0.0196719, 0.032525826, -0.028907698, 0.02260226, 0.015686408, -0.07463341, -0.0079098595, 0.029778915, -0.06125525, 0.0036563154, -0.0040505044, -0.012098108, -0.006210615, 0.009392283, 0.0386851, 0.013326361, -0.029435202, -0.056242753, -0.011141092, 0.014621513, -0.050183717, 0.056263648, -0.009112959, 0.042079296, -0.047752704, -0.052308172, 0.018017776, -0.018876042, -0.0022440858, 0.03576033, -0.042368952, -0.015322941, 0.013110456, -0.02364287, 0.029309278, -0.06595836, 0.014284257, -0.020155601, -0.034827527, -0.033606093, -0.009210897, 0.07025189, -0.005101388, 0.02559814, -0.013000293, -0.036378313, -0.08500185, -0.014252068, -0.07535983, 0.0055811326, -0.01579316, -0.0040760655, -0.008332974, -0.05119764, -0.0053076893, 0.07911381, 0.028728358, -0.017210983, 0.008274916, -0.05227967, -0.052079327, -0.06694503, -0.04297323, -0.023692768, 0.0059672417, 0.00928399, 0.09133918, 0.020097483, -0.015641041, -0.023926033, 0.06366386, 0.0029844618, -0.017071437, -0.07646343, -0.01658822, 0.009933037], "index": 1, "object": "embedding"}, {"embedding": [-0.049475074, 0.10754233, -0.13505219, -0.016039414, 0.07347489, 0.050882153, 0.04599789, -0.014887928, 0.029129442, -0.01856043, -0.0054727, -0.014483493, 0.071458854, 0.031795584, -0.009806721, 0.032931775, -0.03687481, -0.034026317, 0.041088916, -0.00072927674, 0.012634075, -0.073835045, 0.006251697, 0.022173889, 0.017018648, -0.0054920455, -0.025304675, 0.056575164, 0.0037216814, 0.009114903, 0.03466728, -0.014603252, 0.01939993, 0.0022914368, -0.06795418, -0.05747289, 0.045753922, 0.015256427, -0.043120373, 0.00039666658, 0.015590479, -0.016873565, 0.018930765, -0.0025484383, 0.025920121, 0.027548287, 0.06816302, -0.013795113, 0.09827647, -0.07102682, 0.043865837, -0.054869033, -0.009387353, 0.01190316, 0.033941902, -0.014859897, 0.046056475, -0.01733601, -0.04102789, -0.10465935, 0.0593878, 0.04797953, 0.007836239, 0.07150179, 0.0059742597, 0.031041257, -0.02290586, 0.01305436, 0.0132204415, -0.040098496, 0.025645407, -0.021366827, 0.009045292, 0.060886405, 0.00023641299, -0.007555748, -0.009659185, -0.036556877, 0.012464505, 0.07839037, 0.090530306, 0.033136, 0.011688181, -0.042600073, 0.05562753, 0.019895704, -0.027307825, -0.071512304, -0.028803041, 0.095713146, -0.015327033, 0.044306498, 0.004146939, 0.0027157129, -0.053448927, 0.039804514, -0.041850906, -0.0059709, -0.03497368, 0.018978737, -0.04659313, 0.0063605495, -0.021349236, -0.018386735, 0.056298327, 0.02474235, -0.015364079, 0.022338657, -0.023328556, -0.009194683, -0.01338999, 0.031785846, -0.009421298, -0.024850855, -0.045867037, -0.0617493, 0.017588202, 0.01585178, 0.065780304, 0.013663417, 0.017587759, -0.041252058, -0.013804133, 0.021522481, 0.006009511, 0.0065618535, -0.08601071, 0.013081681, 0.020853506, -0.07008556, 0.016817177, -0.018199548, -0.0070614005, -0.010354517, -0.010608709, 0.075837396, -0.06404909, -0.020853275, 0.039431076, 0.07028881, 0.016472137, 0.09119399, 0.011910219, 0.021564856, 0.025715945, -0.0323293, 0.010953835, 0.019447561, -0.006951444, 0.044392288, -0.028326796, 0.054499134, -0.015938777, 0.064722806, 0.021228854, 0.013241072, -0.020361563, -0.007837368, 0.028115511, -0.034817874, 0.04817299, 0.008268499, -0.008397842, 0.018262621, 0.025710816, -0.028336171, 0.0011298738, 0.016163526, 0.04357828, 0.017896306, -0.012890843, -0.00486377, 0.03571796, 0.020729916, 0.0045591313, -0.005090007, 0.063397124, -0.055896226, 0.0129377125, -0.06987075, 0.012243426, -0.06455638, -0.0033207755, 0.028109895, -0.010231483, 0.029873349, -0.018742431, -0.025467988, -0.008743113, -0.034806017, 0.011545675, -5.3338255e-05, -0.033037134, -0.018866811, -0.06555365, -0.00017043643, 0.049706116, 0.041199703, 0.06869951, -0.025828736, -0.009457292, 0.025657073, -0.054898236, 0.04564596, -0.027329482, 0.04884064, 0.019711431, 0.04273683, -0.007503952, -0.040544543, 0.075590074, -0.0019053457, -0.0051810914, -0.0406432, 0.012589602, 0.013239907, 0.023566205, -0.043047987, -0.02924948, 0.023898982, -0.002925854, -0.029550761, 0.022780072, 0.0019353663, 0.040739, -0.030169105, -0.042628065, 0.034549057, -0.005321186, 0.0043095304, 0.0034755962, -0.05386461, 0.041172113, 0.009157546, 0.0145780295, 0.056905787, 0.014256537, 0.03737304, -0.040761568, 0.009132882, -0.021676755, 0.010154157, -0.004069934, 0.017623814, -0.036810245, -0.008747418, -0.013763172, -0.026736347, -0.00995868, 0.03261885, 0.00968042, 0.002250294, -0.0035824096, -0.02213169, 0.049323834, 0.0072595365, 0.017870154, 0.028309336, 0.040399004, -0.05236605, 0.021468692, -0.0777046, 0.0637085, 0.008080464, -0.0044804793, -0.0023256326, -0.0326088, 0.012328739, 0.010191425, -0.024420373, -0.02424475, 0.002964544, -0.002569895, 0.028307775, -0.023231788, 0.038098086, 0.024700142, 0.0046537593, 0.0118887415, 0.003219544, -0.07027142, -0.06681831, -0.03158608, 0.011702717, 0.022734625, 0.056086067, -0.004538822, 0.013658444, 0.011397097, 0.0034620764, 0.014767846, -0.01694974, -0.057171654, -0.046087913, -0.01249858, 0.006806041, 0.07597621, -0.022473354, 0.007986061, -0.0141359, 0.029025428, 0.023081284, 0.08288318, 0.042746022, -0.011153874, -0.020270593, 0.0003796317, -0.018305186, 0.031025367, -0.00092838093, -0.06713434, 0.027761944, -0.017516632, -0.03400952, -0.010183153, -0.030453328, 0.023572747, 0.039619986, 0.020876016, 0.007222028, -0.014760306, -0.07737475, -0.042489044, -0.0017046214, -0.0067108665, 0.073353305, -0.012634396, -0.0044602905, -0.064673275, -0.018522969, -0.00012631527, 0.025512353, 0.00976858, -0.022301255, -0.09939433, 0.009210411, -0.0043873023, -0.0065184985, 0.040135887, 0.020186719, 0.05735669, -0.0118373325, -0.013003443, -0.0067933663, -0.07195267, -0.009444113, -0.033221424, -0.0042068483, 0.014807566, 0.01844594, 0.0063090003, -0.020455996, 0.0029023706, -0.008427904, -0.0035538385, -0.021248683, 0.031023161, 0.061698467, -0.02099775, 0.011572312, 0.037530795, -0.01484064, 0.013953566, -0.051336765, 0.012767292, 0.06197235, 0.0018354832, 0.0076932786, 0.03805881, -0.011124796, 0.04754358, -0.01220465, -0.008479467, 0.05028121, -0.024605386, 0.0017074825, -0.07066758, -0.049438816, 0.028363833, 0.0059684305, -0.00015735216, -0.034796767, -0.02389369, -0.0063113044, 0.0146108065, 0.02275743, 0.035082713, -0.020567995, 0.015455112, 0.01595709, -0.0118018985, -0.046534996, -0.053354315, -0.02516158, 0.014755829, -0.00095827936, -0.012614975, -0.011143584, -0.040817782, 0.05969689, -0.05971741, -0.038164385, 0.025785737, -0.0340891, 0.008993013, -0.011896683, -0.038127635, -0.039040018, 0.015366823, -0.029916, 0.029242467, 0.044080213, -0.06665949, -0.06520157, 0.033347297, -0.04795282, 0.027934207, 0.025208287, -0.04781346, -0.018186994, 0.01443272, 0.016855247, 0.021150284, 0.0108634615, 0.035410926, 0.0044909483, -0.008976033, 0.0005653477, -0.040333007, -0.05505407, 0.014477716, 0.0051301275, 0.010683771, -0.029420253, -0.0077795056, 0.017403403, 0.010962754, 0.0056897467, 0.018044792, 0.015831059, 0.027233934, 0.031353593, 0.019299546, -0.06762447, 9.4283445e-05, 0.08136591, 0.04605217, 0.019624135, -0.053994842, 0.02280399, 0.061575104, -0.005806832, 0.044275157, 0.010960547, 0.056247864, -0.0025976237, 0.061029546, 0.0112828035, 0.008242056, 0.014633951, -0.016767884, -0.004176352, -0.048352078, 0.0036313152, -0.009145713, -0.01339097, 0.016583443, -0.015007801, -0.020448783, 0.03649464, -0.01397612, -0.00650147, 0.0096556535, -0.003884663, -0.049224492, -0.011868004, 0.008766528, -0.021970276, 0.054551184, 0.034744572, 0.0016502867, -0.03824833, -0.029649587, -0.04750716, -0.0008139043, 0.048415083, 0.0013610648, 0.005349827, 0.0034928764, 0.009089651, 0.042955, 0.010371028, -0.04524713, -0.011613824, -0.03687486, -0.022133587, -0.015274412, 0.035056435, 0.048279334, 0.0040615182, 0.045517065, -0.012270883, 0.02036745, 0.04944514, -0.025319923, -0.015211609, 0.022000022, -0.036551356, -0.000597792, 0.03267401, -0.00044877786, -0.00084326457, -0.004697533, 0.038021207, 0.044017714, -0.02272533, 0.024842987, 0.019784747, -0.07106673, -0.03329617, 0.00022434391, -0.018338703, -0.0001299913, -0.07635953, -0.07514235, -0.0038905297, 0.024263384, -0.0035539952, -0.009500178, 0.011113018, 0.023836164, 0.051918097, -0.046925303, -0.025457446, 0.061771497, -0.039925046, -0.041763883, -0.0067474972, 0.07853515, 0.023516027, -0.008897051, -0.03925102, -0.004622029, -0.05523114, -0.005422115, -0.014742584, 0.017200265, 0.037482254, 0.041534342, -0.07553541, 0.030329986, -0.073481694, 0.019121345, -0.020579878, -0.000629779, -0.032116294, -0.076084815, -0.008105116, -0.013921008, -0.021904103, 0.001061159, 0.034935288, 0.074219644, -0.036289126, -0.0149121005, -0.03221632, 0.009637766, 0.0026259343, 0.02187927, 0.075322196, -0.005773737, -0.06771752, -0.027200015, 0.021423329, 0.038981244, -0.029495386, -0.01749862, -0.020243239, 0.00034016467, -4.9694536e-06, 0.002434769, -0.06395606, 0.14331411, 0.015288923, 0.04137096, 0.005678891, -0.056630973, -0.01971801, 0.017999837, -0.08664916, -0.043854594, 0.018374166, 0.023019668, -0.049357906, -0.03800664, 0.008650771, 0.002398193, -0.0013981195, -0.025653683, -0.10147292, -0.02253563, -0.02017896, 0.03309917, -0.024355559, 0.036815085, 0.07360527, 0.014243374, 0.0035551842, 0.031765323, 0.0034594622, 0.011633998, 0.0069757663, -0.027277533, -0.0064671244, 0.07179829, -0.025435047, 0.047934093, -0.039637905, -0.004263984, 0.0071875537, -0.048434798, -0.0470197, 0.029485097, -0.012222897, -0.049148742, -0.06344635, -0.052235417, 0.022116408, -0.02181189, 0.10555305, -0.05331306, 0.008197982, -0.018705852, -0.028252635, -0.039830968, 0.026212895, -0.027791198, -0.015097854, -0.026101124, 0.018802369, 0.009721677, -0.02197912, -0.0074628177, 0.07798009, 0.02939706, -0.019908851, 0.051155232, 0.050210305, 0.023493107, 0.012755898, 0.044752635, 0.089899324, 0.030731734, -0.03636034, 0.02167362, 0.005403689, -0.022600567, -0.022230392, -0.0009987694, 0.021995297, 0.021197774, 0.019269716, -0.026559021, 0.012609453, 0.01657658, -0.021786857, -0.0332371, 0.012370195, -0.019927487, -0.0032914039, 0.037585113, 0.023501514, 0.010237594, -0.06523166, 0.042510446, 0.045285463, 0.06517508, -0.016028484, -0.0002064162, -0.028382733, -0.011454698, -0.03592147, 0.00055071287, -0.007669325, 0.041621152, -0.017525895, 0.03985842, 0.0064485003, -0.013366795, -0.036709692, -0.01622014, -0.07022573, 0.011022745, -0.023467975, 0.0017995574, 0.011519302, -0.0014742326, -0.03222282, 0.018872282, -0.004753475, -0.08558731, -0.0009436031, 0.021999765, 0.030244997, 0.0025781486, -0.0016768605, 0.016992971, 0.022510054, -0.031808227, -0.049138147, 0.014526064, 0.022348937, -0.022426696, 0.08849866, -0.008377943, 0.009249886, -0.014233515, -0.032543194, -0.019400425, 0.038148418, 0.009314789, -0.011507233, -0.041921996, -0.017698837, -0.06065142, 0.0023919581, 0.028015537, -0.09294487, 0.008763134, -0.02216427, 0.001341533, -0.0021245824, 0.011809576, 0.04823913, -0.033557404, -0.017459527, -0.033045266, -0.03079918, -0.07183936, 0.005655433, -0.059408985, 0.011156964, -0.044424113, -0.011262263, 0.012962694, -0.013971447, 0.0030250736, 0.07937536, 0.03646362, -0.008365879, -0.0039495807, -0.013761351, -0.04854666, -0.047702223, -0.064640425, 0.05149599, 0.018415801, 0.018538512, 0.07647487, -0.0016611624, 0.014464581, -0.05063517, 0.038770415, -0.011596802, -0.033939317, -0.03163948, -0.034347378, 0.019237317], "index": 2, "object": "embedding"}, {"embedding": [-0.010333589, 0.09159401, -0.14541674, -0.033613347, 0.05826616, -0.023441095, 0.024058985, 0.0036031953, -0.02757775, 0.04297713, -0.050581034, 0.023650264, 0.0783537, 0.008649461, 0.0020029051, 0.02316579, -0.00041132295, -0.022328993, 0.0066362103, 0.008690576, 0.01451357, -0.047893737, 0.0132532045, -0.055549856, 0.044208243, 0.00049401005, -0.0421018, 0.03188352, 0.016748047, 0.041297052, 0.064876236, -0.049437337, 0.0069100205, -0.023648245, -0.021829562, -0.014199278, 0.06277766, -0.012306901, -0.007278675, 0.043063805, 0.04639742, 0.00094328454, 0.029705793, 0.011599911, 0.009117285, 0.031539377, 0.04412648, -0.03329272, 0.03812203, -0.073404506, 0.0050346977, 0.03061571, -0.024305992, 0.019876784, 0.08054556, 0.0313446, -0.048365466, 0.06183543, -0.015691014, -0.04015825, 0.10323447, 0.01698889, -0.036923826, 0.0661155, -0.023657478, -0.024884602, -0.055876732, 0.008687994, 0.02423152, -0.041743252, -0.014040384, 0.012798055, 0.020748302, 0.03748901, -0.04938669, -0.03404128, -0.0025421085, 0.01377776, -0.0680967, 0.025031997, 0.03833588, -0.00039107873, 0.058186844, -0.026603539, 0.03398277, -0.021073349, -0.060045365, -0.0022832055, -0.036843248, 0.102249734, 0.028602673, 0.05578119, 0.013052612, 0.034006372, -0.025525019, 0.029359136, -0.05338354, -0.0021080493, 0.00565454, -0.034902204, -0.045859057, 0.031461496, -0.023321774, -0.07331512, 0.016281705, 0.026802989, 0.0030132511, -0.016621972, -0.019760342, 0.0052154907, -0.0005001639, -0.0068114907, -0.036069013, 0.0032537393, -0.047700644, -0.027790202, 0.0068666064, -0.026153998, 0.0010105934, 0.09939251, -0.005749838, -0.05042786, -0.021571763, -0.00056041504, -0.036189385, 0.0061487365, -0.065451376, 0.01090616, -0.039938252, -0.022271529, 0.020150343, -0.016504716, -0.029075125, 0.048146255, 0.012905386, 0.10026567, -0.06085872, -0.020519918, 0.028212545, 0.026891284, -0.0040178266, 0.024566062, -0.024561193, 0.0043367012, 0.009973056, -0.06901387, 0.014620712, 0.01667363, -0.062329423, 0.017540006, -0.017581208, 0.03438078, -0.010524571, 0.03782949, 0.05737756, -0.0014662778, -0.035106752, 0.0043779467, 0.023734817, -0.047988232, 0.023924118, -0.008652131, -0.015189574, 0.08041693, 0.028429683, 0.0017270934, 0.058791555, 0.05000129, 0.051615518, 0.044947345, -0.0947119, -0.021382095, 0.0026115484, -0.03072452, 0.043538164, 0.0007219282, 0.040815882, -0.065947734, 0.01606451, -0.02488171, 0.032044277, -0.063812815, -0.00589447, 0.030756954, -0.026368693, -0.021058768, -0.022424648, -0.03350257, 0.02342617, -0.025508499, 0.0026536433, 0.049192194, -0.075232774, -0.002630777, -0.02595489, 0.0019942946, 0.06534322, -0.009894938, 0.022589628, -0.011917918, -0.02746056, -0.03404467, -0.0423796, 0.043465413, 0.014334845, 0.047264174, 0.071759105, 0.039929662, -0.01971045, 0.018320384, 0.03235299, 0.018477196, 0.039062098, -0.00017332878, 0.021376206, 0.044278, -0.0064719557, -0.04446426, 0.026335794, 0.013903291, 0.03301781, -0.03438008, 0.028204193, -0.009846981, 0.03832006, 0.0038533292, -0.02552122, 0.023253933, -0.035998225, 0.030361297, -0.033030976, -0.03527291, 0.046398804, -0.0147121595, 0.03497651, 0.019466363, -0.032827113, 0.059303213, -0.013938068, 0.039695237, 0.015472993, 0.05323087, -0.027763506, -0.07511915, -0.07956608, -0.036368586, -0.054854207, -0.028201995, -0.014171268, 0.075512856, -0.026682816, -0.02180809, -0.022625418, 0.027566068, 0.043808196, -0.02827359, 0.00550573, 0.013579152, 0.010262098, 0.010258094, 0.0424919, -0.06332354, 0.0046777837, 0.041255746, 0.024016794, 0.006136597, 0.029516796, -0.025156507, -0.020434793, 0.030059045, 0.021139277, 0.028271815, 0.0048682126, 0.013033832, -0.00197714, 0.039261837, 0.019359047, -0.016409963, 0.01913819, 0.028653843, -0.020921849, -0.08028155, -0.04359885, -0.016984852, 0.015544088, 0.053864773, 0.014996461, -0.017072927, 0.023422701, 0.010875267, 0.0702011, 0.022536317, -0.012561964, -0.02987489, -0.06348359, 0.017300304, 0.0038994283, 0.006731195, 0.04033917, -0.020278372, -0.007048802, -0.017594166, 0.04487118, 0.073680274, 0.0018738636, 0.00093438337, -0.0035490168, 0.019608187, -0.0029052421, -0.020932283, -0.06713028, -0.0374404, -0.021403924, 0.0062965304, 0.012631555, -0.010014987, 0.015221374, 0.019852635, 0.011731321, -0.0133805275, 0.02838467, -0.07282728, -0.07217905, -0.04883099, -0.0084898705, 0.04168751, 0.030711083, 0.001848249, 0.022351325, 0.015419704, 0.0442286, 0.02575011, 0.043933317, -0.02294329, -0.03672384, 0.009109786, -0.016780082, -0.041093815, 0.022476941, -0.0048362766, 0.0055515007, -0.010938289, 0.031210566, 0.017488118, -0.056014482, -0.002756727, -0.02052235, -0.036615748, 0.04438489, 0.026109736, -0.0041790693, 0.004629336, 0.0023573677, -0.029041717, 0.020416636, -0.03964934, 0.009758068, 0.024761656, 0.030474106, -0.01851462, 0.0522473, 0.00096809777, 0.0005496485, -0.014650634, -0.008401621, 0.0025748925, 0.043048684, 0.0067636236, 0.057189625, 0.0033644242, 0.03230556, 0.008254095, -0.029137738, 0.015284085, -0.021719445, 0.04448635, -0.09725138, -0.012354504, -0.017538335, -0.010292354, -0.0009541401, 0.022233091, -0.008181112, -0.040298443, 0.005496116, 0.0066138436, 0.021363791, -0.026347267, -0.007240977, -0.025327567, -0.058576208, -0.025458379, -0.066014886, -0.008119298, 0.016138619, -0.033159856, 0.026520053, 0.0041653924, -0.034390677, 0.036493152, -0.06222193, -0.057759613, 0.036947, -0.014738665, 0.01026564, -0.05244876, -0.06907933, 0.02537118, 0.0066963127, -0.024256963, -0.0004416369, 0.06374293, 0.029576654, -0.06704559, 0.047852017, -0.043203074, 0.013263682, -0.0062695364, 0.0049257996, -0.0136814425, 0.025523622, 0.007961856, -0.0026337602, -0.003597812, 0.024099674, 0.022871207, 0.017007366, 0.023251606, 0.00076488766, -0.045287553, 0.0379143, -0.011583221, 0.04488923, -0.03180107, 0.022329701, 0.028064877, 0.010714023, 0.021082075, 0.012787222, 0.038157348, 0.0017787591, -0.012385073, -0.011724736, -0.04939992, -0.020304084, 0.06829166, 0.0038143953, -0.015463989, -0.06215223, 0.06856171, 0.013002886, 0.0037150995, 0.028522467, 0.04285611, 0.07196045, -0.012282807, 0.005932929, 0.049333375, 0.051054765, -0.018300388, 0.0003456477, 0.036567252, -0.0918958, 0.04557543, -0.013902189, -0.008923833, -0.020998253, -0.063255295, 0.021437032, 0.05876051, -0.034592286, -0.024455542, 0.0397192, -0.0021234616, -0.046747025, -0.032684706, -0.018132884, 0.010772112, 0.04435417, 0.04824887, 0.009242242, -0.034680698, -0.029059164, -0.014481166, 0.012145882, 0.053496316, -0.002600291, -0.008975269, -0.005248385, 0.0025544488, 0.053798832, -0.0050718407, 0.014320239, 0.012294275, -0.050611548, -0.0068352954, -0.01757607, 0.0099505335, 0.036903802, 0.0039586206, 0.0351755, 0.051540237, -0.00034428225, -0.010625322, -0.0014541504, 0.015246767, 0.053612046, -0.04446526, -0.041865077, 0.047421668, -0.002877041, 0.021856753, -0.0008532999, 0.019941129, 0.015577308, -0.052715566, 0.037015963, 0.022245571, -0.062205043, 0.04125532, 0.023641257, -0.061354984, 0.017274305, -0.04231443, -0.08750836, 0.028906235, 0.08523484, -0.039481666, 0.06307744, -0.027184093, 0.023514831, 0.017340668, -0.056358133, 0.012426055, 0.0072795446, 0.009605522, -0.05333925, -0.01531711, 0.05183462, -0.01909594, 0.0002105068, 0.025573699, 0.04331739, 0.0032933177, -0.0010844761, -0.031325016, -0.001401532, 0.054333206, 0.03601987, -0.07892205, 0.014529531, -0.08214707, 0.04095199, -0.038725737, 0.032028165, -0.023477465, -0.04933217, -0.0359169, 0.019096408, 0.0011774646, -0.011356212, -0.029708361, 0.078027174, -0.023528442, -0.051849402, 0.0019310463, -0.01950101, 0.04275589, 0.015075276, 0.052942958, 0.054238707, -0.06390018, 0.0038195495, -0.030969806, 0.048933577, -0.054711416, 0.008373, -0.006422133, -0.05597087, -0.03706223, -0.0107219145, -0.09261914, 0.04417693, 0.018280597, -0.0044175903, 0.042131443, -0.07698077, -0.02143794, 0.028325737, -0.06518409, 0.028484395, -0.024248997, 0.044473592, -0.06877001, -0.031995796, -0.060333584, -0.01931553, 0.0003004408, -0.036761094, -0.06168421, -0.01224523, 0.0042207683, 0.04844541, -0.023138572, -0.0010195301, 0.032966312, 0.00087706116, 0.043555897, 0.0047045243, -0.034151137, 0.050439548, -0.01635965, -0.020111702, -0.031296372, 0.0029976529, -0.013091481, 0.05212559, -0.059518702, -0.014642966, -0.0009988992, -0.0467051, -0.023744918, 0.028914098, -0.028897744, 0.0006703591, -0.04053502, -0.060598735, -0.0028968987, 0.013326969, 0.031328406, -0.01612008, 0.041203003, -0.029931532, 0.027014418, -0.03506214, 0.0032939217, -0.02779594, -0.027447606, 0.02509525, 0.032072287, 0.026481543, -0.045141116, -0.06767252, 0.021876786, 0.0553572, -0.027299806, -0.0027109839, 0.028742433, 0.026546577, 0.009167693, 0.033655614, 0.06260553, 0.028059617, -0.047903072, -0.046116363, 1.0898213e-05, 0.0032755956, -0.014149632, -0.017451694, 0.0038554033, 0.016348844, 0.009839882, 0.037718996, 0.021316934, 0.013833608, -0.00024076401, -0.011117002, -0.040598, -0.03181582, -0.046110727, 0.043259732, 0.01413813, -0.042903177, -0.027804213, -0.0079156915, 0.0023165282, 0.04160998, -0.007866303, 0.0042562243, -0.0012918463, 0.025531162, -0.03044718, 0.006374511, -0.019974776, -0.00518655, -0.07142176, 0.033684406, 0.026293794, -0.032882754, -0.06232013, -0.009679599, -0.015639227, -0.022519818, 0.031001605, -0.0076105264, 0.03891666, -0.017154308, -0.048790574, 0.028975258, 0.030390976, -0.011569774, 0.05860311, 0.025449606, 0.016120052, -0.013572217, -0.0011648918, 0.038489576, 0.058604136, 0.0011286014, -0.040225103, -0.04149855, 0.02146647, -0.07801541, 0.046130903, 0.0007918995, 0.041659664, -0.0140662985, -0.040050324, -0.0222898, 0.028001586, 0.056442477, -0.008004966, -0.03148882, 0.018697545, 0.012348902, -0.030491855, 0.020376137, -0.011038617, 0.008128277, 0.05561093, -0.026682906, -0.036469836, -0.053371713, 0.039264064, 0.009945616, 0.015425427, -0.0013834873, -0.021761952, -0.055614576, 0.021177195, -0.032838155, -0.0046883374, -0.0008663015, -0.0061283284, -0.016715677, -0.024744019, -0.0005502276, 0.05128383, -0.016805967, -0.056395207, 0.005975391, 0.0009654184, -0.037072536, -0.046795785, -0.032267712, 0.028426087, 0.018781781, 0.040217765, 0.113463126, -0.0018773109, 0.048621744, -0.029367177, 0.06821471, -0.011541101, -0.024927143, -0.01804501, 0.0021482108, -0.034008104], "index": 3, "object": "embedding"}, {"embedding": [-0.021767588, 0.051805552, -0.14397132, -0.032564748, 0.0429998, -0.029370204, 0.0004313846, -0.01542163, -0.0001191941, 0.019517386, -0.0226038, -0.0010916139, 0.08979949, 0.084299184, -0.0034786148, -0.0014254081, -0.014219056, -0.008216362, -0.004984319, 0.035169113, 0.0014282698, -0.0047210595, 0.008404088, -0.05103624, 0.045124114, -0.011838627, -0.04116648, 0.030610805, -0.01231744, 0.02809304, 0.05430086, -0.07231015, 0.011182623, -0.015007377, -0.014211448, -0.052591704, 0.06697806, 0.0051114196, -0.06625251, 0.083340935, 0.007750096, -0.018024221, 0.013331173, 0.011964462, 0.044380758, -0.025060225, 0.045280553, -0.019983632, 0.018736174, -0.07736881, 0.05601512, 0.0005357942, -0.014239452, 0.01804553, 0.08855588, 0.016632892, -0.05524542, 0.04366968, 0.014400661, -0.035803396, 0.09143058, 0.026288109, -0.03622261, 0.08368985, -0.018861527, -0.018537723, -0.048501782, 0.02967607, 0.021126572, -0.02871411, -0.0015153362, 0.06455616, 0.028460829, 0.076981455, -0.02848589, -0.03533969, -0.026543759, -0.005080979, -0.04841396, 0.069596924, 0.05034082, -0.012959739, 0.08925835, -0.0024545747, 0.06554284, -0.023072178, -0.049561363, -0.016680237, -0.04611623, 0.09757851, 0.029895836, 0.046142474, 0.021790843, 0.02017816, -0.043342408, 0.016811298, -0.08378102, 0.0026481461, -0.01608769, -0.0246745, -0.07221833, 0.00995131, 0.014646628, -0.020588472, 0.05000457, 0.04207261, 0.024551142, -0.0034239101, -0.04523818, -0.0027760132, -0.009054272, -0.004984453, -0.013822499, -0.012581499, -0.043764535, -0.024321135, 0.05741884, -0.0011116181, 0.04344601, 0.09892789, -0.026721226, -0.018204056, -0.012417644, -0.017730871, -0.008942769, 0.019460836, -0.027812762, -0.019511517, -0.016368449, -0.013144472, 0.0047004116, -0.025935825, 0.025209786, 0.03538919, 0.022767967, 0.098045, -0.043530125, -0.013950576, 0.041609738, 0.018035887, -0.006131411, 0.0016957563, -0.030871755, 0.0019501185, -0.017442307, -0.07794954, 0.023927873, 0.025091454, -0.03005352, -0.008406526, 0.0003881568, 0.029758595, 0.028434347, 0.029242868, 0.037123356, -0.00871843, -0.024841253, 0.032981433, 0.012616607, -0.033150915, 0.052770488, -0.0028961562, -0.010841485, 0.07815079, -0.0064538256, -0.013671949, 0.0247392, 0.010892133, 0.045111746, 0.019918568, -0.08202961, -0.009737802, -0.005646953, -0.004273896, 0.043880153, 0.046274472, 0.036829803, -0.068862535, 0.0068230554, -0.01869857, 0.033375014, -0.067516424, -0.03687619, 0.0133015625, -0.016328376, -0.033867195, -0.03462981, -0.02449092, 0.007855442, -0.034056228, 0.010932896, 0.056770578, -0.04948662, -0.017831983, -0.03648087, -0.013100213, 0.037970178, -0.02971497, 0.054616287, -0.061808825, -0.06462824, -0.010167656, -0.05158313, 0.01906714, -0.062158305, 0.058166068, 0.019127896, 0.053090576, -0.04793144, 0.029357798, 0.05620027, 0.015315812, 0.0052178153, -0.028227143, -0.018228905, 0.03959562, -0.025203299, -0.028328432, 0.019938422, -0.0031608518, 0.025143625, -0.0049327943, 0.0005409755, -0.0041110073, 0.012032395, -0.0016311648, -0.030844595, 0.013394281, -0.043594282, 0.047090873, -0.045000385, -0.052916057, 0.062941514, -0.0004500529, 0.031461652, 0.022541372, -0.0032731195, 0.07160358, -0.0145120695, 0.0210756, 0.014236072, 0.031929176, -0.012134359, -0.037645373, -0.090653606, -0.03390476, -0.043936335, -0.048128255, -0.033319503, 0.06362256, -0.028155833, -0.0040370873, 0.0030628273, -0.0018537465, 0.048028905, -0.0012667001, -0.008689764, 0.01562032, 0.0061532552, 0.052465774, 0.05152146, -0.041492134, -0.012011569, 0.0021372985, 0.058044493, -0.03804901, -0.003342332, 0.005298333, -0.004664324, 0.022324482, -0.014480338, 0.04223618, 0.018964771, 0.0015686381, -0.0076468936, 0.044389315, -0.0041598612, -0.011371274, 0.015704138, 0.023616254, -0.028237496, -0.06998086, -0.045250513, -0.05204488, 0.009386252, 0.041602194, 0.009014269, 0.0035618069, -0.010154706, -0.028819457, -0.011377157, 0.008587392, -0.025487136, -0.028637897, -0.046987776, 0.019936448, 0.0066831345, 0.031480245, 0.018403767, -0.027024394, 0.005626194, -0.015459005, 0.069388315, 0.058097024, -0.00892769, -0.0033225964, 0.022044636, -0.0110343695, 0.039437123, 0.009194461, -0.049840942, -0.030480403, -0.026767833, 0.030539412, 0.010487152, 0.019920073, 0.0012668018, 0.039462015, 0.03674682, 0.0058439285, 0.027608141, -0.04765372, -0.027330132, -0.020632623, -0.0225679, 0.050220676, 0.032673527, -0.0024419038, 0.010225867, 0.020495437, 0.047918033, 0.04628932, -0.010919368, -0.03145318, 0.011846621, 0.017807191, -0.026153242, -0.011969634, 0.03362116, -0.01507562, 0.0681097, -0.036119353, 0.041329496, -0.0065891617, -0.0632638, -0.015749972, -0.051409177, -0.032794, 0.024920465, 0.036554758, -0.026795939, 0.011347614, 0.010209543, -0.007110033, 0.018772097, -0.053807076, 0.007813641, 0.030594826, 0.056283575, 0.0015441311, 0.051952027, 0.009395615, -0.019871814, -0.018397234, 0.0024194939, 0.0032172946, 0.059022367, 0.0031525623, 0.034534115, -0.018020393, 0.018167311, -0.01422879, -0.04740319, 0.016370729, -0.04364464, 0.058846865, -0.06877227, -0.013246867, -0.038518336, 0.0026924592, 0.0063238805, -0.0032129956, 0.027159022, -0.0183348, 0.033988886, -0.018746695, 0.046769306, -0.03603209, -0.0054656575, -0.0025042745, -0.031426225, -0.04398981, -0.041851517, -0.018359479, 0.021216568, -0.036129273, 0.019129157, -0.037892547, 0.008988673, 0.04707744, -0.02517837, -0.023907874, 0.022894764, -0.0039219116, 0.03233662, -0.019871728, -0.033103313, 0.020423016, -0.00096583867, -0.018601397, -0.0045806183, 0.022794865, 0.015778845, -0.04623559, 0.041109703, -0.008721058, 0.018491603, 0.04112471, -0.011203426, 0.036353786, 0.03746994, 0.054655652, -0.020332392, -0.0059072655, 0.026296612, 0.026511954, 0.023657894, 0.06798529, -0.002497375, -0.08736093, 0.019029958, -0.011679416, 0.049954906, -0.0363503, -0.019828036, 0.027738089, -0.010014823, 0.041214798, 0.04003875, 0.027248487, 0.022405915, -0.04565635, -0.015664464, -0.05506321, 0.03870516, 0.07870882, 0.049650352, 0.003281033, -0.07254791, 0.04121565, -0.046289805, -0.002944605, 0.002276273, 0.03798094, 0.072058044, -0.010725032, 0.012810991, 0.03338467, 0.038329676, 0.027189171, 0.033751205, 0.029133415, -0.064333126, 0.033757314, 0.0068011177, -0.029701097, 0.008356536, -0.044783812, 0.030538647, 0.026238514, -0.0041147494, -0.012050919, 0.050132856, 0.00251203, -0.039120253, -0.035989102, -0.040385067, 0.016054405, 0.02857645, 0.014830599, -0.0030180574, -0.05453319, 0.006170272, -0.010828348, -0.0021705579, 0.03945004, -0.0038992157, 0.01956747, 0.0011888078, 0.0410472, 0.05288434, -0.03937547, -0.014845546, -0.011719814, -0.019397927, -0.000608171, -0.018787602, 0.023520475, 0.024503456, -0.005655368, 0.01993474, 0.039539423, -0.008184132, -0.03973279, -0.003430781, -0.009685696, 0.033618845, -0.073302746, -0.051004477, 0.06823582, 0.006373511, -0.008437335, -0.033669807, 0.05845854, 0.013942184, -0.07694175, 0.047215372, 0.030794015, -0.04139984, 0.027152445, 0.008519343, -0.034771513, -0.009960101, -0.042265397, -0.074153766, -0.0041269525, 0.06837861, -0.056352016, 0.04698326, 0.0035709254, 0.0050180033, 0.016260298, -0.015207573, -0.007714661, 0.025404146, 0.0045371503, -0.026582193, 0.009268022, 0.019971183, 0.023340391, 0.021742389, 0.0067230067, 0.035809882, -0.005788733, 0.050601367, -0.015473911, 0.024503438, 0.057955038, 0.014573314, -0.052323334, 0.021319583, -0.08718523, 0.016511383, -0.02708875, 0.02721481, -0.038873028, -0.07047177, -0.028642261, 0.0025489726, -0.04752846, 0.021223122, -0.031942606, 0.08377085, 0.01238619, -0.02846633, -0.0016417, -0.024138773, -0.006089435, 0.027203057, 0.04863543, 0.005509803, -0.077436425, 0.030564599, -0.035013113, 0.023121348, -0.022675345, 0.009107896, -0.009352627, -0.065795355, -0.012822141, 0.01864813, -0.07450107, 0.025591433, 0.015057567, -0.0039797598, 0.038924795, -0.05028388, -0.014821114, 0.010796096, -0.056032486, -0.023912989, -0.04838519, 0.04349111, -0.031258665, -0.033196807, -0.028907353, -0.006293884, -0.053717185, -0.0020723124, -0.071551, -0.003802626, 0.017916242, 0.003290291, 0.010345803, 0.009058493, 0.064420186, -0.022735417, 0.018817877, 0.009380837, -0.035291646, 0.00816494, 0.017989837, 0.0039271535, -0.009054212, -0.0073739407, -0.01948849, 0.013467943, -0.0669155, -0.025569536, 0.005585674, -0.063306026, -0.03443346, 0.054708615, -0.007116224, 0.0043032602, -0.010195564, -0.10341911, -0.040565282, 0.052579504, 0.056293465, -0.024048684, -0.028335158, -0.039127883, 0.02812778, -0.035353974, 0.008103869, -0.01677864, -0.002443294, 0.025752872, 0.026953414, -0.016716931, -0.044827066, -0.02737815, 0.0062963613, 0.07870013, -0.05168795, 0.007002086, 0.038703304, 0.012046608, -0.007477564, 0.047811817, 0.07328619, 0.0021113127, -0.035198823, -0.042263277, 0.028430466, 0.0013282434, -0.0048739505, -0.017542794, -0.0028794105, 0.024929825, 0.008345281, 0.005318998, 0.014278034, 0.008332069, -0.013377152, 0.009856358, -0.0212244, -0.02338421, -0.023565108, 0.029185187, 6.7902965e-07, -0.00015407232, -0.028342137, -0.034219258, 5.105357e-05, 0.008681284, 0.00963968, 0.023490272, 0.00951162, 0.03851215, -0.03433914, 0.022010105, -0.03829601, -0.01743817, -0.096822634, 0.020361967, 0.042923752, -0.04295516, -0.060708053, -0.008413485, -0.0037414774, -0.000738978, 0.021801094, -0.027103055, 0.026966978, -0.0019338086, -0.004947761, 0.021605467, 0.06238953, -0.025993852, 0.030491702, 0.003684609, 0.023999197, -0.029611794, -0.016111923, 0.045964926, 0.0122993095, 0.0057339035, -0.022846462, -0.098415285, 0.010997213, -0.06397519, 0.070679836, 0.008666992, 0.007716128, 0.021770436, -0.036668234, -0.042425293, 0.017740987, 0.051545005, -0.02355115, -0.041920464, 0.009501597, 0.009215535, -0.03949825, 0.014539813, -0.025083585, 0.009553218, 0.025493184, -0.0034940536, -0.042843267, -0.020492109, 0.030930158, -0.010567947, -0.033475526, -0.0072360816, -0.038522277, -0.030558063, 0.009694724, -0.021698054, 0.010374854, 0.002876193, 0.034480434, -0.045191523, -0.02650845, -0.0097017, 0.08614296, -0.0016834722, -0.08370527, -0.04765715, 0.004109145, -0.013278033, -0.0198765, 0.004993002, 0.0072857197, -0.006522445, 0.03611225, 0.14256045, 0.010592124, 0.04785058, -0.022943178, 0.03714876, -0.043532178, -0.03142969, -0.023432272, -0.03325529, 0.0049749846], "index": 4, "object": "embedding"}, {"embedding": [0.0047714068, 0.006072953, -0.18363257, -0.05029282, 0.049223516, -0.05680999, 0.026437739, -0.001146072, -0.009761509, -0.03669834, -0.016459122, 0.03428724, 0.08631657, 0.07776128, 0.0074159326, -0.015240976, 0.006237069, -0.037208043, -0.028018352, 0.029830178, -0.013711134, 0.012233598, 0.0036025639, -0.054873757, 0.02170619, 0.012747182, -0.016281761, 0.010328017, -0.04622885, 0.030817883, 0.039491143, -0.069195434, -0.04578326, -0.07281391, 0.0035501712, -0.054659177, 0.037495352, 0.03674151, -0.031332225, 0.06131053, 0.058420017, -0.012422442, 0.022668341, 0.0025582025, 0.031067532, -0.026574273, 0.006824709, -0.015784347, -0.0059032054, 0.013353799, 0.045650337, -0.0048547685, 0.018316641, -0.015899852, 0.08039539, 0.040440284, -0.03716223, 0.0028878066, 0.026574677, -0.06388625, 0.06632344, 0.051101256, -0.10213593, 0.049470425, -0.0070033446, -0.0093351975, -0.075916484, 0.030095456, 0.035841726, -0.055413634, 0.040515695, 0.02845649, -0.01620546, 0.06331885, -0.055339564, -0.03468167, -0.02352176, -0.0003964865, -6.058077e-05, 0.02524032, 0.030842416, 0.0073978887, 0.059946943, 0.02369792, 0.033009764, -0.0056282887, -0.024755182, -0.00042198974, -0.066348016, 0.080969945, -0.00062295987, 0.016602483, 0.009724179, 0.04930479, -0.028902415, 0.027340088, -0.053176828, 0.042760044, -0.008640211, -0.05255506, -0.048332274, 0.026969684, 0.034836724, -0.0075351032, -0.042211287, 0.0644818, 0.04942314, -0.04164795, -0.019364849, -0.010883505, -0.05274659, 0.027137171, -0.049792595, -0.0010879536, -0.042725105, -0.024455754, 0.075086676, -0.038091682, 0.018635433, 0.029980468, 0.021176826, -0.06189269, 0.009952223, 0.012228539, 0.0027821455, 0.029033324, -0.07585654, -0.017422635, -0.021891909, -0.030050525, -0.0051760636, -0.023272231, 0.019449404, 0.010403779, 0.022283884, 0.09182462, -0.0264165, -0.03488903, 0.02861063, 0.03604585, 0.005319946, 0.00888933, -0.037691668, -0.030492242, 0.0072960085, -0.07618003, -0.017408052, 0.007458527, -0.002229059, 0.003331225, 0.030904975, 0.037778694, 0.04462331, 0.039072793, 0.04966725, -0.007619552, -0.024631582, -0.0036609708, -0.0073221102, 0.0044250707, 0.026616089, -0.004039269, 0.004635354, 0.036070026, -0.01357426, -0.049385007, 0.0026842095, 0.021726461, 0.01439731, 0.030210113, -0.05301105, -0.02839031, 0.0019025647, -0.037999813, 0.036782376, 0.032432694, 0.058683813, -0.04780129, 0.014308476, -0.0065776566, 0.042504, -0.039756786, 0.0040433193, 0.023064096, 0.009863814, -0.0052325544, -0.014902005, 0.013155845, -0.015669815, -0.011705167, -0.008142899, 0.045749657, -0.09810461, -0.050767045, -0.051863577, -0.046257257, 0.04136968, -0.041910104, 0.021132777, -0.035416335, -0.027121466, -0.03078587, -0.06576448, 0.04756202, -0.04354852, 0.06675296, 0.028722756, 0.029776765, -0.02178728, 0.04190193, 0.030061232, 0.012557085, -0.0015629374, 0.031780094, 0.025305266, 0.0021051771, -0.022406626, 0.0246211, -0.005245697, -0.006630072, 0.029623976, -0.017667035, 0.024793735, -0.01314129, 0.027428871, -0.022317473, 0.011342949, 0.028996348, -0.06521092, 0.04691084, 0.0010383471, -0.025617797, 0.054176874, 0.011971033, -0.009260048, 0.019315312, 0.0009079119, 0.07436186, -0.016616445, 0.03705276, 0.007015486, 0.039978903, -0.028809877, -0.024230236, -0.04998346, -0.022107132, 0.029080074, -0.028218849, -0.004364698, 0.07298799, -0.03447526, 0.038978, 0.0462163, -0.008933419, 0.004984081, -0.005918932, -0.024705887, -0.0061568925, 0.04453866, 0.02776039, 0.04770607, -0.005442735, -0.03201325, -0.022508992, -0.009039239, -0.014479737, 0.040920675, 0.011651258, 0.0066813426, -0.0045497487, 0.010810818, 0.027945537, 0.033618245, 0.02418374, -0.028824715, 0.025982939, -0.033608116, -0.046127185, 0.016639391, 0.039650995, -0.011664652, -0.032283027, -0.0056736968, -0.029163118, 0.011452038, 0.036507048, 0.03890957, 0.01822205, 0.03258194, -0.040686943, -0.007608225, 0.017263012, 0.01765474, 0.0037640957, -0.043934785, 0.0188364, 0.0697534, 0.003970849, 0.022536306, -0.013708214, 0.0120524075, 0.015332936, 0.08243013, 0.040580586, 0.022962313, -0.015790954, -0.01350905, -0.031185577, 0.04538849, -0.0048865294, -0.07470024, -0.014886092, -0.09282675, 0.041138764, -0.0028369653, -0.012603551, 0.00813366, 0.045517042, 0.047048498, 0.0007705562, 0.021103732, -0.0145708155, 0.0153930625, -0.048054714, -0.03457096, 0.033745058, 0.024994252, -0.022978313, -0.031092282, 0.027755668, 0.052932, 0.0523747, 0.022141894, -0.02713223, 0.009554451, -0.00037947035, -0.021007048, -0.01087777, 0.012703982, 0.029482834, 0.07668183, -0.03328303, 0.046425447, -0.05346519, -0.0034023523, 0.00546717, -0.045170773, -0.043663006, 0.04008647, 0.04027377, -0.010254101, -0.016840927, -0.020987837, -0.006780909, 0.04643623, -0.004893663, 0.04657346, 0.026116926, 0.024348106, -0.009331331, 0.0432037, -0.0070803976, -0.010034487, -0.048684046, -0.02428865, -0.02017059, 0.042523324, -0.016742991, 0.034882568, -0.0082559055, 0.020817675, -0.01893233, -0.065397255, 0.004736879, -0.042589936, 0.030810857, -0.028469058, 0.00014098486, -0.034726635, -0.014270054, -0.01121762, 0.0052322946, 0.008101284, -0.00037339714, -0.022236055, -0.0075761247, 0.057729073, -0.002523403, 0.024486905, 0.013939765, -0.02073651, -0.07656041, -0.01835564, 0.008786886, 0.0028650204, -0.053501002, 0.072603956, -0.018458774, 0.0033551909, 0.050556473, -0.07465815, -0.0237914, -0.0028944022, -0.023707094, -0.0033385665, -0.024479967, -0.059896175, -0.022884637, 0.009513433, -0.022422615, 0.03864937, 0.04338665, 0.0014526764, -0.04434801, 0.0057448233, 0.006484649, 0.03442875, 0.025785193, -0.02563654, 0.029345615, 0.0140549205, 0.061745435, -0.025940398, -0.020868752, 0.040364187, 0.024814522, 0.014447743, 0.057827957, 0.02122391, -0.057967946, 0.03808877, -0.018285383, 0.0674909, 0.003051207, -0.023094846, 0.00794749, -0.023554465, 0.010539514, 0.026001679, 0.041656543, 0.0028936674, -0.06862831, -0.02330331, -0.030672234, 0.01818559, 0.09337798, 0.026632346, -0.012186667, -0.045506835, 0.030780813, 0.005033051, 0.00795034, 0.011853744, 0.046080455, 0.11664228, -0.027021792, 0.028599497, 0.027274163, 0.038978584, 0.020680742, 0.061087795, 0.034471735, -0.045186363, 0.00992634, 0.002548837, -0.047745034, -0.0016431047, -0.054466285, 0.015328242, 0.0433608, -0.040729728, 0.0033521198, 0.03361714, -0.003537946, -0.015027371, -0.030353477, -0.002993352, -0.015284793, 0.062053487, 0.057781894, -0.0026491887, -0.04146973, -0.03838519, -0.0653992, -0.026942555, 0.052629143, 0.033776633, 0.0062795402, 0.0011823797, 0.037038766, 0.052054826, -0.01220554, 0.014060462, -0.0006541397, -0.02653646, 0.018043935, -0.04110078, 0.057555847, 0.043056346, -0.0010178454, 0.025909316, 0.015922118, -0.014483998, -0.02856071, 0.009586961, -0.02728593, 0.026293539, -0.05171985, -0.0921185, 0.05611413, -0.03766273, 0.00127989, -0.001653298, 0.03972219, 0.0034497015, -0.08682105, -0.0013773476, 0.03182397, -0.048339292, -0.014025655, 0.010160436, -0.040560853, -0.05477776, 0.015851306, -0.091769256, 0.014815864, 0.046590928, -0.05873247, 0.035168096, 0.03734631, 0.014298403, 0.042965822, -0.04114266, 0.0112335, 0.016482774, 0.012833306, -0.029325306, 0.041272298, 0.0016854987, 0.024180146, 0.018698297, 0.061819512, 0.027754981, -0.005605539, 0.0004925613, -0.0114706615, -0.0018299969, 0.037328858, -0.019348519, -0.06438251, 0.012157872, -0.07090689, -0.010081351, -0.04602943, 0.033412464, -0.027599003, 0.0039239232, -0.04723124, -0.03302235, -0.014509626, -0.032405037, -0.009201108, 0.06660006, 0.016357325, -0.019702215, 0.013513492, -0.036057115, -0.01812719, 0.023265485, 0.025519252, 0.019740405, -0.079361886, 0.01010314, -0.012831429, -0.0383202, -0.0056033293, 0.01793401, 0.01314778, -0.06897642, -0.055940446, 0.041071173, -0.03054294, 0.00080955186, -0.0035823733, -0.018766668, 0.0035313528, -0.049775332, -0.01858502, 0.0075920667, -0.01764967, -0.031321578, -0.033257727, 0.021191552, -0.035957858, 0.029989576, -0.03713747, 0.027513629, -0.02772246, -0.06429519, -0.0495305, 0.004758071, 0.042691402, 0.03909059, -0.026973441, 0.036716215, 0.06371146, -0.033431757, 0.04265189, 0.013926218, -0.0010690055, 0.019505385, -0.005325194, -0.009545234, -0.010955563, -0.0075683747, -0.028260997, 0.03245889, -0.026345186, -0.018464275, -0.008337513, -0.01948827, -0.04409933, 0.037546765, -0.09536318, 0.008098634, -0.016542248, -0.043316334, -0.02992699, 0.034514967, 0.027156245, -0.043508872, -0.03837304, -0.04510947, 0.0014520403, -0.0668977, 0.027441323, -0.024974048, 0.024471654, -0.0013214552, 0.06083542, 0.0028602744, -0.0020488303, 0.008325114, 0.013465694, 0.046658874, 0.0036517407, 0.042872958, 0.061553147, 0.0158121, -0.04600478, 0.06612204, 0.08518536, 0.0052149068, -0.011828587, -0.013826481, 0.06077574, 0.05051519, -0.028308362, -0.021644779, -0.015760327, 0.023346711, -0.019247646, -0.024756156, -0.013494967, 0.031021444, 0.008751634, -0.012872351, -0.039513636, -0.028192718, -0.0141522335, 0.033884585, 0.0074705714, 0.00340796, -0.007402888, 0.016468434, 0.027262896, 0.0048549543, 0.06946463, 0.014623438, -0.006913699, 0.028273929, -0.02612713, 0.017863177, -0.027236605, -0.018104896, -0.06960337, 0.02690617, 0.005992867, -0.035746343, -0.039192613, -0.016388992, -0.04676921, -0.026013164, 0.0022860626, 0.00013063711, 0.0141986115, 0.008690025, -0.015803399, 0.01274577, 0.05286544, -0.018321823, 0.053884473, -0.04034829, 0.052302115, -0.04173576, -0.010385128, 0.03380125, -0.031696144, 0.043826003, -0.061163872, -0.034552597, 0.023520786, -0.022782847, 0.05432148, -0.016498359, 0.010581636, -0.034680475, -0.022229968, -0.026744261, 0.034715872, 0.019010637, 0.004668776, -0.03615581, -0.004278144, 0.06932505, -0.030575586, -0.004852216, -0.034140926, 0.023450438, 0.024486257, -0.045513958, -0.05186469, -0.04099255, 0.057016745, -0.008466339, 0.011844431, -0.008049393, -0.016601283, -0.059944138, -0.00679542, -0.026723957, 0.03531602, 0.008813015, 0.0028459015, -0.0011978421, -0.016422568, 0.016005194, 0.060653362, 0.0001881762, -0.05773982, -0.033617616, -0.00053765305, -0.026994256, -0.02097155, -0.01975095, 0.03420983, -0.01998761, 0.023665506, 0.13337468, -0.008233787, 0.00038753162, -0.034712173, 0.06564353, -0.034412112, -0.03167496, 0.008107157, -0.0284861, -0.024327643], "index": 5, "object": "embedding"}], "model": "nomic-embed-text:v1.5", "object": "list", "usage": {"prompt_tokens": 7268, "total_tokens": 7268}}, "input": {"input": [" and language translation.\n79\n10.1.2\nBest Practices of Using Autotrain\n Data Quality: Ensure high-quality, well-labelled data for better model performance.\n Model Selection: Choose pre-trained models that are well-suited to your specific task to minimize\nfine-tuning effort.\n Hyperparameter Optimisation: Leverage Autotrains automated hyperparameter tuning to\nachieve optimal performance without manual intervention.\n10.1.3\nChallenges of Using Autotrain\n Data Privacy: Ensuring the privacy and security of sensitive data during the fine-tuning process.\n Resource Constraints: Managing computational resources effectively, especially in environments\nwith limited access to powerful hardware.\n Model Overfitting: Avoiding overfitting by ensuring diverse and representative training data\nand using appropriate regularization techniques.\n10.1.4\nWhen to Use Autotrain\n1. Lack of Deep Technical Expertise: Ideal for individuals or small teams without extensive\nmachine learning or LLM background who need to fine-tune models quickly and effectively.\n2. Quick Prototyping and Deployment: Suitable for rapid development cycles where time is\ncritical, such as proof-of-concept projects or MVPs.\n3. Resource-Constrained Environments: Useful for scenarios with limited computational re-\nsources or where a quick turnaround is necessary.\nIn summary, Autotrain is an excellent tool for quick, user-friendly fine-tuning of LLMs for standard NLP\ntasks, especially in environments with limited resources or expertise. However, it may not be suitable\nfor highly specialised applications or those requiring significant customisation and scalability.\n10.1.5\nTutorials\n1. How To Create HuggingFace Custom AI Models Using AutoTrain\n2. Finetune models with HuggingFace AutoTrain\n10.2\nTransformers Library and Trainer API\nThe Transformers Library by HuggingFace stands out as a pivotal tool for fine-tuning large language\nmodels (LLMs) such as BERT, GPT-3, and GPT-4. This comprehensive library offers a wide array of\npre-trained models tailored for various LLM tasks, making it easier for users to adapt these models to\nspecific needs with minimal effort. Whether youre fine-tuning for tasks like sentiment analysis, text\nclassification, or generating customer support responses, the library simplifies the process by allowing\nseamless model selection from the HuggingFace Model Hub and straightforward customisation through\nits high-level APIs.\nCentral to the fine-tuning process within the Transformers Library is the Trainer API. This API includes\nthe Trainer class, which automates and manages the complexities of fine-tuning LLMs. After completing\ndata preprocessing, the Trainer class streamlines the setup for model training, including data handling,\noptimisation, and evaluation. Users only need to configure a few parameters, such as learning rate and\nbatch size, and the API takes care of the rest. However, its crucial to note that running Trainer.train()\ncan be resource-intensive and slow on a CPU. For efficient training, a GPU or TPU is recommended.\nPlatforms like Google Colab provide free access to these resources, making it feasible for users without\nhigh-end hardware to fine-tune models effectively.\n80\nThe Trainer API also supports advanced features like distributed training and mixed precision, which\nare essential for handling the large-scale computations required by modern LLMs. Distributed training\nallows the fine-tuning process to be scaled across multiple GPUs or nodes, significantly reducing training\ntime. Mixed precision training, on the other hand, optimises memory usage and computation speed by\nusing lower precision arithmetic without compromising model performance. HuggingFaces dedication to\naccessibility is evident in the extensive documentation and community support they offer, enabling users\nof all expertise levels to fine-tune LLMs. This democratisation of advanced NLP technology empowers\ndevelopers and researchers to deploy sophisticated, fine-tuned models for a wide range of applications,\nfrom specialised language understanding to large-scale data processing.\n10.2.1\nLimitations of the Transformers Library and Trainer API\n Limited Customisation for Advanced Users: While the Trainer API simplifies many aspects\nof training, it might not offer the deep customisation that advanced users or researchers might need\nfor novel or highly specialised applications.\n Learning Curve: Despite the simplified API, there is still a learning curve associated with un-\nderstanding and effectively using the Transformers Library and Trainer API, particularly for those\nnew to NLP and LLM.\n Integration Limitations: The seamless integration and ease of use are often tied to the Hug-\ngingFace ecosystem, which might not be compatible with all workflows or platforms outside their\nenvironment.\nIn summary, the Transformers Library and Trainer API provide robust, scalable solutions for fine-tuning\nLLMs across a range of applications, offering ease of use and efficient training capabilities. However, users\nmust be mindful of the resource requirements and potential limitations in customisation and complexity\nmanagement.\n10.3\nOptimum: Enhancing LLM Deployment Efficiency\nOptimum6 is HuggingFaces tool designed to optimise the deployment of large language models (LLMs)\nby enhancing their efficiency across various hardware platforms. As LLMs grow in size and complexity,\ndeploying them in a cost-effective and performant manner becomes increasingly challenging. Optimum\naddresses these challenges by applying a range of hardware-specific optimisations, such as quantisation,\npruning, and model distillation, which reduce the models size and improve inference speed without\nsignificantly affecting accuracy. The following are the key techniques supported by Optimum:\n Quantisation: Quantisation is one of the key techniques supported by Optimum. This process in-\nvolves converting the models weights from high-precision floating-point numbers to lower-precision\nformats, such as int8 or float16. This", " a range of hardware-specific optimisations, such as quantisation,\npruning, and model distillation, which reduce the models size and improve inference speed without\nsignificantly affecting accuracy. The following are the key techniques supported by Optimum:\n Quantisation: Quantisation is one of the key techniques supported by Optimum. This process in-\nvolves converting the models weights from high-precision floating-point numbers to lower-precision\nformats, such as int8 or float16. This reduction in precision decreases the models memory foot-\nprint and computational requirements, enabling faster execution and lower power consumption,\nespecially on edge devices and mobile platforms. Optimum automates the quantisation process,\nmaking it accessible to users who may not have expertise in low-level hardware optimisation.\n Pruning: Pruning is another critical optimisation strategy offered by Optimum. It involves iden-\ntifying and removing less significant weights from the LLM, reducing its overall complexity and\nsize. This leads to faster inference times and lower storage needs, which are particularly beneficial\nfor deploying models in environments with limited computational resources. Optimums pruning\nalgorithms carefully eliminate these redundant weights while maintaining the models performance,\nensuring that it continues to deliver high-quality results even after optimisation.\n Model Distillation: In addition to these techniques, Optimum supports model distillation, a\nprocess where a smaller, more efficient model is trained to replicate the behaviour of a larger, more\ncomplex model. This distilled model retains much of the knowledge and capabilities of the original\nwhile being significantly lighter and faster. Optimum provides tools to facilitate the distillation\nprocess, allowing users to create compact LLMs that are well-suited for real-time applications. By\noffering a comprehensive suite of optimisation tools, Optimum ensures that HuggingFaces LLMs\ncan be deployed effectively across a wide range of environments, from powerful cloud servers to\nresource-constrained edge devices.\n6https://huggingface.co/docs/optimum/en/index\n81\n10.3.1\nBest Practices of Using Optimum\n Understand Hardware Requirements: Assess the target deployment environment (e.g., edge\ndevices, cloud servers) to optimise model configuration accordingly.\n Iterative Optimisation: Experiment with different optimisation techniques (quantisation levels,\npruning thresholds) to find the optimal balance between model size, speed, and accuracy.\n Validation and Testing: Validate optimised models thoroughly to ensure they meet performance\nand accuracy requirements across different use cases.\n Documentation and Support: Refer to HuggingFaces resources for detailed guidance on using\nOptimums tools effectively, and leverage community support for troubleshooting and best practices\nsharing.\n Continuous Monitoring: Monitor deployed models post-optimisation to detect any performance\ndegradation and adjust optimisation strategies as needed to maintain optimal performance over\ntime.\n10.3.2\nTutorials\n1. An Introduction to Using Transformers and Hugging Face\n10.4\nAmazon SageMaker JumpStart\nAmazon SageMaker JumpStart is a feature within the SageMaker ecosystem designed to simplify and\nexpedite the fine-tuning of large language models (LLMs). It provides users with a rich library of pre-\nbuilt models and solutions that can be quickly customised for various use cases. This tool is particularly\nvaluable for organisations looking to deploy NLP solutions efficiently without deep expertise in machine\nlearning or the extensive computational resources typically required for training LLMs from scratch. The\narchitecture depicted in Figure 10.2 outlines a comprehensive pipeline for the fine-tuning and deployment\nof large language models (LLMs) Utilising AWS services.\n10.4.1\nSteps Involved in Using JumpStart\n Data Preparation and Preprocessing:\n Data Storage: Begin by securely storing raw datasets in Amazon S3, AWSs scalable object\nstorage service.\n Preprocessing: Utilise the EMR Serverless framework with Apache Spark for efficient data\npreprocessing. This step refines and prepares the raw data for subsequent model training and\nevaluation.\n Data Refinement: Store the processed dataset back into Amazon S3 after preprocessing,\nensuring accessibility and readiness for the next stages.\n Model Fine-Tuning with SageMaker JumpStart:\n Model Selection: Choose from a variety of pre-built models and solutions available through\nSageMaker JumpStarts extensive library, tailored for tasks such as sentiment analysis, text\ngeneration, or customer support automation.\n Fine-Tuning Execution: Utilise Amazon SageMakers capabilities, integrated with Sage-\nMaker JumpStart, to fine-tune the selected model. This involves adjusting parameters and\nconfigurations to optimise the models performance for specific use cases.\n Workflow Simplification: Leverage pre-built algorithms and model templates provided by\nSageMaker JumpStart to streamline the fine-tuning workflow, reducing the time and effort\nrequired for deployment.\n Model Deployment and Hosting:\n82\nFigure 10.2: A step-by-step workflow illustrating the Amazon SageMaker JumpStart process, starting\nfrom data preprocessing using EMR Serverless Spark to the fine-tuning of LLMs, and ending with model\ndeployment on Amazon SageMaker Endpoints. (adapted from [85])\n Deployment Setup: Deploy the fine-tuned model using Amazon SageMakers endpoint\ndeployment capabilities. This setup ensures that the model is hosted in a scalable environment\ncapable of handling real-time predictions efficiently.\n Scalability: Benefit from AWSs infrastructure scalability, allowing seamless scaling of re-\nsources to accommodate varying workloads and operational demands.\n Efficiency and Accessibility: Ensure that the deployed model is accessible via SageMaker\nendpoints, enabling efficient integration into production applications for real-time inference\ntasks.\n10.4.2\nBest Practices for Using JumpStart\n Robust Data Management: Maintain secure and organised data storage practices in Amazon\nS3", "\ncapable of handling real-time predictions efficiently.\n Scalability: Benefit from AWSs infrastructure scalability, allowing seamless scaling of re-\nsources to accommodate varying workloads and operational demands.\n Efficiency and Accessibility: Ensure that the deployed model is accessible via SageMaker\nendpoints, enabling efficient integration into production applications for real-time inference\ntasks.\n10.4.2\nBest Practices for Using JumpStart\n Robust Data Management: Maintain secure and organised data storage practices in Amazon\nS3, facilitating efficient data access and management throughout the pipeline.\n Cost-Effective Processing: Utilise serverless computing frameworks like EMR Serverless with\nApache Spark for cost-effective and scalable data preprocessing.\n Optimised Fine-Tuning: Capitalise on SageMaker JumpStarts pre-built models and algorithms\nto expedite and optimise the fine-tuning process, ensuring optimal model performance without\n83\nextensive manual configuration.\n Continuous Monitoring and Optimisation: Implement robust monitoring mechanisms post-\ndeployment to track model performance metrics. This allows for timely optimisations and adjust-\nments to maintain accuracy and efficiency over time.\n Integration with AWS Services: Leverage AWSs comprehensive suite of services and inte-\ngration capabilities to create end-to-end pipelines that ensure reliable and scalable deployment of\nlarge-scale language models across diverse operational environments.\n10.4.3\nLimitations of Using JumpStart\n Limited Customisation: While JumpStart simplifies the process for common use cases, it may\noffer limited flexibility for highly specialised or complex applications that require significant cus-\ntomisation beyond the provided templates and workflows.\n Dependency on AWS Ecosystem: JumpStart is tightly integrated with AWS services, which\nmay pose challenges for users who prefer or need to operate in multi-cloud environments or those\nwith existing infrastructure outside of AWS.\n Resource Costs: Utilising SageMakers scalable resources for fine-tuning LLMs, especially large\nmodels, can incur substantial costs, which might be a barrier for smaller organisations or those\nwith limited budgets.\n10.4.4\nTutorials\n1. Fine-Tuning LLaMA 2 with Amazon SageMaker JumpStart\n2. LLM Agents Using AWS SageMaker JumpStart Foundation Models\n10.5\nAmazon Bedrock\nAmazon Bedrock7 is a fully managed service designed to simplify access to high-performing foundation\nmodels (FMs) from top AI innovators like AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability\nAI, and Amazon. It provides a unified API that integrates these models and offers extensive capabilities\nfor developing secure, private, and responsible generative AI applications. With Amazon Bedrock, users\ncan effortlessly experiment with and assess leading FMs tailored to their specific needs. The service sup-\nports private customisation of models through fine-tuning and Retrieval Augmented Generation (RAG),\nenabling the creation of intelligent agents that leverage enterprise data and systems. Amazon Bedrocks\nserverless architecture allows for quick deployment, seamless integration, and secure customisation of\nFMs without the burden of infrastructure management, Utilising AWS tools to deploy these models into\napplications efficiently and securely.\n10.5.1\nSteps Involved in Using Amazon Bedrock\nAmazon Bedrock offers a streamlined workflow for deploying and fine-tuning LLMs, making it an ideal\nchoice for businesses looking to quickly integrate advanced AI capabilities into their operations. Heres\na high-level overview of how Bedrock operates:\n Model Selection: Users start by choosing from a curated selection of foundation models available\nthrough Bedrock. These include models from AWS (like Amazon Titan) and third-party providers\n(such as Anthropic Claude and Stability AI).\n Fine-Tuning:\n Once a model is selected, users can fine-tune it to better fit their specific needs. This involves\nfeeding the model with domain-specific data or task-specific instructions to tailor its outputs.\n7https://aws.amazon.com/bedrock/\n84\n The fine-tuning process is handled via simple API calls, eliminating the need for extensive\nsetup or detailed configuration. Users provide their custom data, and Bedrock manages the\ntraining process in the background.\n Deployment:\n After fine-tuning, Bedrock takes care of deploying the model in a scalable and efficient manner.\nThis means that users can quickly integrate the fine-tuned model into their applications or\nservices.\n Bedrock ensures that the model scales according to demand and handles performance optimi-\nsation, providing a seamless user experience.\n Integration and Monitoring:\n Bedrock integrates smoothly with other AWS services, allowing users to embed AI capabilities\ndirectly into their existing AWS ecosystem.\n Users can monitor and manage the performance of their deployed models through AWSs\ncomprehensive monitoring tools, ensuring that the models continue to perform optimally.\n10.5.2\nLimitations of Using Amazon Bedrock\nWhile Amazon Bedrock offers a robust suite of tools and services for addressing certain AI challenges,\nit is not a comprehensive solution for all AI needs. One key limitation is that it does not eliminate the\nrequirement for human expertise. Organisations still need skilled professionals who understand the in-\ntricacies of AI technology to effectively develop, fine-tune, and optimise the models provided by Bedrock.\nAdditionally, Amazon Bedrock is not designed to function as a standalone service. It relies on integration\nwith other AWS services, such as Amazon S3 for data storage, AWS Lambda for serverless computing,\nand AWS SageMaker for machine learning model development. Therefore, businesses leveraging Amazon\nBedrock will also need to use these complementary AWS services to fully realise its potential. This\ninterconnectedness means that while Amazon Bedrock enhances the AI capabilities within an AWS\necosystem, it may present a steep learning curve and require significant infrastructure management for\nthose new to AWS.\n10.5.3\nT", " services, such as Amazon S3 for data storage, AWS Lambda for serverless computing,\nand AWS SageMaker for machine learning model development. Therefore, businesses leveraging Amazon\nBedrock will also need to use these complementary AWS services to fully realise its potential. This\ninterconnectedness means that while Amazon Bedrock enhances the AI capabilities within an AWS\necosystem, it may present a steep learning curve and require significant infrastructure management for\nthose new to AWS.\n10.5.3\nTutorials\n1. Finetuning LLMs on Amazon Bedrock\n2. Amazon Bedrock for Generative AI\n10.6\nOpenAIs Fine-Tuning API\nOpenAIs Fine-Tuning API is a comprehensive platform that facilitates the customisation of OpenAIs\npre-trained LLMs to cater to specific tasks and domains. This service is designed to be user-friendly,\nenabling a broad range of users, from businesses to individual developers, to harness the power of\nadvanced AI without the complexities typically associated with model training and deployment.\n10.6.1\nSteps Involved in Using OpenAIs Fine-Tuning API\n Model Selection:\n Choosing a Pre-Trained Model: Users begin by selecting a base model from OpenAIs\nextensive lineup. This includes powerful models like GPT-4, which offer a robust starting\npoint for a wide range of language processing tasks.\n Customisable Base: These models come pre-trained with vast amounts of data, providing\na solid foundation that can be further refined to suit specific requirements.\n Data Preparation and Upload:\n85\n Curating Relevant Data: Users need to gather and prepare a dataset that reflects the\nspecific task or domain they wish to fine-tune the model for. This data is crucial for teaching\nthe model to perform the desired function more effectively.\n Uploading Data to the API: The Fine-Tuning API facilitates easy data upload. Users\ncan feed their curated datasets into the API through straightforward commands, making the\nprocess accessible even to those with limited technical backgrounds.\n Initiating Fine-Tuning:\n Automated Process: Once the data is uploaded, OpenAIs infrastructure handles the fine-\ntuning process. The API adjusts the models parameters based on the new data to improve\nperformance on the specified tasks.\n Deploying the Fine-Tuned Model:\n API Integration: The fine-tuned model can be accessed and deployed via OpenAIs API.\nThis allows for seamless integration into various applications, such as chatbots, automated\ncontent creation tools, or specialised customer service systems.\n10.6.2\nLimitations of OpenAIs Fine-Tuning API\n Pricing Models: Fine-tuning and using OpenAIs models through the API can be costly, espe-\ncially for large-scale deployments or continuous usage. This can be a significant consideration for\nsmaller organisations or budget-constrained projects.\n Data Privacy and Security: Users must upload their data to OpenAIs servers for the fine-\ntuning process. This raises potential concerns about data privacy and the security of sensitive or\nproprietary information.\n Dependency on OpenAI Infrastructure: The reliance on OpenAIs infrastructure for model\nhosting and API access can lead to vendor lock-in, limiting flexibility and control over the deploy-\nment environment.\n Limited Control Over Training Process: The fine-tuning process is largely automated and\nmanaged by OpenAI, offering limited visibility and control over the specific adjustments made to\nthe model.\n10.6.3\nTutorials\n1. Fine-Tuning GPT-3 Using the OpenAI API\n10.7\nNVIDIA NeMo Customizer\nNVIDIA NeMo Customiser8 is part of the NeMo framework, a suite of tools and models designed by\nNVIDIA to facilitate the development and fine-tuning of LLM models. The Customiser focuses specifi-\ncally on making it easier to fine-tune large language models (LLMs) for specialised tasks and domains.\nLike other fine-tuning tools, NeMo Customiser is geared toward users who want to adapt pre-trained\nmodels for specific applications, such as conversational AI, translation, or domain-specific text gener-\nation. It delivers enterprise-ready models by offering accurate data curation, extensive customisation\noptions, retrieval-augmented generation (RAG), and improved performance features. The platform sup-\nports training and deploying generative AI models across diverse environments, including cloud, data\ncenter, and edge locations. It provides a comprehensive package with support, security, and reliable APIs\nas part of the NVIDIA AI Enterprise.\n8https://developer.nvidia.com/blog/fine-tune-and-align-llms-easily-with-nvidia-nemo-customizer/\n86\n10.7.1\nKey Features of NVIDIA NeMo\nNVIDIA NeMo is designed to enhance AI projects with several standout features.[86]\n State-of-the-Art Training Techniques NeMo employs GPU-accelerated tools like NeMo Cu-\nrator for preparing large-scale, high-quality datasets. These tools facilitate efficient pretraining of\ngenerative AI models by leveraging thousands of compute cores, which significantly reduces training\ntime and enhances the accuracy of large language models (LLMs).\n Advanced Customisation for LLMs The NeMo Customiser microservice allows for precise fine-\ntuning and alignment of LLMs for specific domains. It uses model parallelism to speed up training\nand supports scaling across multiple GPUs and nodes, enabling the fine-tuning of larger models.\n Optimised AI Inference with NVIDIA Triton NeMo includes NVIDIA Triton Inference Server\nto streamline AI inference at scale. This integration accelerates generative AI inference, ensuring\nconfident deployment of AI applications both on-premises and in the cloud.\n User-Friendly Tools for Generative AI NeMo features a modular, reusable architecture that\n", " uses model parallelism to speed up training\nand supports scaling across multiple GPUs and nodes, enabling the fine-tuning of larger models.\n Optimised AI Inference with NVIDIA Triton NeMo includes NVIDIA Triton Inference Server\nto streamline AI inference at scale. This integration accelerates generative AI inference, ensuring\nconfident deployment of AI applications both on-premises and in the cloud.\n User-Friendly Tools for Generative AI NeMo features a modular, reusable architecture that\nsimplifies the development of conversational AI models. It supports comprehensive workflows from\ndata processing to deployment and includes pre-trained models for automatic speech recognition\n(ASR), natural language processing (NLP), and text-to-speech (TTS), which can be fine-tuned or\nused as-is.\n Best-in-Class Pretrained Models NeMo Collections offer a variety of pre-trained models and\ntraining scripts, facilitating rapid application development or fine-tuning for specific tasks. Cur-\nrently, NeMo supports models like Llama 2, Stable Diffusion, and NVIDIAs Nemotron-3 8B family.\n Optimised Retrieval-Augmented Generation NeMo Retriever delivers high-performance, low-\nlatency information retrieval, enhancing generative AI applications with enterprise-grade retrieval-\naugmented generation (RAG) capabilities. This feature supports real-time business insights and\ndata Utilisation.\n10.7.2\nComponents of NVIDIA NeMo\n NeMo Core Provides essential elements like the Neural Module Factory for training and inference,\nstreamlining the development of conversational AI models.\n NeMo Collections Offers specialised modules and models for ASR, NLP, and TTS, including\npre-trained models and training scripts, making the platform versatile.\n Neural Modules Serve as the building blocks of NeMo, defining trainable components such as\nencoders and decoders, which can be connected to create comprehensive models.\n Application Scripts Simplify the deployment of conversational AI models with ready-to-use\nscripts, enabling quick training or fine-tuning on specific datasets for various AI applications.\n10.7.3\nCustomising Large Language Models (LLMs)\nWhile general-purpose LLMs, enhanced with prompt engineering or light fine-tuning, have enabled organ-\nisations to achieve successful proof-of-concept projects, transitioning to production presents additional\nchallenges.\nFigure 10.3 illustrates NVIDIAs detailed LLM customisation lifecycle, offering valuable\nguidance for organisations that are preparing to deploy customised models in a production environment\n[87].\n1. Model Selection or Development\nNVIDIA provides a range of pre-trained models, from 8B to 43B parameters, and supports the\nintegration of other open-source models of any size. Alternatively, users can develop their own\nmodels, starting with data curation, which includes selecting, labeling, cleansing, validating, and\nintegrating data. This process, better termed data engineering, involves additional analysis, de-\nsigning storage, evaluating model training results, and incorporating reinforcement learning with\nhuman feedback (RLHF). While building a custom foundation model is often costly, complex, and\ntime-consuming, most enterprises opt to start with a pre-trained model and focus on customisation.\n87\nFigure 10.3: Nvidia NeMo Framework for Customising and Deploying LLMs. The Nvidia NeMo frame-\nwork is designed for end-to-end customisation and deployment of large language models (LLMs). This\ndiagram illustrates the process from data curation and distributed training of foundation models, through\nmodel customisation, to accelerated inference with guardrails. The platform enables AI developers to\nintegrate in-domain, secure, and cited responses into enterprise applications, ensuring that LLMs are\neffectively tailored for specific tasks and industries. The NeMo framework, supported by Nvidia AI En-\nterprise, also offers robust support for various pre-trained foundation models like OpenAIs GPT family,\nensuring scalability and reliability in AI deployments. (adapted from [87])\n2. Model Customisation\nModel customisation involves optimising performance with task-specific datasets and adjusting\nmodel weights. NeMo offers recipes for customisation, and enterprises can choose models already\ntailored to specific tasks and then fine-tune them with proprietary data.\n3. Inference\nInference refers to running models based on user queries. This phase involves considering hardware,\narchitecture, and performance factors that significantly impact usability and cost in production.\n4. Guardrails\nNVIDIA employs guardrails as intermediary services between models and applications.\nThese\nservices review incoming prompts for policy compliance, execute arbitration or orchestration steps,\nand ensure model responses adhere to policies. Guardrails help maintain relevance, accuracy, safety,\nprivacy, and security.\n5. Applications\nNVIDIAs framework presents enterprise applications as LLM-ready, though this is not always\nthe case.\nExisting applications may be connected to LLMs to enable new features.\nHowever,\ncreating assistants for knowledge access or task execution often involves designing new applications\nspecifically for natural language interfaces.\n10.7.4\nTutorials\n1. Introduction to NVIDIA NeMo  Tutorial and Example\n2. How to fine-tune a Riva NMT Bilingual model with Nvidia NeMo\n88\nChapter 11\nMultimodal LLMs and their\nFine-tuning\nA multimodal model is a machine learning model that can process information from various modalities,\nsuch as images, videos, and text. For instance, Googles multimodal model, Gemini[88], can analyse a\nphoto of a plate of cookies and produce a written recipe in response, and it can perform the reverse as well.\nThe difference between Generative AI and Multimodal AI is that generative AI refers to the use of\nmachine learning models to create new content, such as text, images, music, audio, and videos,", " can process information from various modalities,\nsuch as images, videos, and text. For instance, Googles multimodal model, Gemini[88], can analyse a\nphoto of a plate of cookies and produce a written recipe in response, and it can perform the reverse as well.\nThe difference between Generative AI and Multimodal AI is that generative AI refers to the use of\nmachine learning models to create new content, such as text, images, music, audio, and videos, typically\nfrom a single type of input. Multimodal AI extends these generative capabilities by processing informa-\ntion from multiple modalities, including images, videos, and text. This enables the AI to understand\nand interpret different sensory modes, allowing users to input various types of data and receive a diverse\nrange of content types in return.\nFigure 11.1: Timeline of Multimodal Model Developments  This figure illustrates the progression\nof significant multimodal models, highlighting key releases from major tech companies and research\ninstitutions from December 2023 to March 2024. The timeline showcases models like Googles TinyGPT-\nV and Gemini Nano, along with other innovations such as MoE-LLAVA, DeepSeek-VL, and LLAVA-\nGemma, indicating the rapid advancement in multimodal AI technologies (adapted from [89]).\n89\n11.1\nVision Language Model (VLMs)\nVision language models encompass multimodal models capable of learning from both images and text\ninputs. They belong to the category of generative models that utilise image and text data to produce\ntextual outputs. These models, especially at larger scales, demonstrate strong zero-shot capabilities,\nexhibit robust generalisation across various tasks, and effectively handle diverse types of visual data such\nas documents and web pages. Typical applications include conversational interactions involving images,\nimage interpretation based on textual instructions, answering questions related to visual content, under-\nstanding documents, generating captions for images, and more. Certain advanced vision language models\ncan also understand spatial attributes within images. They can generate bounding boxes or segmentation\nmasks upon request to identify or isolate specific subjects, localise entities within images, or respond to\nqueries regarding their relative or absolute positions. The landscape of large vision language models is\ncharacterised by considerable diversity in training data, image encoding techniques, and consequently,\ntheir functional capabilities.\n11.1.1\nArchitecture\nVision-language models adeptly integrate both visual and textual information, leveraging three funda-\nmental components:\n Image Encoder: This component translates visual data (images) into a format that the model\ncan process.\n Text Encoder: Similar to the image encoder, this component converts textual data (words and\nsentences) into a format the model can understand.\n Fusion Strategy: This component combines the information from both the image and text en-\ncoders, merging the two data types into a unified representation.\nThese elements work collaboratively, with the models learning process (loss functions) specifically tai-\nlored to the architecture and learning strategy employed. Although the concept of vision-language mod-\nels is not new, their construction has evolved significantly. Early models used manually crafted image\ndescriptions and pre-trained word vectors. Modern models, however, utilise transformersan advanced\nneural network architecturefor both image and text encoding. These encoders can learn features either\nindependently or jointly.\nA crucial aspect of these models is pre-training. Before being applied to specific tasks, the models are\ntrained on extensive datasets using carefully selected objectives. This pre-training equips them with the\nfoundational knowledge required to excel in various downstream applications. Following is one of the\nexample architectures of VLMs.\n11.1.2\nContrastive Learning\nContrastive learning is a technique that focuses on understanding the differences between data points. It\ncomputes a similarity score between instances and aims to minimise contrastive loss, making it particu-\nlarly useful in semi-supervised learning where a limited number of labelled samples guide the optimisation\nprocess to classify unseen data points.\nHow it works\nFor instance, to recognise a cat, contrastive learning compares a cat image with a similar cat image and\na dog image. The model learns to distinguish between a cat and a dog by identifying features such as\nfacial structure, body size, and fur. By determining which image is closer to the anchor image, the\nmodel predicts its class.\nCLIP is a model that utilises contrastive learning to compute similarity between text and image embed-\ndings through textual and visual encoders. It follows a three-step process for zero-shot predictions:\n Pre-training: Trains a text and image encoder to learn image-text pairs.\n Caption Conversion: Converts training dataset classes into captions.\n Zero-Shot Prediction: Estimates the best caption for a given input image based on learned\nsimilarities.\n90\nFigure 11.2: Workflow of Contrastive Pre-Training for Multimodal Models. This figure illustrates the\nprocess of contrastive pre-training where text and image encoders are trained to align representations\nfrom both modalities. Step 1 involves contrastive pre-training by pairing text and image data, while\nStep 2 showcases the creation of a dataset classifier using label text encoded by the text encoder. Step\n3 demonstrates the models application for zero-shot prediction by leveraging the pre-trained text and\nimage encoders. This method enables the model to generalise across various tasks without requiring\ntask-specific fine-tuning (adopted from [90]).\n11.2\nFine-tuning of multimodal models\nFor fine-tuning a Multimodal Large Language Model (MLLM), PEFT techniques such as LoRA and\nQLoRA can be utilised. The process of fine-tuning for multimodal applications is analogous to that for\nlarge language models, with the"], "parameters": {"model": "nomic-embed-text:v1.5"}}, "key": "embeddings_2b81294bf897a8ec27edf12bf279ddb28988a18fa71e27cae9ce56396dae8b6d_v2"}