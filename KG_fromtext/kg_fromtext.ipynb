{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03467d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tiago\\anaconda3\\envs\\Graph_implementation\\Lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_core.documents import Document\n",
    "from langchain_ollama import OllamaLLM\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebab4293",
   "metadata": {},
   "source": [
    "# Opportunity KG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76da7130",
   "metadata": {},
   "source": [
    "## Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fd3ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing input_paper.txt...\n",
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Chapter 1\\nIntroduction\\n1.1\\nBackground of Large Language Models (LLMs)\\nLarge Language Models (LLMs) represent a significant leap in computational systems capable of under-\\nstanding and generating human language. Building on traditional language models (LMs) like N-gram\\nmodels [1], LLMs address limitations such as rare word handling, overfitting, and capturing complex\\nlinguistic patterns. Notable examples, such as GPT-3 and GPT-4 [2], leverage the self-attention mecha-\\nnism within Transformer architectures to efficiently manage sequential data and understand long-range\\ndependencies. Key advancements include in-context learning for generating coherent text from prompts\\nand Reinforcement Learning from Human Feedback (RLHF) [3] for refining models using human re-\\nsponses. Techniques like prompt engineering, question-answering, and conversational interactions have\\nsignificantly advanced the field of natural language processing (NLP) [4].\\n1.2\\nHistorical Development and Key Milestones\\nLanguage models are fundamental to natural language processing (NLP), leveraging mathematical tech-\\nniques to generalise linguistic rules and knowledge for tasks involving prediction and generation. Over\\nseveral decades, language modelling has evolved from early statistical language models (SLMs) to to-\\nday’s advanced large language models (LLMs). This rapid advancement has enabled LLMs to process,\\ncomprehend, and generate text at a level comparable to human capabilities [5, 6].\\nFigure 1.1 shows the evolution of large language models from early statistical approaches to current\\nadvanced models.\\n1.3\\nEvolution from Traditional NLP Models to State-of-the-Art\\nLLMs\\nUnderstanding LLMs requires tracing the development of language models through stages such as Statis-\\ntical Language Models (SLMs), Neural Language Models (NLMs), Pre-trained Language Models (PLMs),\\nand LLMs.\\n1.3.1\\nStatistical Language Models (SLMs)\\nEmerging in the 1990s, SLMs analyse natural language using probabilistic methods to determine the\\nlikelihood of sentences within texts.\\nFor instance, the probability P(S) of the sentence “I am very\\nhappy” is given by:\\nP(S) = P(ω1, ω2, ω3, ω4) = P(I, am, very, happy)\\n(1.1)\\nThis probability can be calculated using conditional probabilities:\\nP(I, am, very, happy) = P(I) · P(am | I) · P(very | I, am) · P(happy | I, am, very)\\n(1.2)\\nConditional probabilities are estimated using Maximum Likelihood Estimation (MLE):\\n6\\nFigure 1.1: A chronological timeline showcasing the evolution of Large Language Models (LLMs) from\\n1990 to 2023. This progression begins with early statistical models such as N-grams, transitions through\\nneural language models like Word2Vec and RNN/LSTM, and advances into the era of pre-trained mod-\\nels with the introduction of transformers and attention mechanisms. The figure highlights significant\\nmilestones, including the development of BERT, GPT series, and recent innovations such as GPT-4 and\\nChatGPT, demonstrating the rapid advancements in LLM technology over time. (adapted from [6])\\nP(ωi | ω1ω2 · · · ωi−1) =\\nC(ω1ω2 · · · ωi)\\nC(ω1ω2 · · · ωi−1)\\n(1.3)\\n1.3.2\\nNeural Language Models (NLMs)\\nNLMs leverage neural networks to predict word sequences, overcoming SLM limitations. Word vectors\\nenable computers to understand word meanings. Tools like Word2Vec [7] represent words in a vector\\nspace where semantic relationships are reflected in vector angles. NLMs consist of interconnected neurons\\norganised into layers, resembling the human brain’s structure. The input layer concatenates word vectors,\\nthe hidden layer applies a non-linear activation function, and the output layer predicts subsequent words\\nusing the Softmax function to transform values into a probability distribution.\\nFigure 1.2 illustrates the structure of Neural Language Models, highlighting the layers and connections\\nused to predict subsequent words.\\n1.3.3\\nPre-trained Language Models (PLMs)\\nPLMs are initially trained on extensive volumes of unlabelled text to understand fundamental language\\nstructures (pre-training). They are then fine-tuned on a smaller, task-specific dataset. This ”pre-training\\nand fine-tuning” paradigm, exemplified by GPT-2 [8] and BERT [9], has led to diverse and effective model\\narchitectures.\\n1.3.4\\nLarge Language Models (LLMs)\\nLLMs like GPT-3, GPT-4, PaLM [10], and LLaMA [11] are trained on massive text corpora with tens of\\nbillions of parameters. LLMs undergo a two-stage process: initial pre-training on a vast corpus followed\\n7\\nFigure 1.2: A schematic representation of Neural Language Models, showcasing the layered architecture\\nwhere the input layer processes sequential data, the hidden layer captures dependencies, and the output\\nlayer generates predictions. The figure emphasises the flow of information through concatenation and\\nmatrix multiplications, culminating in a probability distribution via the softmax function. (adopted from\\n[6])\\nby alignment with human values. This approach enables LLMs to understand human commands and\\nvalues better.\\n1.4\\nOverview of Current Leading LLMs\\nLLMs are powerful tools in NLP, capable of performing tasks such as translation, summarisation, and\\nconversational interaction. Advances in transformer architectures, computational power, and extensive\\ndatasets have driven their success. These models approximate human-level performance, making them\\ninvaluable for research and practical implementations. LLMs’ rapid development has spurred research\\ninto architectural innovations, training strategies, extending context lengths, fine-tuning techniques, and\\nintegrating multi-modal data. Their applications extend beyond NLP, aiding in human-robot interactions\\nand creating intuitive AI systems. This highlights the importance of comprehensive reviews consolidating\\nthe latest developments [12].\\nFigure 1.3 provides an overview of current leading LLMs, highlighting their capabilities and applications.\\n1.5\\nWhat is Fine-Tuning?\\nFine-tuning uses a pre-trained model, such as OpenAI’s GPT series, as a foundation.\\nThe process\\ninvolves further training on a smaller, domain-specific dataset. This approach builds upon the model’s\\npre-existing knowledge, enhancing performance on specific tasks with reduced data and computational\\nrequirements.\\nFine-tuning transfers the pre-trained model’s learned patterns and features to new tasks, improving\\nperformance and reducing training data needs. It has become popular in NLP for tasks like text classi-\\nfication, sentiment analysis, and question-answering.\\n8\\nFigure 1.3: Mind map depicting various dimensions of Large Language Models (LLMs), covering aspects\\nfrom pre-training and fine-tuning methodologies to efficiency, evaluation, inference, and application do-\\nmains. Each dimension is linked to specific techniques, challenges, and examples of models that exemplify\\nthe discussed characteristics. This diagram serves as an overview of the multifaceted considerations in\\nthe development and deployment of LLMs. (adapted from [13])\\n1.6\\nTypes of LLM Fine-Tuning\\n1.6.1\\nUnsupervised Fine-Tuning\\nThis method does not require labelled data. Instead, the LLM is exposed to a large corpus of unla-\\nbelled text from the target domain, refining its understanding of language. This approach is useful for\\nnew domains like legal or medical fields but is less precise for specific tasks such as classification or\\nsummarisation.\\n1.6.2\\nSupervised Fine-Tuning (SFT)\\nSFT involves providing the LLM with labelled data tailored to the target task. For example, fine-tuning\\nan LLM for text classification in a business context uses a dataset of text snippets with class labels.\\nWhile effective, this method requires substantial labelled data, which can be costly and time-consuming\\nto obtain.\\n9\\n1.6.3\\nInstruction Fine-Tuning via Prompt Engineering\\nThis method relies on providing the LLM with natural language instructions, useful for creating spe-\\ncialised assistants. It reduces the need for vast amounts of labelled data but depends heavily on the\\nquality of the prompts.\\n1.7\\nPre-training vs Fine-tuning\\nTable 1.1 provides a comparison between pre-training and fine-tuning, highlighting their respective char-\\nacteristics and processes.\\nAspect\\nPre-training\\nFine-tuning\\nDefinition\\nTraining on a vast amount of\\nunlabelled text data\\nAdapting a pre-trained model to\\nspecific tasks\\nData Requirement\\nExtensive\\nand\\ndiverse\\nunla-\\nbelled text data\\nSmaller,\\ntask-specific labelled\\ndata\\nObjective\\nBuild general linguistic knowl-\\nedge\\nSpecialise\\nmodel\\nfor\\nspecific\\ntasks\\nProcess\\nData\\ncollection,\\ntraining\\non\\nlarge\\ndataset,\\npredict\\nnext\\nword/sequence\\nTask-specific\\ndata\\ncollection,\\nmodify last layer for task, train\\non new dataset, generate output\\nbased on tasks\\nModel Modification\\nEntire model trained\\nLast layers adapted for new task\\nComputational Cost\\nHigh (large dataset,\\ncomplex\\nmodel)\\nLower (smaller dataset,\\nfine-\\ntuning layers)\\nTraining Duration\\nWeeks to months\\nDays to weeks\\nPurpose\\nGeneral language understand-\\ning\\nTask-specific performance im-\\nprovement\\nExamples\\nGPT, LLaMA 3\\nFine-tuning LLaMA 3 for sum-\\nmarisation\\nTable 1.1: A Comparative Overview of Pre-training and Fine-tuning in Large Language Models (LLMs).\\nThe table outlines key differences between the pre-training and fine-tuning phases across various aspects\\nsuch as definition, data requirements, objectives, processes, model modification, computational costs,\\ntraining duration, and their respective purposes, with examples highlighting specific models and tasks.\\nPre-training involves extensive training on vast amounts of unlabelled data to build general linguistic\\nknowledge, while fine-tuning adapts the pre-trained models to specialised tasks using smaller, labelled\\ndatasets, focusing on task-specific performance improvements.\\n1.8\\nImportance of Fine-Tuning LLMs\\n1. Transfer Learning: Fine-tuning leverages the knowledge acquired during pre-training, adapting\\nit to specific tasks with reduced computation time and resources.\\n2. Reduced Data Requirements: Fine-tuning requires less labelled data, focusing on tailoring\\npre-trained features to the target task.\\n3. Improved Generalisation: Fine-tuning enhances the model’s ability to generalise to specific\\ntasks or domains, capturing general language features and customising them.\\n4. Efficient Model Deployment: Fine-tuned models are more efficient for real-world applications,\\nbeing computationally efficient and well-suited for specific tasks.\\n5. Adaptability to Various Tasks: Fine-tuned LLMs can adapt to a broad range of tasks, per-\\nforming well across various applications without task-specific architectures.\\n6. Domain-Specific Performance: Fine-tuning allows models to excel in domain-specific tasks by\\nadjusting to the nuances and vocabulary of the target domain.\\n10\\n7. Faster Convergence: Fine-tuning usually achieves faster convergence, starting with weights that\\nalready capture general language features.\\n1.9\\nRetrieval Augmented Generation (RAG)\\nA popular method to utilise your own data is by incorporating it into the prompt when querying the LLM\\nmodel. This approach, known as Retrieval-Augmented Generation (RAG), involves retrieving relevant\\ndata and using it as additional context for the LLM. Instead of depending solely on knowledge from the\\ntraining data, a RAG workflow pulls pertinent information, connecting static LLMs with real-time data\\nretrieval. With RAG architecture, organisations can deploy any LLM model and enhance it to return\\nrelevant results by providing a small amount of their own data (see Figure1.4 for visual workflow). This\\nprocess avoids the costs and time associated with fine-tuning or pre-training the model.\\nFigure 1.4: An illustration of the Traditional Retrieval-Augmented Generation (RAG) pipeline steps,\\ndepicting the sequential process from client query to response generation.\\nThe pipeline starts with\\nthe client’s question, followed by semantic search in a vector database, contextually enriching the data\\nbefore generating a prompt for the large language model (LLM). The final response is post-processed\\nand returned to the client.\\n1.9.1\\nTraditional RAG Pipeline and Steps\\n1. Data Indexing: Organise data efficiently for quick retrieval. This involves processing, chunking,\\nand storing data in a vector database using indexing strategies like search indexing, vector indexing,\\nand hybrid indexing.\\n2. Input Query Processing: Refine user queries to improve compatibility with indexed data. This\\ncan include simplification or vector transformation of queries for enhanced search efficiency.\\n3. Searching and Ranking: Retrieve and rank data based on relevance using search algorithms\\nsuch as TF-IDF, BM25, and deep learning models like BERT to interpret the query’s intent and\\ncontext.\\n4. Prompt Augmentation: Incorporate relevant information from the search results into the origi-\\nnal query to provide the LLM with additional context, enhancing response accuracy and relevance.\\n11\\n5. Response Generation: Use the augmented prompt to generate responses that combine the LLM’s\\nknowledge with current, specific data, ensuring high-quality, contextually grounded answers.\\n1.9.2\\nBenefits of Using RAG\\n• Up-to-Date and Accurate Responses: Enhances the LLM’s responses with current external\\ndata, improving accuracy and relevance.\\n• Reducing Inaccurate Responses: Grounds the LLM’s output in relevant knowledge, reducing\\nthe risk of generating incorrect information.\\n• Domain-Specific Responses: Delivers contextually relevant responses tailored to an organisa-\\ntion’s proprietary data.\\n• Efficiency and Cost-Effectiveness: Offers a cost-effective method for customising LLMs without\\nextensive model fine-tuning.\\n1.9.3\\nChallenges and Considerations in Serving RAG\\n1. User Experience: Ensuring rapid response times suitable for real-time applications.\\n2. Cost Efficiency: Managing the costs associated with serving millions of responses.\\n3. Accuracy: Ensuring outputs are accurate to avoid misinformation.\\n4. Recency and Relevance: Keeping responses and content current with the latest data.\\n5. Business Context Awareness: Aligning LLM responses with specific business contexts.\\n6. Service Scalability: Managing increased capacity while controlling costs.\\n7. Security and Governance: Implementing protocols for data security, privacy, and governance.\\n1.9.4\\nUse Cases and Examples\\n1. Question and Answer Chatbots: Integrate LLMs with chatbots to generate accurate answers\\nfrom company documents, enhancing customer support.\\n2. Search Augmentation: Enhance search engines with LLM-generated answers for more accurate\\ninformational queries.\\n3. Knowledge Engine: Use LLMs to answer questions related to internal functions, such as HR\\nand compliance, using company data.\\n1.9.5\\nConsiderations for Choosing Between RAG and Fine-Tuning\\nWhen considering external data access, RAG is likely a superior option for applications needing to access\\nexternal data sources. Fine-tuning, on the other hand, is more suitable if you require the model to ad-\\njust its behaviour, and writing style, or incorporate domain-specific knowledge. In terms of suppressing\\nhallucinations and ensuring accuracy, RAG systems tend to perform better as they are less prone to gen-\\nerating incorrect information. If you have ample domain-specific, labelled training data, fine-tuning can\\nresult in a more tailored model behaviour, whereas RAG systems are robust alternatives when such data\\nis scarce. RAG systems provide an advantage with dynamic data retrieval capabilities for environments\\nwhere data frequently updates or changes. Additionally, it is crucial to ensure the transparency and\\ninterpret ability of the model’s decision-making process. In that case, RAG systems offer insight that is\\ntypically not available in models that are solely fine-tuned. Figure1.5 illustrates the visual representation\\nalongside example use cases.\\n12\\nFigure 1.5: Graph comparing the model adaptation required versus the level of external knowledge needed\\nacross different scenarios, highlighting the roles of Retrieval-Augmented Generation (RAG), Fine-Tuning,\\nand their hybrid applications in various contexts such as Q&A systems, customer support automation,\\nand summarisation tasks. (adapted from [14])\\n1.10\\nObjectives of the Report\\n1.10.1\\nGoals and Scope\\nThe primary goal of this report is to conduct a comprehensive analysis of fine-tuning techniques for LLMs.\\nThis involves exploring theoretical foundations, practical implementation strategies, and challenges. The\\nreport examines various fine-tuning methodologies, their applications, and recent advancements.\\n1.10.2\\nKey Questions and Issues Addressed\\nThis report addresses critical questions surrounding fine-tuning LLMs, starting with foundational in-\\nsights into LLMs, their evolution, and significance in NLP. It defines fine-tuning, distinguishes it from\\npre-training, and emphasises its role in adapting models for specific tasks. Key objectives include en-\\nhancing model performance for targeted applications and domains.\\nThe report outlines a structured fine-tuning process, featuring a high-level pipeline with visual rep-\\nresentations and detailed stage explanations. It covers practical implementation strategies, including\\nmodel initialisation, hyperparameter definition, and fine-tuning techniques such as Parameter-Efficient\\nFine-Tuning (PEFT) and Retrieval-Augmented Generation (RAG). Industry applications, evaluation\\nmethods, deployment challenges, and recent advancements are also explored.\\n1.10.3\\nOverview of the Report Structure\\nThe rest of the report provides a comprehensive understanding of fine-tuning LLMs. The main chapters\\ninclude an in-depth look at the fine-tuning pipeline, practical applications, model alignment, evaluation\\nmetrics, and challenges. The concluding sections discuss the evolution of fine-tuning techniques, highlight\\nongoing research challenges, and provide insights for researchers and practitioners.\\n13\\nChapter 2\\nSeven Stage Fine-Tuning Pipeline\\nfor LLM\\nFine-tuning a Large Language Model (LLM) is a comprehensive process divided into seven distinct\\nstages, each essential for adapting the pre-trained model to specific tasks and ensuring optimal per-\\nformance. These stages encompass everything from initial dataset preparation to the final deployment\\nand maintenance of the fine-tuned model. By following these stages systematically, the model is refined\\nand tailored to meet precise requirements, ultimately enhancing its ability to generate accurate and\\ncontextually appropriate responses. The seven stages include Dataset Preparation, Model Initialisation,\\nTraining Environment Setup, Fine-Tuning, Evaluation and Validation, Deployment, and Monitoring and\\nMaintenance.\\nFigure 2.1 illustrates the comprehensive pipeline for fine-tuning LLMs, encompassing all necessary stages\\nfrom dataset preparation to monitoring and maintenance.\\n2.1\\nStage 1: Dataset Preparation\\nFine-tuning a Large Language Model (LLM) starts with adapting the pre-trained model for specific tasks\\nby updating its parameters using a new dataset. This involves cleaning and formatting the dataset to\\nmatch the target task, such as instruction tuning, sentiment analysis, or topic mapping. The dataset is\\ncomposed of < input, output > pairs, demonstrating the desired behaviour for the model.\\nFor example, in instruction tuning, the dataset may look like:\\n###Human: $<Input Query>$\\n###Assistant: $<Generated Output>$\\nHere, the ’Input Query’ is what the user asks, and the ’Generated Output’ is the model’s response. The\\nstructure and style of these pairs can be adjusted based on the specific needs of the task.\\n2.2\\nStage 2: Model Initialisation\\nModel initialisation is the process of setting up the initial parameters and configurations of the LLM\\nbefore training or deploying it. This step is crucial for ensuring the model performs optimally, trains\\nefficiently, and avoids issues such as vanishing or exploding gradients.\\n2.3\\nStage 3: Training Environment Setup\\nSetting up the training environment for LLM fine-tuning involves configuring the necessary infrastructure\\nto adapt a pre-existing model for specific tasks. This includes selecting relevant training data, defining the\\nmodel’s architecture and hyperparameters, and running training iterations to adjust the model’s weights\\nand biases.\\nThe aim is to enhance the LLM’s performance in generating accurate and contextually\\nappropriate outputs tailored to specific applications, like content creation, translation, or sentiment\\nanalysis. Successful fine-tuning relies on careful preparation and rigorous experimentation.\\n14\\nFigure 2.1: A comprehensive pipeline for fine-tuning Large Language Models (LLMs), illustrating the\\nseven essential stages: Dataset Preparation, Model Initialisation, Training Environment Setup, Fine-\\nTuning, Evaluation and Validation, Deployment, and Monitoring and Maintenance. Each stage plays\\na crucial role in adapting the pre-trained model to specific tasks and ensuring optimal performance\\nthroughout its lifecycle.\\n2.4\\nStage 4: Partial or Full Fine-Tuning\\nThis stage involves updating the parameters of the LLM using a task-specific dataset. Full fine-tuning up-\\ndates all parameters of the model, ensuring comprehensive adaptation to the new task. Alternatively, Half\\nfine-tuning (HFT) [15] or Parameter-Efficient Fine-Tuning (PEFT) approaches, such as using adapter\\nlayers, can be employed to partially fine-tune the model. This method attaches additional layers to the\\npre-trained model, allowing for efficient fine-tuning with fewer parameters, which can address challenges\\nrelated to computational efficiency, overfitting, and optimisation.\\n2.5\\nStage 5: Evaluation and Validation\\nEvaluation and validation involve assessing the fine-tuned LLM’s performance on unseen data to ensure\\nit generalises well and meets the desired objectives. Evaluation metrics, such as cross-entropy, measure\\nprediction errors, while validation monitors loss curves and other performance indicators to detect issues\\nlike overfitting or underfitting.\\nThis stage helps guide further fine-tuning to achieve optimal model\\nperformance.\\n15\\n2.6\\nStage 6: Deployment\\nDeploying an LLM means making it operational and accessible for specific applications. This involves\\nconfiguring the model to run efficiently on designated hardware or software platforms, ensuring it can\\nhandle tasks like natural language processing, text generation, or user query understanding. Deployment\\nalso includes setting up integration, security measures, and monitoring systems to ensure reliable and\\nsecure performance in real-world applications.\\n2.7\\nStage 7: Monitoring and Maintenance\\nMonitoring and maintaining an LLM after deployment is crucial to ensure ongoing performance and\\nreliability.\\nThis involves continuously tracking the model’s performance, addressing any issues that\\narise, and updating the model as needed to adapt to new data or changing requirements.\\nEffective\\nmonitoring and maintenance help sustain the model’s accuracy and effectiveness over time.\\n16\\nChapter 3\\nStage 1: Data Preparation\\n3.1\\nSteps Involved in Data Preparation\\n3.1.1\\nData Collection\\nThe first step in data preparation is to collect data from various sources. These sources can be in any\\nformat such as CSV, web pages, SQL databases, S3 storage, etc. Python provides several libraries to\\ngather the data efficiently and accurately. Table 3.1 presents a selection of commonly used data formats\\nalong with the corresponding Python libraries used for data collection.\\n3.1.2\\nData Preprocessing and Formatting\\nData preprocessing and formatting are crucial for ensuring high-quality data for fine-tuning. This step\\ninvolves tasks such as cleaning the data, handling missing values, and formatting the data to match the\\nspecific requirements of the task. Several libraries assist with text data processing and Table 3.2 contains\\nsome of the most commonly used data preprocessing libraries in python.\\n3.1.3\\nHandling Data Imbalance\\nHandling imbalanced datasets is crucial for ensuring balanced performance across all classes. Several\\ntechniques and strategies are employed:\\n1. Over-sampling and Under-sampling:\\nTechniques like SMOTE (Synthetic Minority Over-\\nsampling Technique) generate synthetic examples to achieve balance.\\nPython Library: imbalanced-learn\\nDescription: imbalanced-learn provides various methods to deal with imbalanced datasets, in-\\ncluding oversampling techniques like SMOTE.\\n2. Adjusting Loss Function: Modify the loss function to give more weight to the minority class,\\nsetting class weights inversely proportional to the class frequencies.\\n3. Focal Loss: A variant of cross-entropy loss that adds a factor to down-weight easy examples and\\nfocus training on hard negatives.\\nPython Library: focal loss\\nDescription: The focal loss package provides robust implementations of various focal loss func-\\ntions, including BinaryFocalLoss and SparseCategoricalFocalLoss.\\n4. Cost-sensitive Learning: Incorporating the cost of misclassifications directly into the learning\\nalgorithm, assigning a higher cost to misclassifying minority class samples.\\n5. Ensemble Methods: Using techniques like bagging and boosting to combine multiple models\\nand handle class imbalance.\\nPython Library: sklearn.ensemble\\nDescription: scikit-learn provides robust implementations of various ensemble methods, including\\nbagging and boosting.\\n17\\nData Format\\nPython\\nLi-\\nbrary\\nDescription\\nLibrary Link\\nCSV Files\\npandas\\npandas is a powerful library for data ma-\\nnipulation and analysis. It provides the\\nread csv function for easy and efficient\\nreading of CSV files into DataFrame ob-\\njects.\\nIt also supports reading data in\\nExcel, JSON, and more.\\npandas documenta-\\ntion\\nWeb Pages\\nBeautifulSoup\\nand requests\\nBeautifulSoup is a library for parsing\\nHTML and XML documents. Combined\\nwith requests for sending HTTP re-\\nquests, it enables data extraction from\\nweb pages, essential for web scraping\\ntasks.\\nBeautifulSoup\\ndocumentation,\\nrequests documen-\\ntation\\nSQL Databases\\nSQLAlchemy\\nSQLAlchemy\\nis\\na\\nSQL\\ntoolkit\\nand\\nObject-Relational Mapping (ORM) li-\\nbrary for Python, providing a full suite\\nof enterprise-level persistence patterns.\\nSQLAlchemy docu-\\nmentation\\nS3 Storage\\nboto3\\nboto3 is the Amazon Web Services\\n(AWS) SDK for Python, allowing devel-\\nopers to use services like Amazon S3 and\\nEC2. It enables interaction with AWS\\nservices, including uploading, download-\\ning, and managing S3 bucket files.\\nboto3\\ndocumenta-\\ntion\\nData\\nIntegra-\\ntion\\nRapidMiner\\nRapidMiner is a comprehensive envi-\\nronment for data preparation, machine\\nlearning, and predictive analytics, allow-\\ning efficient processing and transforma-\\ntion of raw data into actionable insights.\\nRapidMiner\\ndocu-\\nmentation\\nData Cleaning\\nTrifacta\\nWran-\\ngler\\nTrifacta Wrangler focuses on simplify-\\ning and automating data wrangling pro-\\ncesses, transforming raw data into clean\\nand structured formats.\\nTrifacta\\nWrangler\\ndocumentation\\nTable 3.1: Python libraries and tools for data collection and integration in various formats, providing\\nan overview of commonly used libraries, their functions, and links to their official documentation for\\nefficient data management and processing.\\n6. Stratified Sampling: Ensuring that each mini-batch during training contains an equal or pro-\\nportional representation of each class.\\nPython Library: sklearn.model selection.StratifiedShuffleSplit\\nDescription: scikit-learn offers tools for stratified sampling, ensuring balanced representation\\nacross classes.\\n7. Data Cleaning: Removing noisy and mislabelled data, which can disproportionately affect the\\nminority class.\\nPython Library: pandas.DataFrame.sample\\nDescription: pandas provides methods for sampling data from DataFrames, useful for data clean-\\ning and preprocessing.\\n8. Using Appropriate Metrics: Metrics like Precision-Recall AUC, F1-score, and Cohen’s Kappa\\nare more informative than accuracy when dealing with imbalanced datasets.\\nPython Library: sklearn.metrics\\nDescription: scikit-learn offers a comprehensive set of tools for evaluating the performance of\\nclassification models, particularly with imbalanced datasets.\\n18\\nLibrary Name\\nData Preprocessing Options\\nLink\\nspaCy\\nspaCy provides robust capabilities for text prepro-\\ncessing, including tokenization, lemmatization, and\\nefficient sentence boundary detection.\\nspaCy documentation\\nNLTK\\nNLTK offers a comprehensive set of tools for data\\npreprocessing, such as tokenization, stemming, and\\nstop word removal.\\nNLTK documentation\\nHuggingFace\\nHuggingFace provides extensive capabilities for\\ntext preprocessing through its transformers library,\\nincluding functionalities for tokenization and sup-\\nport for various pre-trained models.\\nHuggingFace documentation\\nKNIME\\nKNIME Analytics Platform allows visual workflow\\ndesign for data integration, preprocessing, and ad-\\nvanced manipulations like text mining and image\\nanalysis.\\nKNIME documentation\\nTable 3.2: Outline of Python libraries commonly used for text data preprocessing, including spaCy,\\nNLTK, HuggingFace, and KNIME. It details the specific preprocessing options offered by each library\\nand provides links to their official documentation for users seeking more in-depth guidance on their use.\\n3.1.4\\nSplitting Dataset\\nSplitting the dataset for fine-tuning involves dividing it into training and validation sets, typically using\\nan 80:20 ratio. Different techniques include:\\n1. Random Sampling: Selecting a subset of data randomly to create a representative sample.\\nPython Library: sklearn.model selection.train test split\\n2. Stratified Sampling: Dividing the dataset into subgroups and sampling from each to maintain\\nclass balance.\\nPython Library: sklearn.model selection.StratifiedShuffleSplit\\n3. K-Fold Cross Validation: Splitting the dataset into K folds and performing training and vali-\\ndation K times.\\nPython Library: sklearn.model selection.KFold\\n4. Leave-One-Out Cross Validation: Using a single data point as the validation set and the rest\\nfor training, repeated for each data point.\\nPython Library: sklearn.model selection.LeaveOneOut\\nFurther details can be found in scikit-learn’s documentation on model selection.\\n3.2\\nExisting and Potential Research Methodologies\\n3.2.1\\nData Annotation\\nData annotation involves labelling or tagging textual data with specific attributes relevant to the model’s\\ntraining objectives.\\nThis process is crucial for supervised learning tasks and greatly influences the\\nperformance of the fine-tuned model. Recent research highlights various approaches to data annotation:\\n• Human Annotation: Manual annotation by human experts remains a gold standard due to its\\naccuracy and context understanding. However, it is time-consuming and costly for large datasets\\n[16]. Tools like Excel, Prodigy1, and Innodata2 facilitate this process.\\n• Semi-automatic Annotation: Combining machine learning algorithms with human review to\\ncreate labelled datasets more efficiently. This approach balances efficiency and accuracy. Tools\\nlike Snorkel3 use weak supervision to generate initial labels, which are then refined by human\\nannotators [17].\\n1https://prodi.gy\\n2https://innodata.com/\\n3https://snorkel.ai/\\n19\\n• Automatic Annotation: Fully automated annotation leverages machine learning algorithms to\\nlabel data without human intervention, offering scalability and cost-effectiveness.\\nServices like\\nAmazon SageMaker Ground Truth4 utilise machine learning to automate data labelling, al-\\nthough the accuracy may vary depending on the complexity of the task [18].\\n3.2.2\\nData Augmentation\\nData Augmentation (DA) techniques expand training datasets artificially to address data scarcity and\\nimprove model performance. Advanced techniques often used in NLP include:\\n• Word Embeddings: Using word embeddings like Word2Vec and GloVe to replace words with\\ntheir semantic equivalents, thereby generating new data instances [19, 20].\\n• Back Translation: Translating text to another language and then back to the original language\\nto create paraphrased data. This technique helps in generating diverse training samples [21]. Tools\\nlike Google Translate API5 are commonly used for this purpose.\\n• Adversarial Attacks: Generating augmented data through adversarial examples that slightly\\nmodify the original text to create new training samples while preserving the original meaning [22].\\nLibraries like TextAttack6 provide frameworks for such augmentations.\\n• NLP-AUG7: This library offers a variety of augmenters for character, word, sentence, audio, and\\nspectrogram augmentation, enhancing dataset diversity.\\n3.2.3\\nSynthetic Data Generation using LLMs\\nLarge Language Models (LLMs) can generate synthetic data through innovative techniques such as:\\n• Prompt Engineering: Crafting specific prompts to guide LLMs like GPT-3 in generating relevant\\nand high-quality synthetic data [23].\\n• Multi-Step Generation: Employing iterative generation processes where LLMs generate initial\\ndata that is refined through subsequent steps [24]. This method can produce high-quality synthetic\\ndata for various tasks, including summarising and bias detection.\\nIt is crucial to verify the accuracy and relevance of synthetic data generated by LLMs before using\\nthem for fine-tuning processes [25].\\n3.3\\nChallenges in Data Preparation for Fine-Tuning LLMs\\nKey challenges in data preparation include:\\n1. Domain Relevance: Ensuring that the data is relevant to the specific domain for accurate model\\nperformance. Mismatched domain data can lead to poor generalisation and inaccurate outputs\\n[26].\\n2. Data Diversity: Including diverse and well-balanced data to prevent model biases and improve\\ngeneralisation. A lack of diversity can cause the model to perform poorly on underrepresented\\nscenarios [27].\\n3. Data Size: Managing and processing large datasets, with at least 1000 samples recommended for\\neffective fine-tuning. However, large datasets pose challenges in terms of storage, computational\\nrequirements, and processing time.\\n4. Data Cleaning and Preprocessing: Removing noise, errors, and inconsistencies are critical for\\nproviding clean inputs to the model. Poorly preprocessed data can degrade model performance\\nsignificantly.\\n4https://aws.amazon.com/sagemaker/groundtruth/\\n5https://translate.google.com/?sl=auto&tl=en&op=translate\\n6https://github.com/QData/TextAttack\\n7https://github.com/makcedward/nlpaug\\n20\\n5. Data Annotation: Ensuring precise and consistent labelling is essential for tasks requiring la-\\nbelled data. Inconsistent annotation can lead to unreliable model predictions.\\n6. Handling Rare Cases: Adequately representing rare but important instances in the dataset to\\nensure the model can generalise to less frequent but critical scenarios.\\n7. Ethical Considerations: Scrutinising data for harmful or biased content to prevent unintended\\nconsequences. Ethical data handling includes removing biases and ensuring privacy [28].\\n3.4\\nAvailable LLM Fine-Tuning Datasets\\nFor a comprehensive list of datasets suitable for fine-tuning LLMs, refer to resources like LLMXplorer,\\nwhich provides domain and task-specific datasets.\\n3.5\\nBest Practices\\n3.5.1\\nHigh-Quality Data Collection\\nEnsuring high-quality, diverse, and representative data is critical. Leveraging curated sources and en-\\nsuring comprehensive coverage across different scenarios enhances model robustness [29].\\nTools like\\nDataRobot Paxata8 and KNIME Analytics Platform9 offer robust data profiling and transforma-\\ntion capabilities.\\n3.5.2\\nEffective Data Preprocessing\\nProper data preprocessing is essential for model performance. Utilising libraries like spaCy, NLTK, and\\nHuggingFace Transformers can streamline preprocessing tasks. Platforms like Trifacta Wrangler\\nand RapidMiner automate data cleaning tasks, improving efficiency and ensuring consistency [30].\\n3.5.3\\nManaging Data Imbalance\\nAddressing data imbalance is crucial.\\nTechniques like over-sampling, under-sampling, and SMOTE\\nhelp balance datasets. Libraries like imbalanced-learn and ensemble methods in scikit-learn provide\\nrobust tools for managing imbalanced datasets [31].\\n3.5.4\\nAugmenting and Annotating Data\\nData augmentation and annotation improve model robustness. Tools like NLP-AUG, TextAttack,\\nand Snorkel offer sophisticated capabilities for creating diverse and well-labelled datasets [32, 33].\\n3.5.5\\nEthical Data Handling\\nEnsuring ethical data handling involves thorough scrutiny for biases and privacy concerns. Implement-\\ning privacy-preserving techniques and filtering harmful content is critical. Services like Amazon Sage-\\nMaker Ground Truth ensure scalable and secure data annotation [34].\\n3.5.6\\nRegular Evaluation and Iteration\\nContinuous evaluation and iteration of the data preparation pipeline help maintain data quality and\\nrelevance. Leveraging feedback loops and performance metrics ensures ongoing improvements and adap-\\ntation to new data requirements.\\nBy integrating these best practices, researchers and practitioners can enhance the effectiveness of LLM\\nfine-tuning, ensuring robust and reliable model performance.\\n8https://www.datarobot.com/platform/preparation/\\n9https://www.knime.com/\\n21\\nChapter 4\\nStage 2: Model Initialisation\\n4.1\\nSteps Involved in Model Initialisation\\nFigure 4.1: Sequential steps involved in Initialising a Large Language Model (LLM), illustrating the\\nprocess from setting up the environment to executing tasks. Each step is critical for ensuring that the\\nLLM is correctly configured and ready for operation. This includes installing necessary dependencies,\\nimporting libraries, selecting and downloading the appropriate language model from a repository, and\\nfinally, loading the model to perform specific tasks.\\n1. Set Up the Environment: Configure your environment, such as setting up GPU/TPU usage if\\navailable, which can significantly speed up model loading and inference.\\n2. Install the Dependencies: Ensure that all necessary software and libraries are installed. This\\ntypically includes package managers like pip and frameworks like PyTorch or TensorFlow.\\n22\\n3. Import the Libraries: Import the required libraries in your script or notebook. Common libraries\\ninclude transformers from Hugging Face, torch for PyTorch, and other utility libraries.\\n4. Choose the Language Model: Select the appropriate pre-trained language model based on your\\ntask requirements. This could be models like BERT, GPT-3, or others available on platforms like\\nHugging Face’s Model Hub.\\n5. Download the Model from the Repository: Use the chosen framework’s functions to download\\nthe pre-trained model from an online repository. For instance, using transformers, you might use\\nAutoModel.from pretrained(’model name’).\\n6. Load the Model in the Memory: Load the model into memory, ready for inference or further\\nfine-tuning. This step ensures the model weights are initialised and ready for use.\\n7. Execute Tasks: Perform the desired tasks using the loaded model. This could involve making\\npredictions, generating text, or fine-tuning the model on a new dataset.\\n4.2\\nTools and Libraries for Model Initialisation\\nPython offers a wide range of libraries for Initialising large language models, providing access to both\\nopen and closed-source models. Here are some notable libraries:\\n1. Python Library: HuggingFace\\nDescription: HuggingFace is renowned for its support of numerous pre-trained large language\\nmodels, ranging from Phi-3 mini to Llama-3 70B. The transformers library, part of HuggingFace,\\nenables users to access these models via classes such as AutoModelForCausalLM. This library\\nsupports loading fine-tuned models as well as 4-bit quantised models. Additionally, the transformers\\nlibrary includes the ”pipeline” feature, making it easy to use pre-trained models for various tasks\\n[35].\\n2. Python Framework: PyTorch\\nDescription: PyTorch offers comprehensive tools and libraries for Initialising and fine-tuning\\nlarge language models. It provides a flexible and efficient platform for building and deploying deep\\nlearning models. HuggingFace’s transformers library bridges the gap between PyTorch and other\\nframeworks, enhancing its usability for state-of-the-art language models [36].\\n3. Python Framework: TensorFlow\\nDescription: TensorFlow also provides extensive tools and libraries for Initialising and fine-tuning\\nlarge language models. Similar to PyTorch, it benefits from the HuggingFace transformers library,\\nwhich provides a versatile and user-friendly API and interface for working with the latest advance-\\nments in large language models [37].\\n23\\n4.3\\nChallenges in Model Initialisation\\nChallenge\\nDescription\\nAlignment with the\\nTarget Task\\nIt’s essential that the pre-trained model closely aligns with your specific\\ntask or domain. This initial alignment serves as a solid foundation for\\nfurther fine-tuning efforts, leading to improved efficiency and results [38].\\nUnderstanding the\\nPre-trained Model\\nBefore making a selection, it’s crucial to thoroughly comprehend the\\narchitecture, capabilities, limitations, and the tasks the model was orig-\\ninally trained on. Without this understanding, fine-tuning efforts may\\nnot yield the desired outcomes [23].\\nAvailability and\\nCompatibility\\nCareful consideration of a model’s documentation, license, maintenance,\\nand update frequency is necessary to avoid potential issues and ensure\\nsmooth integration into your application.\\nModel Architecture\\nNot all models excel at every task.\\nEach model architecture has its\\nstrengths and weaknesses, so selecting one aligned with your specific\\ntask is essential for favourable outcomes [39].\\nResource Constraints\\nLoading pre-trained LLMs is resource-heavy and requires more compu-\\ntation. These models need high-performance CPUs and GPUs and a\\nsignificant amount of disk space. For instance, the Llama 3 8B model\\nrequires a minimum of 16GB of memory to load and run the inference.\\nPrivacy\\nPrivacy and confidentiality are crucial factors when selecting a large lan-\\nguage model (LLM). Many businesses prefer not to share their data\\nwith external LLM providers.\\nIn such instances, hosting an LLM on\\nlocal servers or using pre-trained LLMs available through private cloud\\nproviders can be viable solutions. These approaches ensure that data\\nremains within the company’s premises, thereby preserving privacy and\\nconfidentiality.\\nCost and Maintenance\\nHosting LLMs on local servers entails significant time and expense for\\nsetup and ongoing maintenance. Conversely, utilising cloud vendors al-\\nleviates concerns about resource maintenance but incurs monthly billing\\ncosts. These charges are typically based on factors such as model size\\nand the volume of requests per minute.\\nModel Size and\\nQuantisation\\nutilising a pre-trained model with high memory consumption can still be\\nviable by employing its quantised version. Through quantisation, pre-\\ntrained weights can be loaded with reduced precision, typically 4-bit or\\n8-bit floating point, substantially diminishing parameter volume while\\nmaintaining considerable accuracy [40].\\nPre-training Datasets\\nExamine the datasets used for pre-training to gauge the model’s under-\\nstanding of language. These are important as there are models available\\nspecifically for performing code generation, and we do not want to use\\nthose models for finance text classification [41].\\nBias Awareness\\nBe vigilant regarding potential biases in pre-trained models, especially if\\nunbiased predictions are required. The bias awareness can be evaluated\\nby testing different models and backtracking the datasets used for pre-\\ntraining [42].\\nTable 4.1: Comprehensive Overview of Challenges in Initialising a Large Language Model (LLM). This\\ntable highlights critical considerations, such as the importance of aligning pre-trained models with specific\\ntasks, understanding model architecture and compatibility, managing resource constraints, and ensuring\\ndata privacy. Additionally, it discusses the challenges related to cost, maintenance, and the complexities\\nof model size, quantisation, and bias awareness. Each challenge is associated with specific references to\\nensure thorough understanding and proper model deployment.\\n4.4\\nTutorials\\n1. Summarisation using Llama 3\\n24\\n2. HuggingFace tutorial for getting started with LLMs\\n3. PyTorch tutorial for fine-tuning models\\n4. TensorFlow tutorial for transformer models\\n25\\nChapter 5\\nStage 3: Training Setup\\n5.1\\nSteps Involved in Training Setup\\n1. Setting up the training environment: When setting up the environment for training an LLM,\\nit is crucial to configure high-performance hardware, such as GPUs or TPUs, and ensure proper\\ninstallation of necessary software components like CUDA, cuDNN, and deep learning frameworks\\nsuch as PyTorch or TensorFlow. Verify hardware recognition and compatibility with the software to\\nleverage computational power effectively, reducing training time and improving model performance.\\n2. Defining the Hyper-parameters: When defining hyperparameters for fine-tuning an LLM, it is\\nessential to carefully tune key parameters such as learning rate, batch size, and epochs to optimise\\nthe model’s performance.\\n3. Initialising Optimisers and Loss Functions: When initialising optimisers and loss functions\\nfor fine-tuning an LLM, it is crucial to select the appropriate optimiser to efficiently update the\\nmodel’s weights and the correct loss function to measure model performance [43].\\n5.2\\nSetting up Training Environment\\nWhen fine-tuning a large language model (LLM), the computational environment plays a crucial role in\\nensuring efficient training. To achieve optimal performance, it’s essential to configure the environment\\nwith high-performance hardware such as GPUs (Graphics Processing Units) or TPUs (Tensor Processing\\nUnits). GPUs, such as the NVIDIA A100 or V100, are widely used for training deep learning models\\ndue to their parallel processing capabilities. For larger-scale operations, TPUs offered by Google Cloud\\ncan provide even greater acceleration [44].\\nFirst, ensure that your system or cloud environment has the necessary hardware installed. For GPUs,\\nthis involves setting up CUDA1 (Compute Unified Device Architecture) and cuDNN2 (CUDA Deep Neu-\\nral Network library) from NVIDIA, which are essential for enabling GPU acceleration. For TPU usage,\\nyou would typically set up a Google Cloud environment with TPU instances, which includes configuring\\nthe TPU runtime in your training scripts.\\nVerify that your hardware is correctly recognised and utilised by your deep learning frameworks. In\\nPyTorch, for instance, you can check GPU availability with torch.cuda.is available(). Properly setting\\nup and testing the hardware ensures that the training process can leverage the computational power\\neffectively, reducing training time and improving model performance [36].\\nWhen fine-tuning an LLM, both software and hardware considerations are paramount to ensure a smooth\\nand efficient training process. On the software side, you need a compatible deep learning framework like\\nPyTorch or TensorFlow. These frameworks have extensive support for LLMs and provide utilities for\\nefficient model training and evaluation. Installing the latest versions of these frameworks, along with\\nany necessary dependencies, is crucial for leveraging the latest features and performance improvements\\n1https://developer.nvidia.com/cuda-toolkit\\n2https://developer.nvidia.com/cudnn\\n26\\n[45].\\nAdditionally, use libraries like Hugging Face’s transformers to simplify the process of loading pre-trained\\nmodels and tokenizers. This library is particularly well-suited for working with various LLMs and offers\\na user-friendly interface for model fine-tuning. Ensure that all software components, including libraries\\nand dependencies, are compatible with your chosen framework and hardware setup [35].\\nOn the hardware side, consider the memory requirements of the model and your dataset. LLMs typ-\\nically require substantial GPU memory, so opting for GPUs with higher VRAM (e.g., 16GB or more)\\ncan be beneficial. If your model is exceptionally large or if you are training with very large datasets,\\ndistributed training across multiple GPUs or TPUs might be necessary. This requires a careful setup of\\ndata parallelism or model parallelism techniques to efficiently utilise the available hardware [46].\\nLastly, ensure robust cooling and power supply for your hardware, as training LLMs can be resource-\\nintensive, generating significant heat and requiring consistent power. Proper hardware setup not only\\nenhances training performance but also prolongs the lifespan of your equipment [47].\\n5.3\\nDefining Hyperparameters\\nKey hyperparameters like learning rate, batch size, epochs are crucial for enhancing the model’s perfor-\\nmance and obtaining superior outcomes. This process entails adjusting hyperparameters and training\\nsettings to align with your particular use case. Below are the key hyperparameters:\\n1. Learning Rate: Fine-tuning an LLM involves using optimisation algorithms like stochastic gradi-\\nent descent (SGD). This technique estimates the error gradient for the model’s current state using\\nsamples from the training dataset and subsequently updates the model’s weights via the backprop-\\nagation of errors algorithm. The learning rate dictates the speed at which the model adapts to the\\nproblem. Smaller learning rates necessitate more training due to the minimal weight adjustments\\nper update, while larger learning rates lead to quicker changes to weights [48].\\n2. Batch Size: A batch refers to a subset of the training data used to update a model’s weights\\nduring the training process. Batch training involves dividing the entire training set into smaller\\ngroups, updating the model after processing each batch. The batch size is a hyperparameter that\\ndetermines the number of samples processed before the model parameters are updated.\\n3. Epochs: Epoch refers to a full pass through the entire training dataset. This involves a complete\\nforward and backward pass through the dataset. The dataset can be processed as a single batch\\nor divided into multiple smaller batches. An epoch is considered complete once the model has\\nprocessed all batches and updated its parameters based on the calculated loss.\\n5.3.1\\nMethods for Hyperparameter Tuning\\nLLM hyperparameter tuning involves adjusting various hyperparameters during the training process\\nto identify the optimal combination that yields the best output. This process often entails significant\\ntrial and error, meticulously tracking each hyperparameter adjustment, and recording the resulting\\nperformance. Conducting this manually can be highly time-consuming. To address this, automated\\nhyperparameter tuning methods have been developed to streamline the process. The three most common\\nmethods of automated hyperparameter tuning are random search, grid search, and Bayesian optimisation:\\n1. Random Search: This method randomly selects and evaluates combinations of hyperparameters\\nfrom a specified range. It is a straightforward and efficient approach capable of exploring a large\\nparameter space. However, it may not always find the optimal combination of hyperparameters\\nand can be computationally expensive [49].\\n2. Grid Search: Unlike random search, grid search exhaustively evaluates every possible combination\\nof hyperparameters from a given range.\\nAlthough resource-intensive, this systematic approach\\nensures that the optimal set of hyperparameters is found [50].\\n27\\n3. Bayesian Optimisation: This method uses a probabilistic model to predict the performance of\\ndifferent hyperparameters and selects the best ones accordingly. It is an efficient method that can\\nhandle large parameter spaces better and is less resource-intensive than grid search. However, it is\\nmore complex to set up and may be less reliable in identifying the optimal set of hyperparameters\\ncompared to grid search.\\n4. Automated hyperparameter tuning: This facilitates the development of multiple language\\nmodels, each with a unique combination of hyperparameters. By training these models on the same\\ndataset, it becomes possible to compare their outputs and determine which configuration is best\\nsuited for the desired use case. Additionally, models tuned with different sets of hyperparameters\\ncan be tailored to various specific applications.\\n5.4\\nInitialising Optimisers and Loss Functions\\nChoosing the right optimiser and loss function is crucial for training and fine-tuning LLMs.\\nBelow\\nare descriptions of some commonly used optimisation algorithms, their advantages, disadvantages, and\\nappropriate use cases:\\n5.4.1\\nGradient Descent\\nGradient Descent is a fundamental optimisation algorithm used to minimise cost functions in machine\\nlearning models. It aims to find the optimal parameters for a neural network.\\nHow it Works: Gradient Descent iteratively updates model parameters in the direction of the\\nnegative gradient of the cost function. It calculates gradients for each parameter and applies updates\\nacross all data points until convergence. This method utilises the entire dataset to calculate gradients,\\noften requiring a fixed learning rate and being sensitive to the scale of data and learning rate choice.\\nPros:\\n• Simple and easy to implement.\\n• Intuitive and easy to understand.\\n• Converges to the global minimum for convex functions.\\n• Suitable for small-scale problems.\\nCons:\\n• Computationally expensive on large datasets.\\n• May get stuck in local minima.\\n• Requires a large number of iterations.\\n• Sensitive to the choice of learning rate.\\nWhen to Use: Gradient Descent is best used for small datasets where gradient computation is\\ncheap and simplicity and clarity are preferred.\\n5.4.2\\nStochastic Gradient Descent (SGD)\\nStochastic Gradient Descent (SGD) is a variant of Gradient Descent that focuses on reducing computation\\nper iteration.\\nHow it Works: SGD updates parameters using a single or few data points at each iteration, intro-\\nducing randomness in updates. It reduces the computational burden per iteration and often converges\\nfaster than batch Gradient Descent. However, it requires a smaller learning rate due to higher variance\\nand benefits from momentum to stabilise updates.\\nPros:\\n• Fast and handles large datasets well.\\n• Efficient memory usage.\\n28\\n• Simple and easy to implement.\\n• Can escape local minima due to noise.\\nCons:\\n• High variance in updates can lead to instability.\\n• Can overshoot the minimum.\\n• Sensitive to the choice of learning rate.\\n• Can be slower to converge compared to batch methods.\\nWhen to Use: SGD is ideal for large datasets, incremental learning scenarios, and real-time learning\\nenvironments where computational resources are limited.\\n5.4.3\\nMini-batch Gradient Descent\\nMini-batch Gradient Descent combines the efficiency of SGD and the stability of batch Gradient Descent,\\noffering a compromise between batch and stochastic approaches.\\nHow it Works: It splits data into small batches and updates parameters using gradients averaged\\nover each mini-batch. This reduces variance compared to SGD and is more efficient than batch Gradient\\nDescent, helping in generalising the updates.\\nPros:\\n• Balances between efficiency and stability.\\n• More generalisable updates.\\n• Reduces the variance of parameter updates.\\n• Provides a compromise between SGD and batch.\\nCons:\\n• Requires tuning of batch size.\\n• Can still be computationally expensive for very large datasets.\\n• More complex implementation.\\n• Can require more iterations than full-batch Gradient Descent.\\nWhen to Use: Mini-batch Gradient Descent is suitable for most deep learning tasks, especially\\nwhen working with moderate to large datasets.\\n5.4.4\\nAdaGrad\\nAdaptive Gradient Algorithm (AdaGrad) is designed for sparse data and high-dimensional models, ad-\\njusting learning rates to improve performance on sparse data.\\nHow it Works: AdaGrad adapts the learning rate for each parameter based on historical gradi-\\nent information, accumulating squared gradients. This approach prevents large updates for frequent\\nparameters and helps in dealing with sparse features.\\nPros:\\n• Adapts learning rate for each parameter.\\n• Good for sparse data.\\n• No need to manually tune learning rates.\\n• Works well with high-dimensional data.\\nCons:\\n• Learning rate can diminish to zero, stopping learning.\\n29\\n• May require more tuning for convergence.\\n• Accumulation of squared gradients can lead to overly small learning rates.\\n• Can slow down significantly.\\nWhen to Use: AdaGrad is useful for sparse datasets like text and images where learning rates need\\nto adapt to feature frequency.\\n5.4.5\\nRMSprop\\nRoot Mean Square Propagation (RMSprop) is an adaptive learning rate method designed to perform\\nbetter on non-stationary and online problems.\\nHow it Works: RMSprop modifies AdaGrad by using a moving average of squared gradients to\\nadapt learning rates based on recent gradient magnitudes. It maintains a running average of squared\\ngradients to help in maintaining steady learning rates.\\nPros:\\n• Addresses the diminishing learning rate problem of AdaGrad.\\n• Adapts learning rate based on recent gradients.\\n• Effective for recurrent neural networks.\\n• More robust against non-stationary targets.\\nCons:\\n• Can still get stuck in local minima on non-convex problems.\\n• Requires hyperparameter tuning.\\n• Requires careful tuning of the decay rate.\\n• Can be sensitive to the initial learning rate.\\nWhen to Use: RMSprop is best for non-convex optimisation problems, training RNNs and LSTMs,\\nand dealing with noisy or non-stationary objectives.\\n5.4.6\\nAdaDelta\\nAdaptive Delta (AdaDelta) improves on AdaGrad and RMSprop, focusing on adaptive learning rates\\nwithout diminishing too quickly.\\nHow it Works: AdaDelta eliminates the need for a default learning rate by using a moving window\\nof gradient updates. It adapts learning rates based on recent gradient magnitudes to ensure consistent\\nupdates even with sparse gradients.\\nPros:\\n• Eliminates the need to set a default learning rate.\\n• Addresses the diminishing learning rate issue.\\n• Does not require manual tuning of the learning rate.\\n• Handles gradient sparsity well.\\nCons:\\n• More complex than RMSprop and AdaGrad.\\n• Can have slower convergence initially.\\n• Can require more iterations to converge.\\n• Implementation can be more complex.\\nWhen to Use: AdaDelta is suitable for scenarios similar to RMSprop but is preferred when avoiding\\nmanual learning rate setting.\\n30\\n5.4.7\\nAdam\\nAdaptive Moment Estimation (Adam) combines the advantages of AdaGrad and RMSprop, making it\\nsuitable for problems with large datasets and high-dimensional spaces.\\nHow it Works: Adam uses running averages of both gradients and their squared values to com-\\npute adaptive learning rates for each parameter. It includes bias correction and often achieves faster\\nconvergence than other methods.\\nPros:\\n• Combines advantages of AdaGrad and RMSprop.\\n• Adaptive learning rates.\\n• Includes bias correction.\\n• Fast convergence.\\n• Works well with large datasets and high-dimensional spaces.\\nCons:\\n• Requires tuning of hyperparameters (though it often works well with defaults).\\n• Computationally intensive.\\n• Can lead to overfitting if not regularised properly.\\n• Requires more memory.\\nWhen to Use: Adam is widely used in most deep learning applications due to its efficiency and\\neffectiveness, particularly in complex neural network architectures.\\n5.4.8\\nAdamW\\nAdamW is an extension of Adam that includes weight decay regularisation to address overfitting issues\\npresent in Adam.\\nHow it Works: AdamW integrates L2 regularisation directly into the parameter updates, decoupling\\nweight decay from the learning rate. This improves generalisation and is suitable for fine-tuning large\\nmodels.\\nPros:\\n• Includes weight decay for better regularisation.\\n• Combines Adam’s adaptive learning rate with L2 regularisation.\\n• Improves generalisation.\\n• Reduces overfitting compared to Adam.\\nCons:\\n• Slightly more complex than Adam.\\n• Requires careful tuning of the weight decay parameter.\\n• Slightly slower than Adam due to additional computations.\\n• Requires more memory.\\nWhen to Use: AdamW is ideal for scenarios where regularisation is needed, such as preventing\\noverfitting in large models and fine-tuning pre-trained models.\\nA comprehensive collection of optimisation algorithms implemented within the PyTorch library can be\\nfound in here. The Hugging Face Transformers package also offers a variety of optimisers for initialising\\nand fine-tuning language models, available here.\\n31\\n5.5\\nChallenges in Training Setup\\n1. Ensuring compatibility and proper configuration of high-performance hardware like GPUs or TPUs\\ncan be complex and time-consuming.\\n2. Managing dependencies and versions of deep learning frameworks and libraries to avoid conflicts\\nand leverage the latest features.\\n3. Selecting an appropriate learning rate is critical, as too high a rate can cause suboptimal conver-\\ngence, while too low a rate can make the training process excessively slow.\\n4. Determining the optimal batch size that balances memory constraints and training efficiency, es-\\npecially given the large memory requirements of LLMs.\\n5. Choosing the right number of epochs to avoid underfitting or overfitting the model, requiring careful\\nmonitoring and validation.\\n6. Selecting the most suitable optimiser for the specific training task to efficiently update the model’s\\nweights.\\n7. Choosing the correct loss function to accurately measure model performance and guide the opti-\\nmisation process.\\n5.6\\nBest Practices\\n• Optimal Learning Rate: Use a lower learning rate, typically between 1e-4 to 2e-4, to ensure\\nstable convergence. A learning rate schedule, such as learning rate warm-up followed by a linear\\ndecay, can also be beneficial. This helps in initially stabilising the training and then allowing the\\nmodel to converge more accurately.\\n• Batch Size Considerations: Opt for a batch size that balances memory constraints and training\\nefficiency.\\nSmaller batch sizes can help in achieving faster convergence but may require more\\nfrequent updates. Conversely, larger batch sizes can be more memory-intensive but may lead to\\nmore stable updates. Experiment with different batch sizes to find the optimal balance for your\\nspecific use case.\\n• Save Checkpoints Regularly: Regularly save model weights at various intervals across 5-8\\nepochs to capture optimal performance without overfitting. Implement early stopping mechanisms\\nto halt training once the model performance starts to degrade on the validation set, thereby pre-\\nventing overfitting [51].\\n• Hyperparameter Tuning: Utilise hyperparameter tuning methods like grid search, random\\nsearch, and Bayesian optimisation to find the optimal set of hyperparameters.\\nTools such as\\nOptuna, Hyperopt, and Ray Tune can automate this process and help in efficiently exploring the\\nhyperparameter space [49].\\n• Data Parallelism and Model Parallelism: For large-scale training, consider using data paral-\\nlelism or model parallelism techniques to distribute the training workload across multiple GPUs or\\nTPUs. Libraries like Horovod and DeepSpeed can facilitate efficient distributed training, helping\\nto reduce training time and manage memory usage effectively [52, 53].\\n• Regular Monitoring and Logging: Implement robust monitoring and logging to track training\\nmetrics, resource usage, and potential bottlenecks. Tools like TensorBoard, Weights & Biases, and\\nMLflow can provide real-time insights into the training process, allowing for timely interventions\\nand adjustments.\\n• Handling Overfitting and Underfitting: Ensure that your model generalises well by imple-\\nmenting techniques to handle overfitting and underfitting. regularisation techniques such as L2\\nregularisation, dropout, and data augmentation can help prevent overfitting. Conversely, if your\\nmodel is underfitting, consider increasing the model complexity or training for more epochs.\\n32\\n• Use Mixed Precision Training: Mixed precision training involves using both 16-bit and 32-bit\\nfloating-point types to reduce memory usage and increase computational efficiency. This technique\\ncan significantly speed up training and reduce the required memory footprint, especially when\\nusing large models. NVIDIA’s Apex and TensorFlow’s mixed precision API provide support for\\nimplementing mixed precision training [54].\\n• Evaluate and Iterate: Continuously evaluate the model performance using a separate validation\\nset and iterate on the training process based on the results. Regularly update your training data\\nand retrain the model to keep it current with new data trends and patterns.\\n• Documentation and Reproducibility: Maintain thorough documentation of your training\\nsetup, including the hardware configuration, software environment, and hyperparameters used.\\nEnsure reproducibility by setting random seeds and providing detailed records of the training\\nprocess. This practice not only aids in debugging and further development but also facilitates\\ncollaboration and sharing of results with the broader research community.\\n33\\nChapter 6\\nStage 4: Selection of Fine-Tuning\\nTechniques and Appropriate Model\\nConfigurations\\nThis chapter focuses on selecting appropriate fine-tuning techniques and model configurations that suit\\nthe specific requirements of various tasks. Fine-tuning is a crucial stage where pre-trained models are\\nadapted to specific tasks or domains.\\n6.1\\nSteps Involved in Fine-Tuning\\nThe following steps outline the fine-tuning process, integrating advanced techniques and best practices.\\n1. Initialise the Pre-Trained Tokenizer and Model: Begin by loading the pre-trained tokenizer\\nand model. The tokenizer ensures that the input text is converted into a format the model can\\nprocess, while the pre-trained model serves as the foundation for further adaptation. Depending\\non the task, select a model that has been pre-trained on relevant data to provide a strong starting\\npoint.\\n2. Modify the Model’s Output Layer: Adjust the model’s output layer to align with the specific\\nrequirements of the target task. This may involve modifying existing layers or adding new layers.\\nFor instance, tasks like classification may require a softmax layer with the appropriate number of\\nclasses, while text generation tasks might involve changes in the decoding mechanism.\\n3. Choose an Appropriate Fine-Tuning Strategy: Select the fine-tuning strategy that best fits\\nthe task and the model architecture. Some Options include:\\n• Task-Specific Fine-Tuning: For tasks such as text summarisation, code generation, classi-\\nfication, and question answering, adapt the model using relevant datasets.\\n• Domain-Specific Fine-Tuning: Tailor the model to comprehend and generate text relevant\\nto specific domains, such as medical, financial, or legal fields.\\n• Parameter-Efficient Fine-Tuning (PEFT): Techniques like LoRA, QLoRA, and adapters\\nallow for fine-tuning with reduced computational costs by updating a small subset of model\\nparameters.\\n• Half Fine-Tuning (HFT): Balance between retaining pre-trained knowledge and learning\\nnew tasks by updating only half of the model’s parameters during each fine-tuning round.\\n4. Set Up the Training Loop: Establish the training loop, incorporating the selected fine-tuning\\nstrategy. The loop should include data loading, loss computation, backpropagation, and parameter\\nupdates.\\nWhen using PEFT methods, ensure that only the relevant parameters are updated\\nto maximise efficiency. Implement techniques like dynamic learning rates and early stopping to\\nenhance the training process.\\n34\\n5. Incorporate Techniques for Handling Multiple Tasks: If fine-tuning for multiple tasks,\\nconsider strategies like fine-tuning with multiple adapters or leveraging Mixture of Experts (MoE)\\narchitectures. These methods allow a single model to handle various tasks by utilising specialised\\nsub-networks or adapters for each task.\\n6. Monitor Performance on a Validation Set: Regularly evaluate the model’s performance on\\na validation set to ensure it generalises well to unseen data.\\nAdjust hyperparameters such as\\nlearning rate, batch size, and dropout rates based on the validation performance. Utilise advanced\\nmonitoring tools to track metrics like accuracy, loss, and overfitting.\\n7. Optimise Model Using Advanced Techniques: Employ techniques such as Proximal Policy\\nOptimisation (PPO) for reinforcement learning scenarios, or Direct Preference Optimisation (DPO)\\nfor aligning model outputs with human preferences. These techniques are particularly useful in\\nfine-tuning models for tasks requiring nuanced decision-making or human-like responses.\\n8. Prune and optimise the Model (if necessary): To deploy the model in resource-constrained\\nenvironments, consider pruning techniques to reduce its size and complexity. This involves removing\\nunnecessary parameters or components without significantly affecting performance. Utilise dynamic\\npruning methods during inference to optimise the model on-the-fly for different scenarios.\\n9. Continuous Evaluation and Iteration: Continuously evaluate the model’s performance across\\nvarious tasks using appropriate benchmarks. Iterate on the fine-tuning process, making adjustments\\nbased on performance metrics and real-world testing. This iterative approach helps in refining the\\nmodel to meet specific performance criteria.\\n6.2\\nFine-Tuning Strategies for LLMs\\n6.2.1\\nTask-Specific Fine-Tuning\\nTask-specific fine-tuning adapts large language models (LLMs) for particular downstream tasks using\\nappropriately formatted and cleaned data. Below is a summary of key tasks suitable for fine-tuning\\nLLMs, including examples of LLMs tailored to these tasks.\\nTask\\nDescription\\nKey Models\\nText Summarisation\\nCondensing long texts into coherent sum-\\nmaries while retaining key information. Ap-\\nproaches include Extractive (selecting key\\nsentences) and Abstractive summarisation\\n(generating new sentences).\\nBERTSUM, GPT-3, T5\\nCode Generation\\nAutomatically generating programming code\\nbased on natural language descriptions, par-\\ntial code snippets, or structured data inputs.\\nCodex, GPT-3, CodeBERT\\nClassification\\nCategorising text into predefined labels such\\nas Sentiment Analysis, Topic Classification,\\nand Entity Classification.\\nBERT, RoBERTa, GPT-4\\nQ&A\\nUnderstanding and generating accurate, con-\\ntextually relevant answers to natural lan-\\nguage questions.\\nBERT, GPT-3, T5\\nTable 6.1: Overview of tasks such as text summarisation, code generation, classification, and Q&A, along\\nwith their key LLMs and descriptions.\\n6.2.2\\nDomain-Specific Fine-Tuning\\nDomain-specific fine-tuning focuses on tailoring the model to comprehend and produce text relevant to\\na specific domain or industry. By fine-tuning the model on a dataset derived from the target domain,\\nit enhances the model’s contextual understanding and expertise in domain-specific tasks. Below are\\nexamples of domain-specific LLMs.\\n35\\nMedical Domain\\nModel Description: Med-PaLM 2 is trained on meticulously curated medical datasets and is capable\\nof accurately answering medical questions, achieving performance comparable to that of medical profes-\\nsionals [55].\\nBase Model: PaLM 2\\nFine-tuned Model Parameters: Not Known\\nFine-Tuning Techniques Used: Instruction fine-tuning\\nDatasets Used:\\n• MedQA\\n• MedMCQA\\n• LiveQA\\n• MedicationQA\\n• HealthSearchQA\\nResults: Med-PaLM 2 outperformed GPT-4 in several key medical benchmarks, demonstrating superior\\nperformance in handling complex medical knowledge and reasoning tasks.\\nFinance Domain\\nModel Description: FinGPT, an open-source LLM tailored for the financial sector, enhances financial\\nresearch and cooperation by promoting data accessibility and handling finance-specific issues like data\\nacquisition and quality [56].\\nBase Model: LlaMA, ChatGLM, and other Transformer Models\\nFine-tuned Model Parameters: Not Known\\nFine-Tuning Techniques Used: LoRA, Reinforcement Learning on Stock Prices (RLSP)\\nDatasets Used:\\n• Financial News (Reuters, CNBC, Yahoo Finance)\\n• Social Media (Twitter, Facebook, Reddit, Weibo)\\n• Regulatory Filings (e.g., SEC filings)\\n• Trends (Seeking Alpha, Google Trends)\\n• Academic Datasets\\nResults: Not Applicable\\nLegal Domain\\nModel Description: LAWGPT, the first open-source model specifically designed for Chinese legal\\napplications, demonstrates superior capability in handling Chinese legal tasks [57].\\nBase Model: Chinese Alpaca Plus 7B base model\\nFine-tuned Model Parameters: Not Known\\nFine-Tuning Techniques Used: LoRA with Alpaca template\\nDatasets Used:\\n• Open-source dataset: 200,000 examples containing crime type prediction and crime consultation\\ntasks.\\n• JEC-QA dataset: 20,000 examples containing legal question answering tasks.\\n• Constructed legal dataset: 80,000 examples, refined from open-source and JEC-QA datasets using\\nChatGPT.\\nResults: LAWGPT demonstrates notable performance improvements over the LLaMA 7B model in\\nvarious legal tasks, but still trails behind proprietary models like GPT-3.5 Turbo and GPT-4.\\n36\\nPharmaceutical Domain\\nModel Description: PharmaGPT, a suite of domain-specific large language models tailored to the\\nbiopharmaceutical and chemical industries, sets a new benchmark for precision in these fields [58].\\nBase Model: LlaMA series\\nFine-tuned Model Parameters: 13B and 70B\\nFine-Tuning Techniques Used: Instruction fine-tuning and RLHF\\nDatasets Used:\\n• Specific-domain data from academic papers and clinical reports\\n• Text data from NLP dataset formats (e.g., question answering, summarisation, dialogue)\\n• Instruction fine-tuning dataset for multitask learning\\n• RLHF dataset with human preference expert-annotated instructions\\nResults: PharmaGPT models demonstrated impressive performance on various pharmaceutical bench-\\nmarks, consistently outperforming GPT-3.5 Turbo.\\nFinance Domain\\nModel Description: Palmyra-Fin-70B-32K, developed by Writer, is a leading large language model\\nspecifically designed for the financial sector. [59]\\nBase Model: LlaMA\\nFine-tuned Model Parameters: 70B\\nFine-Tuning Techniques Used: Not Known\\nDatasets Used: Not Known\\nResults: Palmyra-Fin-70B-32K exhibits state-of-the-art performance, achieving leading results across\\nvarious financial datasets and excelling in financial document analysis, market trend prediction, and risk\\nassessment.\\n6.3\\nParameter-Efficient Fine-Tuning (PEFT) Techniques\\nParameter Efficient Fine Tuning (PEFT) is an impactful NLP technique that adeptly adapts pre-trained\\nlanguage models to various applications with remarkable efficiency. PEFT methods fine-tune only a\\nsmall subset of (additional) model parameters while keeping most of the pre-trained LLM parameters\\nfrozen, thereby significantly reducing computational and storage costs. This approach mitigates the issue\\nof catastrophic forgetting, a phenomenon where neural networks lose previously acquired knowledge and\\nexperience a significant performance decline on previously learned tasks when trained on new datasets.\\nPEFT methods have demonstrated superior performance compared to full fine-tuning, particularly in\\nlow-data scenarios, and exhibit better generalisation to out-of-domain contexts. This technique is appli-\\ncable to various modalities, such as financial sentiment classification and machine translation of medical\\nterminologies. A taxonomy of PEFT-based fine-tuning approaches is provided in Figure6.1. We will\\nfurther discuss a few key PEFT-based approaches in the following sections.\\n6.3.1\\nAdapters\\nAdapter-based methods introduce additional trainable parameters after the attention and fully connected\\nlayers of a frozen pre-trained model, aiming to reduce memory usage and accelerate training. The specific\\napproach varies depending on the adapter; it might involve adding an extra layer or representing the\\nweight updates delta (W) as a low-rank decomposition of the weight matrix. Regardless of the method,\\nadapters are generally small yet achieve performance comparable to fully fine-tuned models, allowing for\\nthe training of larger models with fewer resources.\\nHuggingFace supports adapter configurations through the PEFT library. During fine-tuning, new adapters\\nare integrated into the model using LoraConfig 1. HuggingFace uses PeftConfig to load existing pre-\\ntrained models and apply PEFT techniques. Additionally, HuggingFace provides built-in support to\\n1https://huggingface.co/docs/peft/en/package_reference/lora\\n37\\nFigure 6.1: Comprehensive Taxonomy of Parameter-Efficient Fine-Tuning (PEFT) Methods for Large\\nLanguage Models (LLMs). This figure categorises various PEFT techniques, highlighting their distinct\\napproaches, from additive and selective fine-tuning to reparameterised and hybrid methods. It details\\nspecific strategies within each category, such as Adapter-Based Fine-Tuning, Soft Prompt-Based Fine-\\nTuning, and their respective sub-techniques like LoRA and its derivatives, showcasing the diverse and\\nevolving landscape of LLM fine-tuning. (adapted from [60])\\nrun the fine-tuning process across any distributed configuration using Accelerate2, making large-scale\\ntraining and inference simple, efficient, and adaptable.\\n6.3.2\\nLow-Rank Adaptation (LoRA)\\nLow-Rank Adaptation (LoRA)[62] is a technique designed for fine-tuning large language models, which\\nmodifies the fine-tuning process by freezing the original model weights and applying changes to a separate\\nset of weights, added to the original parameters. LoRA transforms the model parameters into a lower-\\nrank dimension, reducing the number of trainable parameters, speeding up the process, and lowering\\ncosts. This method is particularly useful in scenarios where multiple clients require fine-tuned models\\nfor different applications, allowing for the creation of specific weights for each use case without the\\nneed for separate models. By employing low-rank approximation methods, LoRA effectively reduces\\ncomputational and resource requirements while preserving the pre-trained model’s adaptability to specific\\ntasks or domains.\\nBenefits of Using LoRA\\n1. Parameter Efficiency: LoRA significantly reduces the number of parameters that need to be\\ntrained by focusing only on the low-rank matrices, resulting in lower memory and storage require-\\nments compared to full fine-tuning.\\n2. Efficient Storage: The storage of the trained model is more efficient as it only requires storing\\nthe low-rank matrices instead of the full model weights.\\n2https://huggingface.co/docs/accelerate/en/index\\n38\\nFigure 6.2: Schematic representation of the Adapter Architecture used in LLMs. The diagram showcases\\nthe integration of adapters within the Transformer architecture, including the feed-forward up and down\\nlayers and their role in enabling efficient model adaptation by inserting additional parameters while\\nmaintaining the model’s core structure (adapted from [61])\\n3. Reduced Computational Load: Training with low-rank matrices requires fewer computational\\nresources, making it faster and more scalable.\\n4. Lower Memory Footprint: Since fewer parameters are being updated, the memory footprint\\nduring training is reduced, enabling the use of larger batch sizes or more complex models within\\nthe same hardware constraints.\\n5. Flexibility: LoRA can be easily integrated with existing pre-trained models without extensive\\nmodifications to the model architecture.\\n6. Compatibility: It can be used alongside other fine-tuning techniques, such as adapter layers or\\nprompt-tuning, to further enhance performance.\\n7. Comparable Results: Despite the reduction in the number of trainable parameters, LoRA has\\nbeen shown to achieve performance comparable to full fine-tuning in many tasks.\\n8. Task-Specific Adaptation: It effectively adapts the pre-trained model to specific tasks, leverag-\\ning the knowledge already embedded in the original model.\\n9. Avoiding Overfitting: By focusing on low-rank updates, LoRA can help in mitigating overfitting,\\nespecially when dealing with smaller task-specific datasets.\\nLimitations\\nWhile LoRA demonstrates considerable power, it also presents challenges:\\n• Fine-tuning Scope: LoRA may face difficulties when applied to tasks demanding substantial\\nalterations to the pre-trained model’s internal representations.\\n• Hyperparameter Optimisation: Tuning the rank parameter ‘r’ requires meticulous adjustment\\nfor optimal performance.\\n• Ongoing Research: Despite its promise, LoRA is still in active research stages, and its long-term\\nimplications remain to be fully explored.\\n39\\nFigure 6.3: A comparison between weight updates in regular fine-tuning and LoRA fine-tuning.\\nIn\\nregular fine-tuning, the entire weight update matrix (∆W) is applied to the pre-trained weights. In\\ncontrast, LoRA fine-tuning introduces two low-rank matrices (A and B) that approximate the weight\\nupdate matrix (∆W), significantly reducing the number of trainable parameters by leveraging the inner\\ndimension (r), which is a hyperparameter.\\nThis method is more efficient in terms of memory and\\ncomputation, making it ideal for fine-tuning large models. (adapted from [63])\\nDespite these challenges, LoRA stands as a pioneering technique with vast potential to democratise access\\nto the capabilities of LLMs. Continued research and development offer the prospect of overcoming current\\nlimitations and unlocking even greater efficiency and adaptability.\\nTutorial for Fine-Tuning LLM Using LoRA\\nAn open-source template for fine-tuning LLMs using the LoRA method with the Hugging Face library\\ncan be found here. This template is designed specifically for adapting LLMs for instruction fine-tuning\\nprocesses.\\n6.3.3\\nQLoRA\\nQLoRA[64] is an extended version of LoRA designed for greater memory efficiency in large language mod-\\nels (LLMs) by quantising weight parameters to 4-bit precision. Typically, LLM parameters are stored\\nin a 32-bit format, but QLoRA compresses them to 4-bit, significantly reducing the memory footprint.\\nThis allows fine-tuning on less powerful hardware, including consumer GPUs. QLoRA also quantises the\\nweights of the LoRA adapters from 8-bit to 4-bit, further decreasing memory and storage requirements\\n(see Figure 6.4). Despite the reduction in bit precision, QLoRA maintains performance levels comparable\\nto traditional 16-bit fine-tuning.\\nIt achieves this by backpropagating gradients through a frozen, 4-bit quantised pre-trained language\\nmodel into Low-Rank Adapters, making the fine-tuning process efficient while preserving model effective-\\nness. The QLoRA configuration is supported by HuggingFace via the PEFT library, utilising LoraConfig\\nand BitsAndBytesConfig for quantisation. Innovations such as an optimal 4-bit data type, double quan-\\ntisation of constants, and memory spike management enable QLoRA to reduce memory usage from 96\\nbits per parameter in traditional fine-tuning to 5.2 bits per parameter, an 18-fold reduction.\\nPerformance-wise, QLoRA outperforms naive 4-bit quantisation and matches 16-bit quantised models\\non benchmarks. Additionally, QLoRA enabled the fine-tuning of a high-quality 4-bit chatbot using a\\nsingle GPU in 24 hours, achieving quality comparable to ChatGPT.\\nThis tutorial explains the end-to-end steps of fine-tuning QLoRA on a custom dataset for the Phi-2\\nmodel.\\n40\\nFigure 6.4: Quantised Low-Rank Adaptation (QLoRA) Optimisation Workflow. This figure illustrates\\nthe QLoRA optimisation process, showing how the optimisation states, adapters, and the model interact\\nduring fine-tuning. It demonstrates the use of different bit-widths (32-bit, 16-bit, and 4-bit) to optimise\\nthe memory and computational efficiency during the fine-tuning of large language models (adapted from\\n[65]).\\n6.3.4\\nWeight-Decomposed Low-Rank Adaptation (DoRA)\\nIn the context of optimising model fine-tuning, the pattern analysis of LoRA and Full Fine-Tuning\\n(FT) reveals significant differences in learning behaviours and updates. LoRA, employing a strategy of\\nincrementally updating pre-trained weights using the product of two low-rank matrices, maintains the\\noriginal weights largely static during the fine-tuning process, which allows for efficient inference. Despite\\nits computational efficiency, previous studies have suggested that LoRA’s limited number of trainable\\nparameters might contribute to its performance discrepancies when compared to FT.\\nWeight-Decomposed Low-Rank Adaptation (DoRA) [66] is a novel fine-tuning methodology designed to\\noptimise pre-trained models by decomposing their weights into magnitude and directional components.\\nThis approach leverages the efficiency of Low-Rank Adaptation (LoRA) for directional updates, facili-\\ntating substantial parameter updates without altering the entire model architecture. DoRA addresses\\nthe computational challenges associated with traditional full fine-tuning (FT) by maintaining model\\nsimplicity and inference efficiency, while simultaneously bridging the performance gap typically observed\\nbetween LoRA and FT. Empirical and theoretical evaluations demonstrate that DoRA not only achieves\\nlearning outcomes comparable to FT across diverse tasks—including natural language processing and\\nvision-language applications—but also consistently surpasses LoRA in performance, providing a robust\\nsolution for enhancing the adaptability and efficiency of large-scale models.\\nPython Library - DoRA is facilitated via the HuggingFace LoraConfig package. To incorporate DoRA\\ninto the fine-tuning process, it is essential to specify the ’use dora = True’ parameter during the Lora\\nconfiguration. Further information on initialisation can be found here.\\nBenefits of DoRA\\n1. Enhanced Learning Capacity: DoRA achieves a learning capacity closely resembling full fine-\\ntuning (FT) by decomposing pre-trained weights into magnitude and directional components, al-\\nlowing for more nuanced updates.\\n2. Efficient Fine-Tuning: By utilising the structural advantages of Low-Rank Adaptation (LoRA)\\nfor directional updates, DoRA enables efficient fine-tuning without altering the entire model archi-\\ntecture.\\n3. No Additional Inference Latency: Despite its improved learning capabilities, DoRA does not\\nintroduce any additional inference latency over LoRA, maintaining model simplicity and efficiency.\\n4. Superior Performance: Experimental results demonstrate that DoRA consistently outperforms\\nLoRA across a wide range of tasks, including natural language processing (NLP), visual instruction\\ntuning, and image/video-text understanding. For example, it shows significant improvements in\\ncommonsense reasoning and visual instruction tuning benchmarks.\\n5. Versatility Across Backbones: DoRA has been validated across various model backbones,\\nincluding large language models (LLM) and vision-language models (LVLM), indicating its broad\\n41\\nFigure 6.5: An overview of DoRA (Decomposed Representations for Adaptation), which is a method for\\nweight decomposed low-rank adaptation. The figure illustrates how pre-trained weights are decomposed\\nand adapted for fine-tuning. In the left section, pre-trained weights are decomposed into a magnitude and\\ndirection. The right section shows how these decomposed weights are merged with trainable parameters\\nduring fine-tuning, resulting in updated weights that combine both frozen (blue) and trainable (green)\\ncomponents. The process emphasises efficient adaptation by focusing on the most significant directions\\nin the parameter space, facilitating effective fine-tuning while maintaining the integrity of the original\\nmodel (adapted from [66]).\\napplicability and robustness in different domains.\\n6. Innovative Analysis: The introduction of a novel weight decomposition analysis helps uncover\\nfundamental differences in the learning patterns of FT and various parameter-efficient fine-tuning\\n(PEFT) methods, contributing to a deeper understanding of model fine-tuning dynamics.\\nComparison between LoRA and DoRA\\nLow-Rank Adaptation (LoRA) and Weight-Decomposed Low-Rank Adaptation (DoRA) are both ad-\\nvanced techniques designed to improve the efficiency and effectiveness of fine-tuning large pre-trained\\nmodels. While they share the common goal of reducing computational overhead, they employ different\\nstrategies to achieve this (see Table6.2).\\n42\\nCriteria\\nLoRA\\n(Low-Rank\\nAdapta-\\ntion)\\nDoRA\\n(Weight-Decomposed\\nLow-Rank Adaptation)\\nObjective\\nProvide an efficient method for\\nfine-tuning pre-trained models by\\nusing low-rank matrix products\\nto update weights incrementally\\nwithout increasing inference la-\\ntency.\\nImproves\\nlearning\\ncapacity\\nby\\nclosely mimicking the learning pat-\\nterns of full fine-tuning, optimis-\\ning magnitude and direction sep-\\narately.\\nApproach\\nImplements a low-rank decompo-\\nsition where the weight update is\\nmodelled as the product of two\\nlow-rank matrices (B and A), keep-\\ning the original weights static.\\nUses weight decomposition anal-\\nysis to reparameterise the weight\\nmatrix into separate magnitude\\nand direction components for dis-\\ntinct updates.\\nModel Architecture\\nKeeps the pre-trained weight ma-\\ntrix (W0) unchanged and applies\\nupdates using low-rank matrices\\n(B and A). Matrix A is initialised\\nwith a uniform Kaiming distribu-\\ntion, while B is set to zero initially.\\nRestructures\\nthe\\nweight\\nmatrix\\ninto\\nmagnitude\\nand\\ndirectional\\ncomponents, ensuring directional\\nvectors are unit vectors for more\\ndetailed adjustments.\\nTable 6.2:\\nA detailed comparison between LoRA (Low-Rank Adaptation) and DoRA (Weight-\\nDecomposed Low-Rank Adaptation), highlighting their objectives, approaches, and the specific architec-\\ntural strategies they employ for fine-tuning large language models.\\nTutorial for Fine-Tuning LLM using DoRA\\nThis tutorial offers an in-depth guide and detailed explanation of the steps involved in implementing\\nDoRA from scratch, as well as insights into the fine-tuning process essential for optimising performance.\\n6.3.5\\nFine-Tuning with Multiple Adapters\\nDuring fine-tuning, we have explored the method of freezing the parameters of the LLM and focusing\\nsolely on fine-tuning a few million trainable parameters using LoRA. For example, fine-tuning an LLM\\nfor translation involves training a translation adapter with relevant data. This approach allows us to\\nfine-tune separate adapters for each specific task we want the LLM to perform. However, a key question\\narises: can we consolidate multiple adapters into a unified multi-task adapter? For instance, if we have\\nseparate adapters for translation and summarisation tasks, can we merge them so that the LLM can\\nproficiently handle both tasks? (Illustrated via Figure6.6).\\nThe PEFT library simplifies the process of merging adapters with its add weighted adapter function 3,\\nwhich offers three distinct methods:\\n1. Concatenation: This straightforward method concatenates the parameters of the adapters. For\\ninstance, if two adapters each have a rank of 16, the resulting adapter will have a rank of 32. This\\nmethod is highly efficient.\\n2. Linear Combination: Although less documented, this method appears to perform a weighted\\nsum of the adapters’ parameters.\\n3. SVD: The default method employs singular value decomposition through torch.linalg.svd. While\\nversatile, it is notably slower than the other methods, particularly for adapters with high ranks\\n(greater than 100), which can take several hours.\\nEach method allows for customising the combination by adjusting weights. For instance, when merging\\ntwo adapters, X and Y, assigning more weight to X ensures that the resulting adapter prioritises behaviour\\nsimilar to X over Y.\\nThis approach is particularly suited for consolidating a single LLM to handle multiple tasks rather than\\ncreating separate models for each task domain. By adopting this method, there is no longer a need to\\n3https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraModel.add_weighted_adapter\\n43\\nindividually fine-tune a model for each task. Instead, a single adapter layer can be fine-tuned for each\\ntask, allowing queries to yield the desired responses efficiently.\\nFigure 6.6: Overview of how multiple adapters can be used with a pre-trained LLM to fine-tune it for\\nvarious specific tasks, such as summarisation, proofreading, sentiment analysis, and more. (adapted from\\n[67])\\nSteps for Fine-Tuning LLM with LoRA for Multiple Tasks and Adapters\\n1. Adapter Creation: Create multiple adapters, each fine-tuned for specific tasks using different\\nprompt formats or task-identifying tags (e.g., [translate fren], [chat]).\\n2. LoRA Integration: Implement LoRA to efficiently integrate these adapters into the pre-trained\\nLLM. Utilise LoRA’s methods such as concatenation, linear combination, or singular value decom-\\nposition (SVD) to combine adapters while minimising computational overhead and maintaining\\nperformance.\\n3. Task-Specific Adaptation: Fine-tune each adapter with task-specific data to enhance perfor-\\nmance for individual tasks. Ensure adapters are trained with data relevant to their respective\\ntasks, optimising their ability to generate accurate responses.\\n4. Behaviour Adjustment: Monitor the behaviour of combined adapters to identify any undesired\\ninherited behaviours from individual adapters (e.g., short response generation from a translation\\n44\\nadapter). Adjust the combination weights or types to modify adapter behaviour as needed, ensuring\\neach adapter performs optimally for its intended task.\\n5. Evaluation and Iteration: Evaluate the performance of the combined model across multiple\\ntasks using validation datasets. Iterate on the fine-tuning process, making adjustments to adapter\\ncombinations and training parameters based on performance metrics and user feedback.\\nTherefore, for optimal performance, it is advisable to combine adapters that have been fine-tuned with\\ndistinctly varied prompt formats. However, even when using adapters with different prompt formats, the\\nresulting adapter may not exhibit desired behaviour. For example, a newly combined adapter designed for\\nchatting may only generate short responses, inheriting this tendency from an adapter that was originally\\ntrained to halt after producing a single sentence. To adjust the behaviour of the combined adapter,\\none can prioritise the influence of a specific adapter during the combination process and/or modify the\\nmethod of combination used.\\nAn illustrative tutorial demonstrating the fine-tuning of large language models (LLMs) using multiple\\nadapter layers for various tasks can be found here.\\n6.4\\nHalf Fine Tuning\\nHalf Fine-Tuning (HFT)[68] is a technique designed to balance the retention of foundational knowledge\\nwith the acquisition of new skills in large language models (LLMs). HFT involves freezing half of the\\nmodel’s parameters during each fine-tuning round while updating the other half, allowing the model to\\nretain pre-trained knowledge and enhance new task performance without altering the model architecture.\\nEach repetitive transformer layer is divided into three blocks: self-attention, feed-forward, and layernorm,\\nwith half of the parameters in each block updated and the other half frozen, varying with each round.\\nThis strategic parameter update helps maintain knowledge parity across training rounds and enhances\\nscalability in successive training sessions.\\nResearch on models like LLAMA 2-7B demonstrated that HFT could significantly restore forgotten basic\\nknowledge while preserving high general ability performance. This method’s robustness and efficiency\\nmake it applicable to various fine-tuning scenarios, including supervised fine-tuning, direct preference\\noptimisation, and continual learning. Additionally, HFT’s ability to maintain the model architecture\\nsimplifies its implementation and ensures compatibility with existing systems, further promoting its\\npractical adoption.\\n6.4.1\\nBenefits of using Half Fine tuning\\n1. Recovery of Pre-Trained Knowledge: By rolling back half of the fine-tuned parameters to their\\npre-trained state, HFT effectively recovers a portion of the original knowledge, thereby mitigating\\ncatastrophic forgetting of previously acquired capabilities.\\n2. Enhanced Performance: Research experiments shows that HFT maintains or even surpasses\\nthe performance of full fine-tuning (FFT) on downstream tasks, demonstrating its effectiveness in\\nbalancing knowledge retention with task-specific learning.\\n3. Robustness: The method is robust to different selection strategies and the number of parameters\\nchosen for updating, ensuring consistent performance across various configurations.\\n4. Simplicity and Scalability: HFT does not alter the model architecture, which simplifies im-\\nplementation and allows for scalable applications, particularly beneficial in successive fine-tuning\\nscenarios.\\n5. Versatility: The technique has proven effective across diverse fine-tuning scenarios, including\\nsupervised fine-tuning, direct preference optimisation, and continual learning.\\n45\\nFigure 6.7: Schematic illustration of the Half Fine-Tuning (HFT) method as applied to LLAMA 2’s\\narchitecture. The diagram shows multiple stages of fine-tuning, where specific model parameters are\\nselectively activated (orange) while others remain frozen (blue). This approach optimises training by\\nreducing computational requirements while still effectively adapting the model to new tasks or data.\\n(adapted from [68])\\n6.4.2\\nComparison between HFT and LoRA\\nCriteria\\nHFT\\nLoRA\\nObjective\\nThe goal is to retain the foun-\\ndational knowledge acquired dur-\\ning pre-training while learning new\\ntask-specific skills, thus balancing\\nbetween maintaining existing ca-\\npabilities and acquiring new ones.\\nLoRA aims to reduce computa-\\ntional and memory requirements\\nduring fine-tuning, making it more\\nefficient and feasible to train large\\nmodels on limited hardware re-\\nsources.\\nApproach\\nHFT involves freezing half of the\\nmodel’s parameters during each\\nfine-tuning round and updating\\nonly the other half.\\nLoRA reduces the number of train-\\nable parameters by\\nintroducing\\nlow-rank decomposition into the\\nweight matrices of the neural net-\\nwork. This involves injecting low-\\nrank matrices into the model’s lay-\\ners during fine-tuning.\\nModel Architecture\\nHFT does not alter the model’s ar-\\nchitecture or introduce new param-\\neters, making it straightforward\\nto apply without additional struc-\\ntural changes.\\nLoRA\\nmodifies\\nthe\\nmodel\\nby\\nadding low-rank matrices, which\\nchanges the training dynamics and\\nrequires additional computations\\nfor the low-rank updates.\\nPerformance\\nResearch has shown that HFT\\ncan restore forgotten basic knowl-\\nedge while maintaining high per-\\nformance in general abilities.\\nLoRA is designed to achieve com-\\npetitive performance with full fine-\\ntuning but with significantly fewer\\ntrainable\\nparameters\\nand\\nlower\\ncomputational costs.\\nTable 6.3: Comparative Analysis of Half Fine-Tuning (HFT) and Low-Rank Adaptation (LoRA).\\n46\\n6.5\\nLamini Memory Tuning\\nLamini [69] was introduced as a specialised approach to fine-tuning Large Language Models (LLMs),\\ntargeting the reduction of hallucinations. This development was motivated by the need to enhance the\\nreliability and precision of LLMs in domains requiring accurate information retrieval. Traditional training\\nmethods typically consist of running stochastic gradient descent on vast datasets, which, despite fitting\\nthe training data well, often produce models that fail to generalise effectively and are prone to such errors.\\nFoundation models often follow a training regimen similar to the Chinchilla recipe, which prescribes\\ntraining for a single epoch on a massive corpus, such as training Llama 2 7B on about one trillion\\ntokens. This approach results in substantial loss and is geared more towards enhancing generalisation\\nand creativity where a degree of randomness in token selection is permissible. However, it falls short for\\ntasks demanding high factual precision. In contrast, Lamini Memory Tuning delves deeper by analysing\\nthe loss of individual facts, significantly improving the accuracy of factual recall.\\nBy augmenting a\\nmodel with additional parameters specifically for memory (e.g., an 8B parameter model with an extra 2B\\nparameters for weights), Lamini enables the model to memorise and accurately recall a significant number\\nof facts, closely aligning performance with LLM scaling laws without compromising on generalisation.\\n6.5.1\\nLamini-1 - A model architecture based on Lamini\\nDeparting from traditional transformer-based designs, the Lamini-1 model architecture (Figure 6.8) em-\\nploys a massive mixture of memory experts (MoME). This system features a pre-trained transformer\\nbackbone augmented by adapters that are dynamically selected from an index using cross-attention\\nmechanisms. These adapters function similarly to experts in MoE architectures, and the network is\\ntrained end-to-end while freezing the backbone. This setup allows for specific facts to be stored exactly\\nin the selected experts.\\nFigure 6.8: Diagram of the Lamini-1 Model Architecture, featuring a Massive Array of Memory Experts\\n(MoME). This architecture integrates a pre-trained transformer backbone with dynamically selected\\nadapters via cross-attention mechanisms. Each adapter, functioning as a memory expert, is capable of\\nstoring specific factual data. (adopted from [69])\\nAt inference time, only the relevant experts are retrieved from the index, enabling the LLM to store a\\nlarge number of facts while maintaining low inference latency. Specialised GPU kernels written in Triton\\nare used to accelerate the lookup of experts, optimising the system for quick access to stored knowledge.\\nSystems Optimisations for Banishing Hallucinations\\nThe MoME architecture is designed to minimise the computational demand required to memorise facts.\\nDuring training, a subset of experts, such as 32 out of a million, is selected for each fact. The weights of\\nthe backbone network and the cross attention used to select the expert are frozen, and gradient descent\\nsteps are taken until the loss is sufficiently reduced to memorise the fact. This approach prevents the\\nsame expert from being selected multiple times for different facts by first training the cross attention\\n47\\nselection mechanism during a generalisation training phase, then freezing its weights.\\nThis method ensures that computation scales with the number of training examples, not the total\\nnumber of parameters, thereby significantly reducing the computation required for memory tuning.\\nThis optimised approach allows Lamini-1 to achieve near-zero loss in memory tuning on real and random\\nanswers efficiently, demonstrating its efficacy in eliminating hallucinations while improving factual recall.\\n6.6\\nMixture of Experts\\nA mixture of experts (MoE) is an architectural design for neural networks that divides the computation\\nof a layer or operation (e.g., linear layers, MLPs, or attention projection) into several specialised subnet-\\nworks, referred to as ”experts”. Each expert independently carries out its computation, and the results\\nare aggregated to produce the final output of the MoE layer. MoE architectures can be categorised as\\neither dense, where every expert is engaged for each input, or sparse, where only a subset of experts is\\nutilised for each input.\\n6.6.1\\nMixtral 8x7B Architecture and Performance\\nMixtral [70] 8x7B employs a Sparse Mixture of Experts (SMoE) architecture (Figure 6.9), mirroring the\\nstructure of Mistral 7B but incorporating eight feedforward blocks (experts) in each layer. For every\\ntoken at each layer, a router network selects two experts to process the current state and combine their\\noutputs. Although each token interacts with only two experts at a time, the selected experts can vary at\\neach timestep. Consequently, each token has access to 47 billion parameters but utilises only 13 billion\\nactive parameters during inference. Mixtral 8x7B not only matches but often surpasses Llama 2 70B\\nand GPT-3.5 across all evaluated benchmarks. Its performance is notably superior to Llama 2 70B in\\nmathematics, code generation, and multilingual tasks.\\nFigure 6.9: Diagram of the Mixtral 8x7B Mixture of Experts (MoE) model architecture. The model is\\ncomposed of a router network that dynamically selects the most relevant experts from a pool of eight\\ntransformer-based experts, each with 7 billion parameters. The experts are organised into transformer\\nblocks, where the router directs data to the appropriate expert based on the input, optimising com-\\nputational efficiency and model performance. This architecture allows for scalability and specialised\\nprocessing within large language models. (adapted from [71])\\n48\\n6.7\\nMixture of Agents\\nDespite the numerous LLMs and their notable accomplishments, they continue to encounter fundamental\\nlimitations regarding model size and training data. Scaling these models further is prohibitively expen-\\nsive, often necessitating extensive retraining on multiple trillion tokens. Simultaneously, different LLMs\\nexhibit distinct strengths and specialise in various aspects of tasks. A recent study has investigated\\nleveraging the collective expertise of multiple LLMs to develop a more capable and robust model, a\\nmethod known as Mixture of Agents (MoA) [72].\\nMoA functions using a layered architecture, where each layer comprises multiple LLM agents (Figure\\n6.10). This structure reveals a phenomenon known as the “collaborativeness of LLMs.” The innova-\\ntive MoA framework utilises the combined capabilities of several LLMs to enhance both reasoning and\\nlanguage generation proficiency. Research indicates that LLMs naturally collaborate, demonstrating im-\\nproved response quality when incorporating outputs from other models, even if those outputs are not\\nideal.\\nFigure 6.10: Illustration for Mixture of Agents (MoA) LLM configuration. The model consists of multiple\\nlayers, each incorporating several agents that process the input independently before concatenating their\\noutputs to form an intermediate result. The process continues across layers, refining the output at each\\nstage to generate the final output based on the given prompt (adapted from [72]).\\n6.7.1\\nMethodology\\nTo enhance collaboration among multiple LLMs, it is essential to understand their individual strengths\\nand classify them accordingly. The classification includes:\\n1. Proposers: These models excel at generating valuable reference responses for other models. While\\nthey may not perform exceptionally on their own, they provide useful context and varied perspec-\\ntives that improve the final output when utilised by an aggregator.\\n49\\n2. Aggregators: These models are adept at merging responses from various models into a single\\nhigh-quality result. An effective aggregator should maintain or even enhance the quality of the\\nfinal response, regardless of the quality of the individual inputs.\\nThe careful selection of LLMs for each MoA layer is crucial Performance metrics, such as average win\\nrates in a given layer, help assess the suitability of models for subsequent layers, ensuring the production\\nof higher-quality outputs. Diversity in model outputs is vital, as varied responses from different models\\ncontribute significantly more than homogeneous outputs from a single model. In MoA, given an input\\nprompt, the output of the ith MoA layer yi is calculated as follows:\\nyi =\\nn\\nM\\nj=1\\n[Ai,j(xi)] + x1, xi+1 = yi\\n(6.1)\\n6.7.2\\nAnalogy with MoE\\nMixture-of-Experts (MoE) is a well-established machine learning technique where multiple expert net-\\nworks, each with specialised skills, collaborate to address complex problems. This approach has demon-\\nstrated significant success across various applications and serves as the inspiration for the Mixture-of-\\nAgents (MoA) method. In a typical MoE design, a stack of layers, known as MoE layers, consists of\\nmultiple expert networks, a gating network, and residual connections to improve gradient flow. The\\noutput for layer yi is calculated as follows:\\nyi =\\nn\\nX\\nj=1\\nGi,j(xi)Ei,j(xi) + xi\\n(6.2)\\nThe MoA framework advances the MoE concept by operating at the model level through prompt-based\\ninteractions rather than altering internal activations or weights. Instead of relying on specialised sub-\\nnetworks within a single model, MoA utilises multiple full-fledged LLMs across different layers. In this\\napproach, the gating and expert networks’ functions are integrated within an LLM, leveraging its ability\\nto interpret prompts and generate coherent outputs without additional coordination mechanisms.\\n6.7.3\\nWhat makes MoA works well?\\n1. MoA’s Superior Performance: MoA significantly outperforms LLM-based rankers, which select\\none answer from the proposals rather than generating new responses. This suggests that MoA’s\\napproach of aggregating all generated responses provides more effective results than simply choosing\\nfrom pre-existing options.\\n2. Effective Incorporation of Proposals: The aggregator in MoA demonstrates a tendency to\\nintegrate the best proposed answers. This is supported by positive correlations between aggregator\\nresponses and various similarity metrics, such as BLEU scores, which measure n-gram overlaps. The\\nuse of alternative similarity measures also shows a consistent positive correlation with preference\\nscores, indicating that the aggregator effectively utilises the proposed responses.\\n3. Influence of Model Diversity and Proposer Count: Increasing the number of proposers\\nimproves output quality, highlighting the benefits of additional auxiliary information. Additionally,\\nusing a diverse set of LLMs as proposers consistently yields better results compared to using a single\\nLLM. This suggests that both the number and diversity of LLM agents in each MoA layer contribute\\nto enhanced performance, with potential for further improvement through scaling.\\n4. Model Specialisation: Analysis of model roles within the MoA ecosystem reveals that GPT-4o,\\nQwen, and LLaMA-3 are effective in both assisting and aggregating tasks. In contrast, WizardLM\\nexcels as a proposer but struggles with aggregating responses from other models.\\n6.8\\nProximal Policy Optimisation (PPO)\\nPPO [73] is a widely recognised reinforcement learning algorithm used for training agents to perform tasks\\nin diverse environments. This algorithm leverages policy gradient methods, where policies—represented\\n50\\nby neural networks—determine the actions taken by the agent based on the current state. PPO ef-\\nfectively handles the dynamic nature of training data generated through continuous agent-environment\\ninteractions, a feature that differentiates it from static datasets used in supervised learning.\\nThe innovation of PPO lies in its ”surrogate” objective function, optimised via stochastic gradient ascent.\\nThis approach allows for multiple updates from the same batch of data, enhancing both training efficiency\\nand stability over traditional policy gradient methods. Developed by OpenAI, PPO was designed to\\nbalance ease of implementation with the robust performance characteristics of more complex algorithms\\nlike Trust Region Policy Optimisation (TRPO), but without the associated computational complexity.\\nPPO operates by maximising expected cumulative rewards through iterative policy adjustments that\\nincrease the likelihood of actions leading to higher rewards. A key feature of PPO is its use of a clipping\\nmechanism in the objective function, which limits the extent of policy updates, thus preventing drastic\\nchanges and maintaining stability during training.\\nFigure 6.11: Schematic of Proximal Policy Optimisation (PPO) applied in the context of Reinforcement\\nLearning from Human Feedback (RLHF) for fine-tuning a Large Language Model (LLM). The process\\ninvolves using a prompt dataset to train the LLM. The PPO algorithm adjusts the LLM’s policy based\\non rewards provided by the reward model, which is fine-tuned through human feedback. (adapted from\\n[73])\\nPython Library - HuggingFace Transformer Reinforcement Learning (TRL4) package supports the\\nPPO Trainer5 for training language models from the preference data.\\nThe PPOTrainer expects to align a generated response with a query given the rewards obtained from the\\nReward model. During each step of the PPO algorithm we sample a batch of prompts from the dataset,\\nwe then use these prompts to generate the a responses from the SFT model. Next, the Reward model\\nis used to compute the rewards for the generated response. Finally, these rewards are used to optimise\\nthe SFT model using the PPO algorithm. Therefore the dataset should contain a text column which we\\ncan rename to query. Each of the other data-points required to optimise the SFT model are obtained\\nduring the training loop.\\n6.8.1\\nBenefits of PPO\\n1. Stability: Proximal Policy Optimisation (PPO) is designed to ensure stable and reliable policy\\nupdates. The clipped surrogate objective function is central to this stability, as it limits policy\\nupdates to prevent large, potentially destabilising changes. This results in smoother and more\\nconsistent learning.\\n2. Ease of Implementation: Compared to advanced algorithms TRPO, PPO is relatively straight-\\nforward to implement. It avoids the need for second-order optimisation techniques, making it more\\n4https://huggingface.co/docs/trl/en/index\\n5https://huggingface.co/docs/trl/main/en/ppo_trainer\\n51\\naccessible to less experienced practitioners.\\n3. Sample Efficiency: PPO achieves data efficiency through its use of the clipped surrogate objec-\\ntive. This mechanism regulates policy updates, ensuring stability while effectively reusing training\\ndata.\\nConsequently, PPO tends to be more sample-efficient than other reinforcement learning\\nalgorithms, performing well with fewer samples, which is advantageous in scenarios where data\\ncollection is costly or time-consuming.\\n6.8.2\\nLimitations of PPO\\n1. Complexity and Computational Cost: Proximal Policy Optimisation (PPO) involves intricate\\npolicy and value networks, necessitating substantial computational resources for training. This\\ncomplexity often results in extended training durations and increased operational expenses.\\n2. Hyperparameter Sensitivity: PPO’s performance is highly dependent on several hyperparame-\\nters, such as the clipping range, learning rate, and discount factor. Achieving optimal performance\\nrequires meticulous tuning of these parameters. Incorrect settings can lead to suboptimal policy\\noutcomes or instability during the learning process.\\n3. Stability and Convergence Issues: Although PPO is designed to enhance stability compared\\nto earlier methods, it can still encounter convergence issues, particularly in highly dynamic or\\ncomplex environments. Maintaining stable policy updates remains a significant challenge.\\n4. Reward Signal Dependence: PPO’s effectiveness is heavily reliant on a well-defined reward\\nsignal to guide the learning process. In scenarios where designing an appropriate reward function\\nis challenging or impractical, PPO may struggle to attain the desired results.\\n6.8.3\\nTutorial for training models using PPO technique\\nThe tutorial for tuning GPT2 to generate positive movie reviews based on the IMDB dataset using PPO\\ntechnique can be found here.\\n6.9\\nDirect Preference Optimisation (DPO)\\nDirect Preference Optimisation (DPO) [74] offers a streamlined approach to aligning language models\\n(LMs) with human preferences, bypassing the complexity of reinforcement learning from human feedback\\n(RLHF). Large-scale unsupervised LMs typically lack precise behavioural control, necessitating meth-\\nods like RLHF that fine-tune models using human feedback. However, RLHF is intricate, involving the\\ncreation of reward models and the fine-tuning of LMs to maximise estimated rewards, which can be\\nunstable and computationally demanding. DPO addresses these challenges by directly optimising LMs\\nwith a simple classification objective that aligns responses with human preferences. This approach elim-\\ninates the need for explicit reward modelling and extensive hyperparameter tuning, enhancing stability\\nand efficiency. DPO optimises the desired behaviours by increasing the relative likelihood of preferred\\nresponses while incorporating dynamic importance weights to prevent model degeneration. Thus, DPO\\nsimplifies the preference learning pipeline, making it an effective method for training LMs to adhere to\\nhuman preferences.\\nPython Library - HuggingFace TRL package supports the DPO Trainer6 for training language models\\nfrom the preference data. The DPO training process requires a dataset formatted in a very specific\\nmanner. If you are utilising the default DPODataCollatorWithPadding data collator, your final dataset\\nobject must include three specific entries, which should be labelled as follows:\\n• Prompt\\n• Chosen\\n• Rejected\\nHuggingFace offers datasets compatible with DPO and can be accessed here.\\n6https://huggingface.co/docs/trl/main/en/dpo_trainer\\n52\\nFigure 6.12: Direct Preference Optimisation (DPO) Process Flow. This figure illustrates the Direct\\nPreference Optimisation (DPO) technique used in fine-tuning large language models. The process begins\\nwith preference data (Yw > Yl), where Yw represents preferred outputs, and Yl represents less preferred\\noutputs. Through a maximum likelihood estimation process, this preference data is used to optimise\\nthe model’s parameters, resulting in the final large language model (LLM). The method is designed to\\nimprove the alignment of model outputs with desired user preferences, enhancing the model’s effectiveness\\nin specific tasks. (adapted from [74])\\n6.9.1\\nBenefits of DPO\\n1. Direct Alignment with Human Preferences: DPO directly optimises models to generate\\nresponses that align with human preferences, thereby producing more favourable outputs.\\n2. Minimised Dependence on Proxy Objectives: In contrast to methods that rely on next-\\nword prediction, DPO leverages explicit human preferences, resulting in responses that are more\\nreflective of human behaviour.\\n3. Enhanced Performance on Subjective Tasks: For tasks requiring subjective judgement, such\\nas dialogue generation or creative writing, DPO excels in aligning the model with human prefer-\\nences.\\n6.9.2\\nBest Practices for DPO\\n1. High-Quality Preference Data: The performance of the model is heavily influenced by the\\nquality of preference data. Ensure the dataset includes clear and consistent human preferences.\\n2. Optimal Beta Value: Experiment with various beta values to manage the influence of the\\nreference model. Higher beta values prioritise the reference model’s preferences more strongly.\\n3. Hyperparameter Tuning: optimise hyperparameters such as learning rate, batch size, and LoRA\\nconfiguration to determine the best settings for your dataset and task.\\n4. Evaluation on Target Tasks: Continuously assess the model’s performance on the target task\\nusing appropriate metrics to monitor progress and ensure the achievement of desired results.\\n5. Ethical Considerations: Pay attention to potential biases in the preference data and take steps\\nto mitigate them, preventing the model from adopting and amplifying these biases.\\n6.9.3\\nTutorial for training models using DPO technique\\nThe tutorial for DPO training, including the full source code of the training scripts for SFT and DPO,\\nis available here.\\n6.9.4\\nIs DPO Superior to PPO for LLM Alignment?\\nThe recent study on DPO superior to PPO for LLM Alignment[75] investigates the efficacy of reward-\\nbased and reward-free methods within RLHF. Reward-based methods, such as those developed by Ope-\\nnAI, utilise a reward model constructed from preference data and apply actor-critic algorithms like\\nProximal Policy Optimisation (PPO) to optimise the reward signal. Conversely, reward-free methods,\\nincluding Direct Preference Optimisation (DPO), RRHF, and PRO, forego an explicit reward function,\\n53\\nwith DPO focusing exclusively on policy optimisation through a logarithmic representation of the reward\\nfunction.\\nOne of the objectives of this study is to determine whether DPO is genuinely superior to PPO in the\\nRLHF domain. The study combines theoretical and empirical analyses to uncover the inherent limita-\\ntions of DPO and identify critical factors that enhance PPO’s practical performance in RLHF.\\nTheoretical findings suggest that DPO may yield biased solutions by exploiting out-of-distribution re-\\nsponses. Empirical results indicate that DPO’s performance is notably affected by shifts in the distri-\\nbution between model outputs and the preference dataset. Furthermore, the study highlights that while\\niterative DPO may offer improvements over static data training, it still fails to enhance performance\\nin challenging tasks such as code generation. Ablation studies on PPO reveal essential components for\\noptimal performance, including advantage normalisation, large batch sizes, and exponential moving av-\\nerage updates for the reference model’s parameters. These findings form the basis of practical tuning\\nguidelines, demonstrating PPO’s robust effectiveness across diverse tasks and its ability to achieve state-\\nof-the-art results in challenging code competition tasks. Specifically, on the CodeContest dataset, the\\nPPO model with 34 billion parameters surpasses AlphaCode-41B, showing a significant improvement in\\nperformance metrics.\\n6.10\\nOdds-Ratio Preference Optimization (ORPO)\\nOdds-Ratio Preference Optimization (ORPO) is a novel approach designed to align the output of lan-\\nguage models with desired responses by introducing a penalisation mechanism for undesirable outputs.\\nUnlike traditional supervised fine-tuning (SFT) approaches, which focus solely on maximising the likeli-\\nhood of correct responses, ORPO adds a specific odds-ratio based loss to penalise unwanted generations.\\nThis technique provides a refined method for improving preference alignment without relying on a ref-\\nerence model, making it efficient for large-scale implementations.\\nGiven an input sequence x, the log-likelihood of generating an output sequence y of length m is\\ncomputed as:\\nlog Pθ(y|x) = 1\\nm\\nm\\nX\\ni=1\\nlog Pθ(yi|x)\\nThe odds of generating the output sequence y given input x is expressed as:\\noddsθ(y|x) =\\nPθ(y|x)\\n1 −Pθ(y|x)\\nORPO introduces an odds-ratio that contrasts the likelihood of generating a preferred (chosen) re-\\nsponse yw with a less preferred (rejected) response yl, defined as:\\nORθ(yw, yl|x) = oddsθ(yw|x)\\noddsθ(yl|x)\\nThe ORPO loss function incorporates two components:\\n• Supervised Fine-tuning Loss (SFT):\\nLSF T = −1\\nM\\nM\\nX\\nk=1\\n|V |\\nX\\ni=1\\nyk\\ni log pk\\ni\\nwhere yk\\ni is a binary indicator for the i-th token in the vocabulary, and pk\\ni is its predicted probability.\\n• Odds-Ratio Loss:\\nLOR = −log σ\\n\\x12\\nlog oddsθ(yw|x)\\noddsθ(yl|x)\\n\\x13\\nwhere σ is the sigmoid function applied to stabilise the log odds ratio.\\n54\\nThus, the total ORPO objective is:\\nLORP O = LSF T + λLOR\\nwhere λ controls the strength of preference alignment.\\nThis loss function effectively guides the\\nmodel towards generating the chosen response while discouraging the rejected one, facilitating efficient\\nalignment without the need for additional reference models [76].\\nAdvantages of ORPO: ORPO’s strength lies in its ability to perform preference alignment in a\\nmonolithic manner, bypassing the need for separate phases of fine-tuning and preference optimisation.\\nThis reduces computational overhead and provides state-of-the-art performance across various models,\\nincluding LLaMA and Mistral, when evaluated on benchmark tasks such as AlpacaEval and MT-Bench\\n[77].\\n6.11\\nPruning LLMs\\nPruning LLMs involves eliminating unnecessary or redundant components from a neural network to\\nreduce its size and complexity, thereby enhancing its efficiency and performance. This process assists AI\\ndevelopers and engineers in addressing the challenges associated with deploying AI models in resource-\\nlimited environments, such as mobile devices, edge computing, or embedded systems. Pruning AI models\\ncan be achieved through various techniques, each suited to the type and structure of the neural network,\\nthe pruning objective, and the pruning criterion. The following are common approaches:\\n1. Weight Pruning: Involves removing weights or connections with minimal magnitude or impact on\\nthe output. This method reduces the number of parameters and operations in the model, although\\nit may not necessarily decrease memory footprint or latency.\\n2. Unit Pruning: Eliminates entire units or neurons with the lowest activation or contribution to\\nthe output. This technique can reduce the model’s memory footprint and latency but may require\\nretraining or fine-tuning to maintain performance.\\n3. Filter Pruning: Involves removing entire filters or channels in convolutional neural networks that\\nhave the least importance or relevance to the output. This strategy also reduces memory footprint\\nand latency, though it may necessitate retraining or fine-tuning to preserve performance [78].\\n6.11.1\\nWhen to Prune AI Models?\\nPruning AI models can be conducted at various stages of the model development and deployment cycle,\\ncontingent on the chosen technique and objective.\\n1. Pre-Training Pruning: Leverages prior knowledge or heuristics to determine the optimal network\\nstructure before training begins. This approach can save time and resources during training but\\nmay necessitate careful design and experimentation to identify the best configuration.\\n2. Post-Training Pruning: Involves using metrics or criteria to assess the importance or impact of\\neach network component after training. This method helps maintain model performance but may\\nrequire additional validation and testing to ensure quality and robustness.\\n3. Dynamic Pruning: Adjusts the network structure during inference or runtime based on feedback\\nor signals. This approach can optimise the model for different scenarios or tasks but may involve\\nhigher computational overhead and complexity to implement and execute.\\n6.11.2\\nBenefits of Pruning\\n1. Reduced Size and Complexity: Pruning decreases the size and complexity of AI models, making\\nthem easier to store, transmit, and update.\\n2. Improved Efficiency and Performance: Pruned models are faster, more energy-efficient, and\\nmore reliable.\\n3. Enhanced generalisation and Accuracy: Pruning can make models more robust, less prone\\nto overfitting, and more adaptable to new data or tasks.\\n55\\n6.11.3\\nChallenges of Pruning\\n1. Balance Between Size Reduction and Performance: Achieving the optimal balance between\\nreducing size and complexity and maintaining performance is challenging; excessive or insufficient\\npruning can degrade model quality and functionality.\\n2. Choosing Appropriate Techniques: Selecting the right pruning technique, criterion, and objec-\\ntive for the specific neural network type and structure is crucial, as different methods can produce\\nvarying effects and outcomes.\\n3. Evaluation and Validation: Pruned models need thorough evaluation and validation to ensure\\npruning has not introduced errors, biases, or vulnerabilities that could impact performance and\\nrobustness.\\n56\\nChapter 7\\nStage 5: Evaluation and Validation\\n7.1\\nSteps Involved in Evaluating and Validating Fine-Tuned\\nModels\\n1. Set Up Evaluation Metrics: Choose appropriate evaluation metrics, such as cross-entropy, to\\nmeasure the difference between the predicted and actual distributions of the data.\\n2. Interpret Training Loss Curve: Monitor and analyse the training loss curve to ensure the\\nmodel is learning effectively, avoiding patterns of underfitting or overfitting.\\n3. Run Validation Loops: After each training epoch, evaluate the model on the validation set to\\ncompute relevant performance metrics and track the model’s generalisation ability.\\n4. Monitor and Interpret Results: Consistently observe the relationship between training and\\nvalidation metrics to ensure stable and effective model performance.\\n5. Hyperparameter Tuning and Adjustments: Adjust key hyperparameters such as learning\\nrate, batch size, and number of training epochs to optimise model performance and prevent over-\\nfitting.\\n7.2\\nSetting Up Evaluation Metrics\\nCross-entropy is a key metric for evaluating LLMs during training or fine-tuning.\\nOriginating from\\ninformation theory, it quantifies the difference between two probability distributions.\\n7.2.1\\nImportance of Cross-Entropy for LLM Training and Evaluation\\nCross-entropy is crucial for training and fine-tuning LLMs. It serves as a loss function, guiding the model\\nto produce high-quality predictions by minimising discrepancies between the predicted and actual data.\\nIn LLMs, each potential word functions as a separate class, and the model’s task is to predict the next\\nword given the context. This task is inherently complex, requiring the model to understand syntax,\\nsemantics, and context deeply.\\n7.2.2\\nBeyond Cross-Entropy: Advanced LLM Evaluation Metrics\\nWhile cross-entropy remains fundamental, evaluating LLMs effectively necessitates additional metrics\\ntailored to various aspects of model performance. Here are some advanced metrics employed in LLM\\nevaluation:\\nPerplexity\\nPerplexity measures how well a probability distribution or model predicts a sample. In the context of\\nLLMs, it evaluates the model’s uncertainty about the next word in a sequence. Lower perplexity indicates\\nbetter performance, as the model is more confident in its predictions.\\n57\\nFactuality\\nFactuality assesses the accuracy of the information produced by the LLM. It is particularly important for\\napplications where misinformation could have serious consequences. Higher factuality scores correlate\\nwith higher output quality.\\nLLM Uncertainty\\nLLM uncertainty is measured using log probability, helping to identify low-quality generations. Lower\\nuncertainty indicates higher output quality. This metric leverages the log probability of each generated\\ntoken, providing insights into the model’s confidence in its responses.\\nPrompt Perplexity\\nThis metric evaluates how well the model understands the input prompt.\\nLower prompt perplexity\\nindicates a clear and comprehensible prompt, which is likely to yield better model performance.\\nContext Relevance\\nIn retrieval-augmented generation (RAG) systems, context relevance measures how pertinent the re-\\ntrieved context is to the user query. Higher context relevance improves the quality of generated responses\\nby ensuring that the model utilises the most relevant information.\\nCompleteness\\nCompleteness assesses whether the model’s response fully addresses the query based on the provided\\ncontext. High completeness ensures that all relevant information is included in the response, enhancing\\nits utility and accuracy.\\nChunk Attribution and Utilisation\\nThese metrics evaluate how effectively the retrieved chunks of information contribute to the final response.\\nHigher chunk attribution and utilisation scores indicate that the model is efficiently using the available\\ncontext to generate accurate and relevant answers.\\nData Error Potential\\nThis metric quantifies the difficulty the model faces in learning from the training data. Higher data\\nquality results in lower error potential, leading to better model performance.\\nSafety Metrics\\nSafety metrics ensure that the LLM’s outputs are appropriate and non-harmful. These are included in\\nthe final sections of the chapter.\\nIntegrating these advanced metrics provides a holistic view of LLM performance, enabling developers to\\nfine-tune and optimise models more effectively. By employing a metrics-first approach, it is possible to\\nensure that LLMs not only produce accurate and high-quality outputs but also do so consistently and\\nreliably across diverse applications1.\\n7.3\\nUnderstanding the Training Loss Curve\\nThe training loss curve plots the loss value against training epochs and is essential for monitoring model\\nperformance.\\n1https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation\\n58\\n7.3.1\\nInterpreting Loss Curves\\nAn ideal training loss curve shows a rapid decrease in loss during initial stages, followed by a gradual\\ndecline and eventual plateau. Specific patterns to look for include:\\n1. Underfitting: High loss value that does not decrease significantly over time, suggesting the model\\ncannot learn the data.\\n2. Overfitting: Decreasing training loss with increasing validation loss, indicating the model mem-\\norises the training data.\\n3. Fluctuations: Significant variations may indicate a high learning rate or noisy gradients.\\nFigure 7.1: Example training loss curve showing the decline in loss over iterations during the fine-tuning\\nof Llama2 13B on a financial Q/A dataset. The curve illustrates the effectiveness of the fine-tuning\\nprocess in reducing the loss and improving model performance.\\n7.3.2\\nAvoiding Overfitting\\nTechniques to prevent overfitting include:\\n1. Regularisation: Adds a penalty term to the loss function to encourage smaller weights.\\n2. Early Stopping: Stops training when validation performance no longer improves.\\n3. Dropout: Randomly deactivates neurons during training to reduce sensitivity to noise.\\n4. Cross-Validation: Splits data into multiple subsets for training and validation to assess model\\ngeneralisation.\\n5. Batch Normalisation: Normalises inputs to each layer during training to stabilise the learning\\nprocess.\\n6. Larger Datasets and Batch Sizes: Reduces overfitting by increasing the amount of diverse\\ndata and batch sizes.\\n59\\n7.3.3\\nSources of Noisy Gradients\\nNoisy gradients are common during the training of machine learning models, including LLMs. They arise\\nfrom variability in gradient estimates due to stochastic gradient descent and its variants. Strategies to\\nmanage noisy gradients include:\\n1. Learning Rate Scheduling: Gradually decreasing the learning rate during training can reduce\\nthe impact of noisy gradients.\\n2. Gradient Clipping: Setting a threshold for gradient values prevents large updates that can\\ndestabilise training.\\n7.4\\nRunning Validation Loops\\nValidation loops provide an unbiased evaluation of model performance. Typical steps include:\\n1. Split Data: Divide the dataset into training and validation sets.\\n2. Initialise Validation: Evaluate the model on the validation set at the end of each epoch.\\n3. Calculate Metrics: Compute relevant performance metrics, such as cross-entropy loss.\\n4. Record Results: Log validation metrics for each epoch.\\n5. Early Stopping: Optionally stop training if validation loss does not improve for a predefined\\nnumber of epochs.\\n7.5\\nMonitoring and Interpreting Results\\nMonitoring validation results involves analysing trends in validation metrics over epochs. Key aspects\\ninclude:\\n1. Consistent Improvement: Indicates good model generalisation if both training and validation\\nmetrics improve and plateau.\\n2. Divergence: Suggests overfitting if training metrics improve while validation metrics deteriorate.\\n3. Stability: Ensure validation metrics do not fluctuate significantly, indicating stable training.\\n7.6\\nHyperparameter Tuning and Other Adjustments\\nFine-tuning involves adjusting key hyperparameters to achieve optimal performance. Important hyper-\\nparameters include:\\n1. Learning Rate: Determines the step size for updating model weights. A good starting point is\\n2e-4, but this can vary.\\n2. Batch Size: Larger batch sizes lead to more stable updates but require more memory.\\n3. Number of Training Epochs: Balancing the number of epochs ensures the model learns suffi-\\nciently without overfitting or underfitting.\\n4. Optimiser: Optimisers like Paged ADAM optimise memory usage, advantageous for large models.\\nOther tunable parameters include dropout rate, weight decay, and warmup steps.\\n7.6.1\\nData Size and Quality\\nThe efficacy of LLMs is directly impacted by the quality of their training data. Ensuring that datasets\\nare clean, relevant, and adequate is crucial. Data cleanliness refers to the absence of noise, errors, and\\ninconsistencies within the labelled data. For example, having a phrase like “This article suggests. . . ”\\nmultiple times in the training data can corrupt the response of LLMs and add a bias towards using this\\nspecific phrase more often and in inappropriate situations.\\n60\\n7.7\\nBenchmarking Fine-Tuned LLMs\\nModern LLMs are assessed using standardised benchmarks such as GLUE, SuperGLUE, HellaSwag,\\nTruthfulQA, and MMLU (See Table 7.1). These benchmarks evaluate various capabilities and provide\\nan overall view of LLM performance.\\nBenchmark\\nDescription\\nReference URL\\nGLUE\\nProvides a standardised set of diverse NLP tasks to\\nevaluate the effectiveness of different language mod-\\nels\\nSource\\nSuperGLUE\\nCompares more challenging and diverse tasks with\\nGLUE, with comprehensive human baselines\\nSource\\nHellaSwag\\nEvaluates how well an LLM can complete a sentence\\nSource\\nTruthfulQA\\nMeasures truthfulness of model responses\\nSource\\nMMLU\\nEvaluates how well the LLM can multitask\\nSource\\nIFEval\\nTests a model’s ability to follow explicit instructions,\\nfocusing on formatting adherence\\nSource\\nBBH (Big Bench Hard)\\n23 challenging tasks from the BigBench dataset to\\nevaluate LLMs using objective metrics\\nSource\\nMATH\\nCompilation of high-school level competition prob-\\nlems formatted using LaTeX and Asymptote\\nSource\\nGPQA\\nChallenging\\nknowledge\\ndataset\\nwith\\nquestions\\ncrafted by PhD-level domain experts\\nSource\\nMuSR\\nDataset with complex problems requiring models to\\nintegrate reasoning with long-range context parsing\\nSource\\nMMLU-PRO\\nRefined version of MMLU with higher quality and\\nmore challenging multiple-choice questions\\nSource\\nARC\\nMeasures machine reasoning with a dataset of grade-\\nschool science questions\\nSource\\nCOQA\\nA dataset for building conversational question-\\nanswering systems\\nSource\\nDROP\\nEvaluates the ability to perform discrete reasoning\\nover paragraphs of text\\nSource\\nSQuAD\\nA reading comprehension dataset for evaluating\\nmodels’ ability to answer questions based on pas-\\nsages of text\\nSource\\nTREC\\nA benchmark for evaluating text retrieval method-\\nologies\\nSource\\nWMT\\nA dataset and benchmark for evaluating machine\\ntranslation models\\nSource\\nXNLI\\nA dataset for evaluating cross-lingual language un-\\nderstanding\\nSource\\nPiQA\\nA dataset for evaluating models’ understanding of\\nphysical interactions\\nSource\\nWinogrande\\nA large-scale dataset for evaluating commonsense\\nreasoning\\nSource\\nTable 7.1: Detailed Overview of Benchmark Datasets Used for Evaluating Language Model Performance.\\nAs LLMs evolve, so do benchmarks, with new standards such as BigCodeBench challenging current\\nbenchmarks and setting new standards in the domain. Given the diverse nature of LLMs and the tasks\\nthey can perform, the choice of benchmarks depends on the specific tasks the LLM is expected to handle.\\nFor generic applicability, various benchmarks for different downstream applications and reasoning should\\nbe utilised. For domain/task-specific LLMs, benchmarking can be limited to relevant benchmarks like\\nBigCodeBench for coding.\\n61\\n7.8\\nEvaluating Fine-Tuned LLMs on Safety Benchmark\\nThe safety aspects of Large Language Models (LLMs) are increasingly scrutinised due to their ability\\nto generate harmful content when influenced by jailbreaking prompts. These prompts can bypass the\\nembedded safety and ethical guidelines within the models, similar to code injection techniques used in\\ntraditional computer security to circumvent safety protocols. Notably, models like ChatGPT, GPT-\\n3, and InstructGPT are vulnerable to such manipulations that remove content generation restrictions,\\npotentially violating OpenAI’s guidelines. This underscores the necessity for robust safeguards to ensure\\nLLM outputs adhere to ethical and safety standards.\\nDecodingTrust [79] provides a comprehensive evaluation of the trustworthiness of LLMs, notably com-\\nparing GPT-4 with GPT-3.5 (ChatGPT). This evaluation spans several critical areas:\\n1. Toxicity: Optimisation algorithms and generative models are employed to create challenging\\nprompts that test the model’s ability to avoid generating harmful content.\\n2. Stereotype Bias: An array of demographic groups and stereotype topics are utilised to assess\\nmodel bias, helping to understand and mitigate prejudiced responses.\\n3. Adversarial Robustness: The resilience of models against adversarial attacks is tested by chal-\\nlenging them with sophisticated algorithms intended to deceive or mislead.\\n4. Out-of-Distribution (OOD) Robustness: Models are evaluated on their ability to handle\\ninputs that differ significantly from their training data, such as poetic or Shakespearean styles.\\n5. Robustness to Adversarial Demonstrations: Demonstrations that contain misleading infor-\\nmation are used to test the model’s robustness across various tasks.\\n6. Privacy: Various levels of privacy evaluation assess how well models safeguard sensitive informa-\\ntion during interactions and understand privacy-related contexts.\\n7. Hallucination Detection: Identifies instances where the model generates information not grounded\\nin the provided context or factual data. Lower hallucination rates improve the reliability and trust-\\nworthiness of the LLM’s outputs.\\n8. Tone Appropriateness: Assesses whether the model’s output maintains an appropriate tone for\\nthe given context. This is particularly important for applications in customer service, healthcare,\\nand other sensitive areas.\\n9. Machine Ethics: Ethical assessments involve testing models with scenarios that require moral\\njudgments, using datasets like ETHICS and Jiminy Cricket.\\n10. Fairness: The fairness of models is evaluated by generating tasks that vary protected attributes,\\nensuring equitable responses across different demographic groups.\\nThe dataset employed for evaluating the aforementioned eight safety dimensions can be found here.\\nIn partnership with HuggingFace, the LLM Safety Leaderboard utilises DecodingTrust’s framework to\\nprovide a unified evaluation platform for LLM safety.\\nThis allows researchers and practitioners to\\nbetter understand the capabilities, limitations, and risks associated with LLMs. Users are encouraged to\\nsubmit their models to HuggingFace for evaluation, ensuring they meet the evolving standards of safety\\nand reliability in the field.\\n7.9\\nEvaluating Safety of Fine-Tuned LLM using AI Models\\n7.9.1\\nLlama Guard\\nLlama Guard 2[80] is a safeguard model built on LLMs for managing risks in conversational AI applica-\\ntions. It effectively categorises both input prompts and responses from AI agents using a detailed safety\\nrisk taxonomy tailored to identify potential legal and policy risks in AI interactions. It utilises a detailed\\nsafety risk taxonomy designed to identify and manage potential legal and policy risks in interactions\\ninvolving conversational AI. This taxonomy enables effective classification in areas such as:\\n• Violence & Hate, addressing content that could incite violent acts or discrimination.\\n62\\n• Sexual Content, targeting sexually explicit material or behaviour, especially involving minors.\\n• Guns & Illegal Weapons, concerning the promotion or instruction of illegal armaments.\\n• Regulated or Controlled Substances, covering illegal drugs and other controlled substances.\\n• Suicide & Self-Harm, aimed at content that could encourage self-destructive behaviour.\\n• Criminal Planning, for content that could assist in planning or executing criminal activities.\\nThe core of Llama Guard 2 is its robust framework that allows for both prompt and response classifica-\\ntion, supported by a high-quality dataset that enhances its ability to monitor conversational exchanges.\\nOperating on a Llama2-7b model, Llama Guard 2 has been instruction-tuned to deliver strong perfor-\\nmance on benchmarks like the OpenAI Moderation Evaluation dataset and ToxicChat, where it matches\\nor surpasses the capabilities of existing content moderation tools.\\nThe model supports multi-class classification and generates binary decision scores. Its instruction fine-\\ntuning allows for extensive customisation of tasks and adaptation of output formats. This feature enables\\nusers to modify taxonomy categories to align with specific use cases and supports flexible prompting\\ncapabilities, including zero-shot and few-shot applications. The adaptability and effectiveness of Llama\\nGuard make it a vital resource for developers and researchers. By making its model weights publicly\\navailable, Llama Guard 2 encourages ongoing development and customisation to meet the evolving needs\\nof AI safety within the community.\\nLlama Guard 3 represents the latest advancement over Llama Guard 2, having been fine-tuned on the\\nLlama 3 8b model. The key difference between the two versions is that Llama Guard 3 expands upon\\nthe capabilities of Llama Guard 2 by introducing three new categories: Defamation, Elections, and\\nCode Interpreter Abuse.\\nPython Library: Llama Guard 3 is accessible via HuggingFace’s AutoModelForCausalLM.2 A detailed\\ntutorial is available at this link. Please note that access to the model requires submitting a request to\\nHugging Face with the user details. Additionally, the model weights can be downloaded from the Meta\\nplatform by providing user details, and the link can be found here.\\nThe prompt formats for these two models also differ, with the specific formats for Llama Guard 2 available\\nhere and Llama Guard 3 is accessible here.\\n7.9.2\\nShield Gemma\\nShieldGemma [81] is an advanced content moderation model built on the Gemma2 platform, designed\\nto enhance the safety and reliability of interactions between LLMs and users. It effectively filters both\\nuser inputs and model outputs to mitigate key harm types, including offensive language, hate speech,\\nmisinformation, and explicit content. The model’s scalability, with options ranging from 2B to 27B\\nparameters, allows for tailored applications that meet specific needs, such as reducing latency in online\\nsafety applications or enhancing performance in complex decision-making tasks.\\nA distinguishing feature of ShieldGemma is its novel approach to data curation. It leverages synthetic\\ndata generation techniques to create high-quality datasets that are robust against adversarial prompts\\nand fair across diverse identity groups. This reduces the need for extensive human annotation, streamlin-\\ning the data preparation process while ensuring the model’s effectiveness. Compared to existing content\\nmoderation tools like LlamaGuard and WildGuard, which typically offer fixed-size models and limited\\ncustomisation, ShieldGemma’s flexible architecture and advanced data handling capabilities provide a\\nmore adaptable and efficient solution.\\nThese innovations position ShieldGemma as a significant ad-\\nvancement in LLM-based content moderation, offering developers and researchers a versatile tool that\\npromotes safer and more reliable AI interactions across various platforms.\\nPython Library: The ShieldGemma series is available on HuggingFace via AutoModelForCausalLM.\\nThe models can be accessed here. A tutorial for running ShieldGemma 2B on Google Colab can be found\\nhere. Similar to Llama Guard series, ShieldGemma series also has guidelines for prompting and it can\\nbe found here.\\n7.9.3\\nWILDGUARD\\nWILDGUARD [82] is an innovative open-source tool developed to enhance the safety of interactions\\nwith large language models (LLMs).\\nThis tool addresses three critical moderation tasks: detecting\\n2https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForCausalLM\\n63\\nharmful intent in user prompts, identifying safety risks in model responses, and determining when a\\nmodel appropriately refuses unsafe requests.\\nCentral to its development is WILDGUARD MIX3, a\\nmeticulously curated dataset comprising 92,000 labelled examples that include both benign prompts and\\nadversarial attempts to bypass safety measures. The dataset is divided into WILDGUARD TRAIN, used\\nfor training the model, and WILDGUARD TEST, consisting of high-quality human-annotated examples\\nfor evaluation.\\nThe WILDGUARD model itself is fine-tuned on the Mistral-7B language model using the WILDGUARD\\nTRAIN dataset, enabling it to perform all three moderation tasks in a unified, multi-task manner. Results\\nshow that WILDGUARD surpasses existing open-source moderation tools in effectiveness, particularly\\nexcelling in handling adversarial prompts and accurately detecting model refusals. On many benchmarks,\\nWILDGUARD’s performance is on par with or exceeds that of GPT-4, a much larger, closed-source\\nmodel.\\nThe quick start guide and additional information on WILDGUARD are available in GitHub and it can\\nbe accessed here.\\n3https://huggingface.co/datasets/allenai/wildguardmix\\n64\\nChapter 8\\nStage 6: Deployment\\n8.1\\nSteps Involved in Deploying the Fine-Tuned Model\\n1. Model Export: Save the fine-tuned model in a suitable format (e.g., ONNX, TensorFlow Saved-\\nModel, PyTorch) for deployment.\\n2. Infrastructure Setup: Prepare the deployment environment, including necessary hardware, cloud\\nservices, and containerisation tools.\\n3. API Development: Create APIs to allow applications to interact with the model, facilitating\\nprediction requests and responses.\\n4. Deployment: Deploy the model to the production environment, making it accessible to end-users\\nor applications.\\n8.2\\nCloud-Based Providers for LLM Deployment\\nCloud-based large language model (LLM) inferencing frequently employs a pricing model based on the\\nnumber of tokens processed. Users are charged according to the volume of text analysed or generated\\nby the model. While this pricing structure can be cost-effective for sporadic or small-scale usage, it may\\nnot always be economical for larger or continuous workloads.\\nIn some scenarios, hosting an LLM solution in-house may offer better long-term cost savings, especially if\\nthere is consistent or high-volume usage. Managing your own infrastructure provides greater control over\\nresource allocation and allows for cost optimisation based on specific needs. Additionally, self-hosting\\noffers advantages in terms of data privacy and security, as sensitive information remains within your own\\nenvironment.\\nHowever, it is crucial to carefully evaluate the total cost of ownership when comparing cloud-based\\nsolutions with self-hosted alternatives. This evaluation should consider factors such as hardware expenses,\\nmaintenance, and operational overheads. Ultimately, the decision should be informed by a comprehensive\\ncost-benefit analysis, considering both short-term affordability and long-term sustainability.\\nSeveral companies offer deployment services for large language models (LLMs), providing a range of\\ntools and platforms to efficiently implement and manage these models. Here’s a detailed list of some\\nprominent providers and their services:\\n• Amazon Web Services (AWS)\\n– Amazon Bedrock: This service offers a suite of foundation models including Amazon Ti-\\ntan, which supports various NLP tasks such as summarisation and text generation. Bedrock\\nintegrates seamlessly with other AWS services for scalable and secure deployment.\\n– Amazon SageMaker: Provides an end-to-end machine learning service that includes tools\\nfor building, training, and deploying LLMs. SageMaker JumpStart offers pre-trained models\\nand step-by-step guides to simplify the deployment process.\\n65\\n– Tutorial: This tutorial explains the deployment of LLM Agents on Amazon Bedrock. An-\\nother tutorial explains end-to-end fine-tuning and deployment of LLMs with Sagemaker Can-\\nvas and Amazon Bedrock. General guidelines of Amazon Bedrock for LLM users can be found\\nhere.\\n• Microsoft Azure\\n– Azure OpenAI Service: This service offers access to OpenAI’s powerful models like GPT-\\n3.5 and Codex. It provides capabilities for embedding, image generation with DALL-E, and\\nspeech-to-text with Whisper. Azure’s integration with OpenAI models ensures robust deploy-\\nment options for various applications.\\n– Azure Machine Learning: Supports the deployment of custom and pre-trained models,\\noffering tools for model management, deployment, and monitoring. It integrates with Azure’s\\nbroader ecosystem for scalable and secure ML operations.\\n– Tutorial: Here is the tutorial for creating and deploying an Azure OpenAI Service in Mi-\\ncrosoft Azure platform.\\n• Google Cloud Platform (GCP)\\n– Vertex AI: This platform allows the deployment of large language models with tools for\\ntraining, tuning, and serving models. Vertex AI supports models like BERT and GPT-3,\\nproviding extensive MLOps capabilities for end-to-end management.\\n– Cloud AI API: Offers APIs for NLP tasks such as translation, sentiment analysis, and\\nentity recognition. These APIs are backed by Google’s powerful infrastructure, ensuring high\\nperformance and reliability.\\n– Tutorial: This document contains a tutorial for training and deploying an LLM in GCP.\\n• Hugging Face\\n– Inference API: This service allows users to deploy and manage LLMs hosted on Hugging\\nFace’s infrastructure. It supports various models from the Transformers library and provides\\nan easy-to-use API for integrating these models into applications.\\n– Spaces: A collaborative environment where users can deploy and share models using Hugging\\nFace’s hosting platform. It supports deploying custom models and interactive demos.\\n– Tutorial: This document contains a tutorial for training and deploying an LLM using Hug-\\ngingFace Inference API.\\n• Other Platforms\\n– OpenLLM: Provides deployment solutions here.\\n– Deepseed: Offers deployment solutions here.\\n8.3\\nTechniques for Optimising Model Performance During In-\\nference\\nOptimising model performance during inference is crucial for the efficient deployment of large language\\nmodels (LLMs). The following advanced techniques offer various strategies to enhance performance,\\nreduce latency, and manage computational resources effectively.\\n8.3.1\\nTraditional On-Premises GPU-Based Deployments\\nThis conventional approach to deploying large language models (LLMs) involves using Graphics Process-\\ning Units (GPUs) due to their parallel processing capabilities, which enable fast and efficient inference.\\nHowever, this method requires upfront hardware investment and may not be suitable for applications\\nwith fluctuating demand or limited budgets. GPU-based deployments face several challenges:\\n1. Resource utilisation may suffer during periods of low demand due to idle servers.\\n2. Scaling up or down often requires physical hardware modifications, which can be time-consuming.\\n66\\n3. Centralised servers can introduce single points of failure and scalability limitations.\\nTo mitigate these issues, strategies such as load balancing between multiple GPUs, fallback routing, model\\nparallelism, and data parallelism can be employed to achieve better results. Optimisation techniques like\\ndistributed inference using PartialState from accelerate can further enhance efficiency.\\nExample use case: Large-Scale NLP Application\\nFor instance, a large e-commerce platform implemented traditional on-premises GPU-based deployment\\nto handle millions of customer queries daily. By utilising load balancing and model parallelism, they\\nwere able to achieve a significant reduction in latency and improved customer satisfaction.\\n8.3.2\\nDistributed LLM: Torrent-Style Deployment and Parallel Forward Passes\\nAn innovative deployment strategy for large language models (LLMs) involves distributing them across\\nmultiple GPUs in a decentralised, torrent-style manner. Libraries like Petals1 can perform this task.\\nPetals functions as a decentralised pipeline designed for rapid neural network inference by partitioning\\nthe model into distinct blocks or layers, which are distributed across multiple geographically dispersed\\nservers. Users can connect their own GPUs to this network, acting as both contributors and clients who\\ncan access and apply the model to their data.\\nWhen a client request is received, the network routes it through a series of servers optimised to minimise\\nthe total forward pass time. Each server dynamically selects the most optimal set of blocks, adapting to\\nthe current bottlenecks in the pipeline. This framework leverages decentralisation principles to distribute\\ncomputational load across diverse regions, sharing computational resources and GPUs in a way that\\nreduces the financial burden on individual organisations. This collaborative approach not only optimises\\nresource utilisation but also fosters a global community dedicated to shared AI goals.\\nFigure 8.1: Conceptual Representation of Distributed LLM Deployment Using a Torrent-Style Approach.\\nThis figure illustrates the distributed deployment of a Large Language Model (LLM) using a torrent-style\\napproach, where multiple GPT model layers (stacks) are distributed across different nodes (represented\\nby chefs) and perform parallel forward passes. The process mimics the flow of orders from customers\\n(input data) through restaurants (intermediate processing layers) to chefs (model layers), highlighting\\nthe efficiency of parallel processing and distributed computing in handling large-scale language models.\\nThis approach is essential for reducing inference latency and improving the scalability of LLMs across\\ndiverse computational environments. (adapted from [83])\\n1https://github.com/bigscience-workshop/petals\\n67\\nExample use case: Global Research Collaboration\\nA consortium of research institutions implemented a distributed LLM using the Petals framework to\\nanalyse large datasets across different continents. By leveraging the decentralised nature of Petals, they\\nachieved high efficiency in processing and collaborative model development.\\n8.3.3\\nWebGPU-Based Deployment of LLM\\nThis deployment option for large language models (LLMs) involves utilising WebGPU, a web standard\\nthat provides a low-level interface for graphics and compute applications on the web platform. With\\nWebGPU, organisations can harness the power of GPUs directly within web browsers, enabling effi-\\ncient inference for LLMs in web-based applications. WebGPU enables high-performance computing and\\ngraphics rendering directly within the client’s web browser. It allows developers to utilise the client’s\\nGPU for tasks such as rendering graphics, accelerating computational workloads, and performing par-\\nallel processing, all without the need for plugins or additional software installations. This capability\\npermits complex computations to be executed efficiently on the client’s device, leading to faster and\\nmore responsive web applications.\\n8.3.4\\nLLM on WebGPU using WebLLM\\nClients can access powerful large language models and chatbots directly in their browser, leveraging\\nWebGPU acceleration. This approach eliminates server dependencies, providing users with exceptional\\nperformance and enhanced privacy. WebLLM facilitates the use of large language models directly in the\\nclient’s browser to perform tasks such as filtering out personally identifiable information (PII) or named\\nentity recognition (NER) on data without transmitting it over the network.\\nThis ensures enhanced\\nprivacy and security by retaining sensitive information on the client side.\\n68\\nFigure 8.2: WebGPU-Based Deployment of LLM: This diagram illustrates the architecture of deploying\\na large language model (LLM) using WebGPU technology. The CPU manages the distribution of prompt\\ninferencing tasks to multiple GPUs, which then process these prompts in parallel, enhancing efficiency\\nand scalability in LLM deployment across web-based platforms. (adapted from [83])\\nAdditional Use Cases for WebLLM\\n1. Language Translation: Enable real-time translation of text directly in the browser, allowing\\nusers to communicate across language barriers without transmitting their messages over the net-\\nwork.\\n2. Code Autocompletion: Develop code editors that provide intelligent autocompletion suggestions\\nbased on context, leveraging WebLLM to understand and predict code snippets.\\n3. Customer Support Chatbots: Implement chatbots on websites to provide instant customer\\nsupport and answer frequently asked questions without relying on external servers.\\n4. Data Analysis and Visualisation: Create browser-based tools for analysing and visualising\\ndata, with WebLLM assisting in data processing, interpretation, and generating insights.\\n5. Personalised Recommendations:\\nDevelop recommendation engines that offer personalised\\nproduct recommendations, content suggestions, or movie/music recommendations based on user\\npreferences and behaviour.\\n6. Privacy-Preserving Analytics: Develop analytics platforms that perform data analysis directly\\nin the browser, ensuring that sensitive information remains on the client side and reducing the risk\\nof data breaches.\\n69\\nExample use case: Privacy-Focused Web Application\\nA healthcare startup deployed an LLM using WebLLM to process patient information directly within the\\nbrowser, ensuring data privacy and compliance with healthcare regulations. This approach significantly\\nreduced the risk of data breaches and improved user trust.\\n8.3.5\\nQuantised LLMs\\nModel quantisation is a technique utilised to reduce the size of an AI model by representing its parameters\\nwith fewer bits. In traditional machine learning models, each parameter (e.g., weights and biases in neural\\nnetworks) is typically stored as a 32-bit floating-point number, necessitating significant memory and\\ncomputational resources, particularly for large models. Quantisation aims to alleviate this by reducing\\nthe precision of these parameters. For instance, instead of storing each parameter as a 32-bit floating-\\npoint number, they may be represented using fewer bits, such as 8-bit integers.\\nThis compression\\nreduces the memory footprint of the model, making it more efficient to deploy and execute, especially in\\nresource-constrained environments like mobile devices or edge devices. QLoRA is a popular example of\\nthis quantisation for LLMs and can be used to deploy LLMs locally or host them on external servers.\\nExample use case: Edge Device Deployment\\nA tech company used quantised LLMs to deploy advanced NLP models on mobile devices, enabling offline\\nfunctionality for applications such as voice recognition and translation. This deployment significantly\\nimproved app performance and user experience by reducing latency and reliance on internet connectivity.\\n8.3.6\\nvLLMs\\nThe vLLM2 system efficiently handles requests by employing a block-level memory management method\\nand preemptive request scheduling. It utilises the PagedAttention[84] algorithm to manage the key-\\nvalue (KV) cache, thereby reducing memory waste and fragmentation. By batching requests and sharing\\nphysical blocks across multiple samples, vLLM optimises memory usage and enhances throughput. Per-\\nformance tests indicate that vLLM surpasses other systems in various decoding scenarios. Consider a\\ntransformer-based model tasked with summarising a lengthy book. Traditional transformers process the\\nentire book simultaneously, which can be both computationally and memory-intensive, especially for ex-\\ntended texts. With PagedAttention, the book is divided into smaller segments or pages. The model then\\nfocuses on summarising one page at a time, rather than the entire book simultaneously. This approach\\nreduces computational complexity and memory requirements, making it more feasible to process and\\nsummarise lengthy texts efficiently.\\nExample use case: High-Volume Content Generation\\nA content marketing agency implemented vLLMs for generating large volumes of SEO-optimised content.\\nBy leveraging the efficient memory management of vLLMs, they were able to handle multiple concurrent\\nrequests, significantly increasing their content production rate while maintaining high quality.\\n8.4\\nKey Considerations for Deployment of LLMs\\nDeploying large language models (LLMs) effectively requires careful planning and consideration of various\\nfactors to ensure optimal performance, cost-efficiency, and security. Key considerations include:\\n• Infrastructure Requirements:\\n– Compute Resources: Ensure adequate CPU/GPU resources to handle the model’s compu-\\ntational demands. High-performance GPUs are typically required for efficient inference and\\ntraining.\\n– Memory: LLMs, especially those with billions of parameters, require substantial memory.\\nMemory management techniques such as quantisation and model parallelism can be employed\\nto optimise usage.\\n2https://docs.vllm.ai/en/stable/\\n70\\n• Scalability:\\n– Horizontal Scaling: Plan for horizontal scaling to distribute the load across multiple servers,\\nwhich can improve performance and handle increased demand.\\n– Load Balancing: Implement load balancing strategies to ensure even distribution of requests\\nand prevent any single point of failure.\\n• Cost Management:\\n– Token-based Pricing: Understand the cost implications of token-based pricing models of-\\nfered by cloud providers. This model charges based on the number of tokens processed, which\\ncan become expensive with high usage.\\n– Self-Hosting: Evaluate the costs and benefits of self-hosting versus cloud hosting.\\nSelf-\\nhosting might offer long-term savings for consistent, high-volume usage but requires significant\\nupfront investment in hardware and ongoing maintenance.\\n• Performance Optimisation:\\n– Latency: Minimise latency to ensure real-time performance, particularly for applications\\nrequiring instant responses like chatbots and virtual assistants.\\n– Throughput: Maximise throughput to handle a high volume of requests efficiently. Tech-\\nniques like batching and efficient memory management (e.g., PagedAttention) can help.\\n• Security and Privacy:\\n– Data Security: Implement robust security measures to protect sensitive data, including\\nencryption and secure access controls.\\n– Privacy: Ensure compliance with data privacy regulations by keeping sensitive data within\\nyour environment if self-hosting, or ensuring cloud providers comply with relevant privacy\\nstandards.\\n• Maintenance and Updates:\\n– Model Updates: Regularly update the model to incorporate new data and improve perfor-\\nmance. Automate this process if possible to reduce manual effort.\\n– System Maintenance: Plan for regular maintenance of the infrastructure to prevent down-\\ntime and ensure smooth operation.\\n• Flexibility and Customisation:\\n– Fine-Tuning:\\nAllow for model fine-tuning to adapt the LLM to specific use cases and\\ndatasets. Fine-tuning can improve accuracy and relevance in responses.\\n– API Integration: Ensure the deployment platform supports easy integration with existing\\nsystems and workflows through APIs and SDKs.\\n• User Management:\\n– Access Control: Implement role-based access control to manage who can deploy, use, and\\nmaintain the LLM.\\n– Monitoring and Logging: Set up comprehensive monitoring and logging to track usage,\\nperformance, and potential issues. This helps in proactive troubleshooting and optimisation.\\n• Compliance:\\n– Regulatory Compliance: Ensure that the deployment adheres to all relevant regulatory\\nand legal requirements, including data protection laws like GDPR, HIPAA, etc.\\n– Ethical Considerations: Implement ethical guidelines to avoid biases and ensure the re-\\nsponsible use of LLMs.\\n• Support and Documentation:\\n– Technical Support: Choose a deployment platform that offers robust technical support and\\nresources.\\n– Documentation: Provide comprehensive documentation for developers and users to facili-\\ntate smooth deployment and usage.\\n71\\nChapter 9\\nStage 7: Monitoring and\\nMaintenance\\n9.1\\nSteps Involved in Monitoring and Maintenance of Deployed\\nFine-Tuned LLMs\\nContinuous monitoring and maintenance of fine-tuned LLMs are essential to ensure their optimal per-\\nformance, accuracy, and security over time. Below are the key steps involved in this process:\\n1. Setup Initial Baselines: Establish initial performance baselines by evaluating the model on a\\ncomprehensive test dataset. Record metrics such as accuracy, latency, throughput, and error rates\\nto serve as reference points for future monitoring.\\n2. Performance Monitoring: Implement systems to continuously track key performance metrics\\nsuch as response time, server load, and token usage. Regularly compare these metrics against the\\nestablished baselines to detect any deviations.\\n3. Accuracy Monitoring: Continuously evaluate the model’s predictions against a ground truth\\ndataset. Use metrics like precision, recall, F1 score, and cross-entropy loss to ensure the model\\nmaintains high accuracy levels.\\n4. Error Monitoring: Track and analyse errors, including runtime errors and prediction errors.\\nImplement logging mechanisms to capture detailed information about each error for troubleshooting\\nand improvement.\\n5. Log Analysis: Maintain comprehensive logs for each prediction request and response, including\\ninput data, output predictions, response times, and encountered errors. Regularly review logs to\\nidentify patterns and areas for improvement.\\n6. Alerting Mechanisms: Set up automated alerting systems to notify stakeholders of any anomalies\\nor deviations from expected performance metrics. Integrate alerts with communication tools like\\nSlack, PagerDuty, or email for timely responses.\\n7. Feedback Loop: Establish a feedback loop with end-users to gather insights on model performance\\nand user satisfaction. Use this feedback to continuously refine and improve the model.\\n8. Security Monitoring: Implement robust security measures to monitor for threats, including\\nunauthorised access, data breaches, and adversarial attacks. Use encryption, access control, and\\nregular security audits to protect the model and data.\\n9. Drift Detection: Continuously monitor for data and concept drift using statistical tests and\\ndrift detectors. Regularly evaluate the model on holdout datasets to detect changes in input data\\ndistribution or model performance.\\n10. Model Versioning: Maintain version control for different iterations of the model. Track perfor-\\nmance metrics for each version to ensure that the best-performing model is in production.\\n72\\n11. Documentation and Reporting: Keep detailed documentation of monitoring procedures, met-\\nrics, and findings. Generate regular reports to provide stakeholders with insights into the model’s\\nperformance and maintenance activities.\\n12. Periodic Review and Update: Regularly assess and update the monitoring processes to incor-\\nporate new techniques, tools, and best practices, ensuring the monitoring system remains effective\\nand up-to-date.\\n9.2\\nContinuous Monitoring of Model Performance\\nWhile large language model (LLM) applications undergo some form of evaluation, continuous monitoring\\nremains inadequately implemented in most cases. This section outlines the components necessary to\\nestablish an effective monitoring programme aimed at safeguarding users and preserving brand integrity.\\n9.2.1\\nFunctional Monitoring\\nInitially, it is crucial to monitor fundamental metrics consistently. This includes tracking metrics such\\nas request volume, response times, token utilisation, costs incurred, and error rates.\\n9.2.2\\nPrompt Monitoring\\nFollowing functional metrics, attention should be directed towards monitoring user-generated prompts\\nor inputs. Metrics like readability can provide valuable insights. LLM evaluators should be employed to\\ndetect potential toxicity in responses. Additionally, metrics such as embedding distances from reference\\nprompts prove insightful, ensuring adaptability to varying user interactions over time.\\nIntroducing a new evaluation category involves identifying adversarial attempts or malicious prompt\\ninjections, often overlooked in initial evaluations. Comparison against reference sets of known adversarial\\nprompts helps identify and flag malicious activities. Evaluative LLMs play a crucial role in classifying\\nprompts as benign or malicious.\\n9.2.3\\nResponse Monitoring\\nMonitoring responses involves several critical checks to ensure alignment with expected outcomes. Pa-\\nrameters such as relevance, coherence (hallucination), topical alignment, sentiment, and their evolution\\nover time are essential. Metrics related to toxicity and harmful output require frequent monitoring due\\nto their critical impact. Prompt leakage represents an adversarial tactic wherein sensitive prompt in-\\nformation is illicitly extracted from the application’s stored data. Monitoring responses and comparing\\nthem against the database of prompt instructions can help detect such breaches. Embedding distance\\nmetrics are particularly effective in this regard. Regular testing against evaluation datasets provides\\nbenchmarks for accuracy and highlights any performance drift over time. Tools capable of managing\\nembeddings allow exportation of underperforming output datasets for targeted improvements.\\n9.2.4\\nAlerting Mechanisms and Thresholds\\nEffective monitoring necessitates well-calibrated alerting thresholds to avoid excessive false alarms. Im-\\nplementing multivariate drift detection and alerting mechanisms can enhance accuracy. Consideration\\nof false alarm rates and best practices for setting thresholds is paramount for effective monitoring sys-\\ntem design. Alerting features should include integration with communication tools such as Slack and\\nPagerDuty. Some systems offer automated response blocking in case of alerts triggered by problematic\\nprompts. Similar mechanisms can be employed to screen responses for personal identifiable information\\n(PII), toxicity, and other quality metrics before delivery to users. Custom metrics tailored to specific\\napplication nuances or innovative insights from data scientists can significantly enhance monitoring ef-\\nficacy. Flexibility to incorporate such metrics is essential to adapt to evolving monitoring needs and\\nadvancements in the field.\\n73\\n9.2.5\\nMonitoring User Interface (UI)\\nThe monitoring system’s UI is pivotal, typically featuring time-series graphs of monitored metrics. Dif-\\nferentiated UIs facilitate in-depth analysis of alert trends, aiding root cause analysis.\\nAdvanced UI\\ncapabilities may include visualisations of embedding spaces through clustering and projections, provid-\\ning insights into data patterns and relationships. Mature monitoring systems categorise data by users,\\nprojects, and teams, ensuring role-based access control (RBAC) to protect sensitive information. Op-\\ntimising alert analysis within the UI interface remains an area where improvements can significantly\\nreduce false alarm rates and enhance operational efficiency.\\n9.3\\nUpdating LLM Knowledge\\nTo improve the knowledge base of an LLM, continued pretraining is used to help LLM evolve with the\\nlatest knowledge and information. The world and language are constantly evolving. New information\\nemerges, trends shift, and cultural references change. LLMs trained on static data can become outdated,\\nleading to:\\n• Factual Errors: Outdated information can cause LLMs to provide inaccurate responses.\\n• Irrelevance: Models might miss the context of current events or use outdated references.\\n• Bias Perpetuation: Biases present in training data can become entrenched if not addressed\\nthrough updates.\\n9.3.1\\nRetraining Methods\\n• Periodic Retraining: This involves refreshing the model’s knowledge base at regular intervals\\n(weekly, monthly, yearly) with new data. This is a straightforward method but requires a steady\\nstream of high-quality, unbiased data.\\n• Trigger-Based Retraining: This approach monitors the LLM’s performance. When metrics like\\naccuracy or relevance fall below a certain threshold, a retraining process is triggered. This method\\nis more dynamic but requires robust monitoring systems and clear performance benchmarks.\\n9.3.2\\nAdditional Methods\\n• Fine-Tuning: LLMs can be fine-tuned for specific tasks by training them on smaller, domain-\\nspecific datasets. This allows for specialisation without complete retraining.\\n• Active Learning: This approach involves selectively querying the LLM to identify areas where\\nit lacks knowledge. The retrieved information is then used to update the model.\\n9.3.3\\nKey Considerations\\n• Data Quality and Bias: New training data must be carefully curated to ensure quality and\\nmitigate bias. Techniques like human annotation and fairness checks are crucial.\\n• Computational Cost: Retraining LLMs can be computationally expensive, requiring significant\\nresources. Optimisations like transfer learning (using pre-trained models as a starting point) can\\nhelp reduce costs.\\n• Downtime: Retraining often takes time, leading to LLM downtime. Strategies like rolling updates\\nor deploying multiple models can minimise service disruptions.\\n• Version Control: Tracking different versions of the LLM and their training data is essential for\\nrollbacks in case of performance issues.\\n74\\n9.4\\nThe Future of LLM Updates\\nResearch is ongoing to develop more efficient and effective LLM update strategies. One promising area\\nis continuous learning, where LLMs can continuously learn and adapt from new data streams without\\nretraining from scratch. Continuous learning aims to reduce the need for frequent full-scale retraining by\\nenabling models to update incrementally with new information. This approach can significantly enhance\\nthe model’s ability to remain current with evolving knowledge and language use, improving its long-term\\nperformance and relevance.\\nInnovations in transfer learning and meta-learning are also contributing to advancements in LLM updates.\\nThese techniques allow models to leverage pre-existing knowledge and adapt quickly to new tasks or\\ndomains with minimal additional training.\\nBy integrating these advanced learning methods, future\\nLLMs can become more adaptable and efficient in processing and understanding new information.\\nFurthermore, ongoing improvements in hardware and computational resources will support more frequent\\nand efficient updates. As processing power increases and becomes more accessible, the computational\\nburden of updating large models will decrease, enabling more regular and comprehensive updates.\\nCollaboration between academia and industry is vital in driving these advancements. By sharing research\\nfindings and best practices, the field can collectively move towards more robust and efficient LLM update\\nmethodologies, ensuring that models remain accurate, relevant, and valuable over time.\\n75\\nChapter 10\\nIndustrial Fine-Tuning Platforms\\nand Frameworks for LLMs\\nThe evolution of fine-tuning techniques has been propelled by leading tech companies and platforms that\\nhave introduced innovative frameworks and services. Companies like HuggingFace, Amazon Web Services\\n(AWS), Microsoft Azure, and OpenAI have developed tools and platforms that simplify and democratise\\nthe fine-tuning process. These advancements have not only lowered the barrier to entry for leveraging\\nstate-of-the-art AI models but have also enabled a wide range of applications across various industries,\\nfrom healthcare and finance to customer service and content creation. Each of these platforms offers\\nunique capabilities that cater to different needs, whether it be through automated fine-tuning workflows,\\nscalable cloud-based training environments, or accessible API interfaces for deploying custom models.\\nHuggingFace, for example, has made significant strides with its Transformers library1 and tools like Au-\\ntotrain2 and SetFit, which allow users to fine-tune models with minimal coding and data. Their platform\\nprovides a robust infrastructure that supports both the research community and industry practitioners,\\nfacilitating the rapid development and deployment of custom AI solutions. Similarly, AWS’s SageMaker3\\nand SetFit4 provides an extensive suite of services that cover the entire machine learning lifecycle, from\\ndata preparation and training to model deployment and optimisation, making it a comprehensive solu-\\ntion for enterprise-level applications.\\nOn the other hand, Microsoft Azure integrates its fine-tuning capabilities with enterprise-grade tools\\nand services, offering solutions like Azure Machine Learning and the Azure OpenAI Service that cater to\\nlarge organisations looking to incorporate advanced AI into their operations. Azure’s focus on MLOps\\nand seamless integration with other Azure services ensures that fine-tuned models can be efficiently de-\\nployed and maintained in production environments. Meanwhile, OpenAI has pioneered the concept of\\n”fine-tuning as a service” allowing businesses to leverage their powerful models like GPT-4 through a\\nuser-friendly API 5, enabling custom model adaptations without the need for in-house AI expertise or\\ninfrastructure.\\nThe collective efforts of these tech companies have not only enhanced the efficiency and scalability of\\nfine-tuning but also democratised access to sophisticated AI tools. By reducing the technical barriers\\nand providing comprehensive, user-friendly platforms, these innovations have enabled a wider range of\\nindustries to deploy advanced AI models tailored to their specific needs. Tables 10.1 and 10.2 offer a\\nquick comparison of LLM fine-tuning tools and frameworks from different providers.\\n1https://huggingface.co/docs/transformers/en/index/\\n2https://huggingface.co/autotrain\\n3https://huggingface.co/autotrain\\n4https://aws.amazon.com/sagemaker/\\n5https://platform.openai.com/docs/guides/fine-tuning/fine-tuning-integrations\\n76\\nParameter\\nNVIDIA\\nNeMo\\nHugging\\nFace\\nAutoTrain\\nAPI\\nAmazon\\nBedrock\\nAWS\\nSage-\\nMaker\\nJump-\\nStart\\nHugging\\nFace\\nTrainer API\\nPrimary Use\\nCase\\nCustom\\nfine-\\ntuning of LLMs\\nwith\\nadvanced\\nNVIDIA GPUs.\\nFine-tuning\\nand deployment\\nof\\nLLMs\\nwith\\nminimal code.\\nFine-tuning and\\ndeploying LLMs\\non AWS infras-\\ntructure.\\nSimplified\\nfine-\\ntuning and de-\\nployment within\\nthe AWS ecosys-\\ntem.\\nManual\\nfine-\\ntuning of LLMs\\nwith\\ndetailed\\ncontrol\\nover\\ntraining\\npro-\\ncesses.\\nModel Support\\nSupports a vari-\\nety of large, pre-\\ntrained\\nmodels,\\nincluding Mega-\\ntron series.\\nSupports a wide\\nrange\\nof\\npre-\\ntrained\\nmodels\\nfrom\\nthe\\nHug-\\nging Face model\\nhub.\\nSupports\\nvari-\\nous\\nLLMs\\nlike\\nAmazon\\nTitan\\nand\\nthird-party\\nmodels.\\nPre-trained\\nmodels\\nfrom\\nAWS and part-\\nners; integration\\nwith\\ncustom\\nmodels.\\nSupports a vast\\narray of models\\nfrom\\nthe\\nHug-\\nging Face model\\nhub.\\nData Handling\\nUsers\\nprovide\\ntask-specific\\ndata\\nfor\\nfine-\\ntuning,\\npro-\\ncessed\\nusing\\nNVIDIA’s\\nin-\\nfrastructure.\\nUploads\\ndatasets\\nvia\\na\\nsimple\\ninter-\\nface; AutoTrain\\nhandles\\npre-\\nprocessing\\nand\\nmodel training.\\nData is uploaded\\nand\\nmanaged\\nwithin the AWS\\nenvironment;\\nintegrates\\nwith\\nAWS data ser-\\nvices.\\nUploads\\nand\\nprocesses\\ndata\\nwithin\\nAWS;\\nsupports various\\ndata formats.\\nUsers\\nmanually\\npreprocess data\\nand\\nmanage\\ntraining steps.\\nCustomisation\\nLevel\\nHigh;\\nextensive\\ncontrol\\nover\\nfine-tuning pro-\\ncess and model\\nparameters.\\nModerate; auto-\\nmated\\nprocess\\nwith\\nsome\\ncustomisation\\noptions.\\nHigh;\\ndetailed\\nconfiguration\\nand\\nintegration\\nwith AWS ser-\\nvices.\\nModerate;\\npre-configured\\nsettings\\nwith\\nsome customisa-\\ntion available.\\nVery\\nHigh;\\ndetailed\\ncon-\\ntrol\\nover\\nevery\\naspect\\nof\\nfine-\\ntuning.\\nScalability\\nHigh;\\nleverages\\nNVIDIA’s GPU\\ncapabilities\\nfor\\nefficient scaling.\\nHigh;\\nscalable\\nvia\\nHugging\\nFace’s\\ncloud\\ninfrastructure.\\nVery\\nHigh;\\nscalable\\nacross\\nAWS’s extensive\\ncloud infrastruc-\\nture.\\nHigh;\\nscalable\\nwithin the AWS\\ncloud\\necosys-\\ntem.\\nHigh; scalability\\ndepends on the\\ninfrastructure\\nused (e.g., local\\nvs. cloud).\\nDeployment\\nOptions\\nOn-premises\\nor\\ncloud\\nde-\\nployment\\nvia\\nNVIDIA infras-\\ntructure.\\nDeployed\\nvia\\nHugging\\nFace’s\\ncloud or can be\\nexported for lo-\\ncal deployment.\\nIntegrated\\ninto\\nAWS\\nservices,\\neasily\\ndeploy-\\nable\\nacross\\nAWS’s\\nglobal\\ninfrastructure.\\nAWS\\ncloud\\ndeployment;\\nintegrates\\nwith\\nother AWS ser-\\nvices.\\nDeployable\\nlo-\\ncally,\\nin cloud,\\nor\\nexported\\nto\\nother platforms.\\nIntegration with\\nEcosystem\\nDeep integration\\nwith\\nNVIDIA\\ntools\\n(e.g.,\\nTensorRT)\\nand\\nGPU-based\\nworkflows.\\nIntegrates\\nwell\\nwith\\nthe\\nHugging\\nFace\\necosystem\\nand\\nother ML tools.\\nSeamless\\ninte-\\ngration\\nwith\\nAWS\\nser-\\nvices\\n(e.g.,\\nS3,\\nLambda,\\nSage-\\nMaker).\\nStrong\\nintegra-\\ntion with AWS\\nservices;\\neasy\\nto connect with\\ndata\\npipelines\\nand analytics.\\nIntegrates\\nwith\\nHugging\\nFace\\necosystem\\nand\\nother\\nPython-\\nbased ML tools.\\nData Privacy\\nUsers\\nmust\\nensure\\ndata\\nprivacy\\ncompli-\\nance;\\nNVIDIA\\nhandles\\ndata\\nduring\\nprocess-\\ning.\\nData\\nhandled\\nwithin\\nHugging\\nFace’s\\nenviron-\\nment;\\nprivacy\\ndepends\\non\\ndata\\nhandling\\npractices.\\nStrong\\nfocus\\non data privacy\\nwithin\\nAWS\\nenvironment;\\ncompliant\\nwith\\nvarious\\nstan-\\ndards.\\nStrong\\nAWS\\nprivacy\\nand\\nsecurity\\nmea-\\nsures; compliant\\nwith\\nindustry\\nstandards.\\nUser-managed;\\ndepends\\non\\nwhere the mod-\\nels and data are\\nhosted.\\nTarget Users\\nEnterprises and\\ndevelopers need-\\ning\\nadvanced\\ncustomisation\\nand\\nperfor-\\nmance in LLM\\nfine-tuning.\\nDevelopers\\nand\\nbusinesses look-\\ning\\nfor\\neasy,\\nautomated LLM\\nfine-tuning solu-\\ntions.\\nBusinesses\\nand\\ndevelopers inte-\\ngrated\\ninto\\nor\\nseeking to lever-\\nage AWS cloud\\nservices.\\nEnterprises and\\ndevelopers seek-\\ning\\nstreamlined\\nAI/ML solutions\\nwithin AWS.\\nResearchers,\\ndevelopers,\\nand\\nML\\nengineers\\nneeding detailed\\ncontrol\\nover\\ntraining.\\nLimitations\\nHigh\\nresource\\ndemand\\nand\\npotential\\ncosts;\\ndependency\\non\\nNVIDIA ecosys-\\ntem.\\nLess\\ncontrol\\nover fine-tuning\\nspecifics; cloud-\\nbased,\\nmay\\nnot suit all on-\\npremises needs.\\nDependency\\non\\nAWS;\\npo-\\ntential\\nvendor\\nlock-in,\\ncost\\nmanagement\\ncomplexity.\\nLimited\\nto\\nAWS\\nservices;\\npre-configured\\noptions\\nmay\\nlimit deep cus-\\ntomisation.\\nRequires\\ntech-\\nnical\\nexpertise;\\nmore\\ncomplex\\nsetup and man-\\nagement.\\nTable 10.1: Detailed Comparison of LLM Fine-Tuning Platforms (Part I). This table provides a compre-\\nhensive comparison of various fine-tuning tools for Large Language Models (LLMs), including NVIDIA\\nNeMo, Hugging Face AutoTrain API, Amazon Bedrock, AWS SageMaker JumpStart, and Hugging Face\\nTrainer API. It covers multiple aspects such as the primary use case, model support, data handling,\\ncustomisation level, scalability, deployment options, integration with the ecosystem, data privacy, target\\nusers, and limitations for each tool.\\n77\\nParameter\\nOpenAI\\nFine-\\nTuning API\\nGoogle Vertex AI\\nStudio\\nMicrosoft\\nAzure\\nAI Studio\\nLangChain\\nPrimary Use\\nCase\\nAPI-based\\nfine-\\ntuning\\nfor\\nOpenAI\\nmodels with custom\\ndatasets.\\nEnd-to-end\\nML\\nmodel\\ndevelopment\\nand\\ndeployment\\nwithin Google Cloud.\\nEnd-to-end AI devel-\\nopment,\\nfine-tuning,\\nand\\ndeployment\\non\\nAzure.\\nBuilding applications\\nusing\\nLLMs\\nwith\\nmodular\\nand\\ncus-\\ntomisable workflows.\\nModel Support\\nLimited\\nto\\nOpenAI\\nmodels\\nlike\\nGPT-3\\nand GPT-4.\\nSupports\\nGoogle’s\\npre-trained\\nmodels\\nand\\nuser-customised\\nmodels.\\nSupports Microsoft’s\\nmodels\\nand\\ncustom\\nmodels\\nfine-tuned\\nwithin Azure.\\nSupports integration\\nwith\\nvarious\\nLLMs\\nand\\nAI\\ntools\\n(e.g.,\\nOpenAI, GPT-4, Co-\\nhere).\\nData Handling\\nUsers upload datasets\\nvia\\nAPI;\\nOpenAI\\nhandles\\npreprocess-\\ning and fine-tuning.\\nData managed within\\nGoogle Cloud;\\nsup-\\nports\\nmultiple\\ndata\\nformats.\\nData\\nintegrated\\nwithin Azure ecosys-\\ntem; supports various\\nformats and sources.\\nData handling is flex-\\nible,\\ndependent\\non\\nthe specific LLM and\\nintegration used.\\nCustomisation\\nLevel\\nModerate; focuses on\\nease of use with lim-\\nited deep customisa-\\ntion.\\nHigh;\\noffers custom\\nmodel\\ntraining\\nand\\ndeployment with de-\\ntailed configuration.\\nHigh; extensive cus-\\ntomisation\\noptions\\nthrough\\nAzure’s\\nAI\\ntools.\\nVery High; allows de-\\ntailed\\ncustomisation\\nof workflows, models,\\nand data processing.\\nScalability\\nHigh;\\nscalable\\nthrough\\nOpenAI’s\\ncloud infrastructure.\\nVery High; leverages\\nGoogle\\nCloud’s\\nin-\\nfrastructure for scal-\\ning.\\nVery High;\\nscalable\\nacross Azure’s global\\ninfrastructure.\\nHigh; scalability de-\\npends on the specific\\ninfrastructure\\nand\\nmodels used.\\nDeployment\\nOptions\\nDeployed via API, in-\\ntegrated into applica-\\ntions using OpenAI’s\\ncloud.\\nDeployed\\nwithin\\nGoogle\\nCloud;\\nin-\\ntegrates\\nwith\\nother\\nGCP services.\\nDeployed\\nwithin\\nAzure;\\nintegrates\\nwith Azure’s suite of\\nservices.\\nDeployed\\nwithin\\ncustom\\ninfrastruc-\\nture; integrates with\\nvarious\\ncloud\\nand\\non-premises services.\\nIntegration with\\nEcosystem\\nLimited\\nto\\nOpenAI\\necosystem; integrates\\nwell\\nwith\\napps\\nvia\\nAPI.\\nSeamless\\nintegration\\nwith\\nGoogle\\nCloud\\nservices\\n(e.g.,\\nBig-\\nQuery, AutoML).\\nDeep integration with\\nAzure’s services (e.g.,\\nData Factory, Power\\nBI).\\nFlexible\\nintegration\\nwith multiple tools,\\nAPIs,\\nand\\ndata\\nsources.\\nData Privacy\\nManaged by OpenAI;\\nusers\\nmust\\nconsider\\ndata transfer and pri-\\nvacy implications.\\nStrong\\nprivacy\\nand\\nsecurity\\nmeasures\\nwithin Google Cloud\\nenvironment.\\nStrong\\nprivacy\\nand\\nsecurity\\nmeasures\\nwithin\\nAzure\\nenvi-\\nronment.\\nDependent on the in-\\ntegrations and infras-\\ntructure used;\\nusers\\nmanage privacy.\\nTarget Users\\nDevelopers\\nand\\nen-\\nterprises\\nlooking\\nfor\\nstraightforward,\\nAPI-based\\nLLM\\nfine-tuning.\\nDevelopers and busi-\\nnesses integrated into\\nGoogle Cloud or seek-\\ning to leverage GCP.\\nEnterprises\\nand\\nde-\\nvelopers\\nintegrated\\ninto Azure or seeking\\nto\\nleverage\\nAzure’s\\nAI tools.\\nDevelopers\\nneeding\\nto\\nbuild\\ncomplex,\\nmodular\\nLLM-based\\napplications\\nwith\\ncustom workflows.\\nLimitations\\nLimited\\ncustomisa-\\ntion; dependency on\\nOpenAI’s infrastruc-\\nture; potential cost.\\nLimited\\nto\\nGoogle\\nCloud ecosystem; po-\\ntential cost and ven-\\ndor lock-in.\\nLimited\\nto\\nAzure\\necosystem;\\npotential\\ncost\\nand\\nvendor\\nlock-in.\\nComplexity in chain-\\ning multiple models\\nand data sources; re-\\nquires more setup.\\nTable 10.2: Detailed Comparison of LLM Fine-Tuning Platforms (Part II). This table continues the\\ncomparison of LLM fine-tuning tools, focusing on OpenAI Fine-Tuning API, Google Vertex AI Studio,\\nMicrosoft Azure AI Studio, and LangChain.\\nIt evaluates the tools based on the primary use case,\\nmodel support, data handling, customisation level, scalability, deployment options, integration with the\\necosystem, data privacy, target users, and limitations, offering a complete view of their capabilities and\\nconstraints.\\n10.1\\nAutotrain\\nAutotrain is HuggingFace’s innovative platform that automates the fine-tuning of large language models,\\nmaking it accessible even to those with limited machine learning expertise. The complexity and resource\\ndemands of fine-tuning LLMs can be daunting, but Autotrain simplifies the process by handling the most\\nchallenging aspects, such as data preparation, model configuration, and hyperparameter optimisation.\\nThis automation is particularly valuable for small teams or individual developers who need to deploy\\ncustom LLMs quickly and efficiently.\\n10.1.1\\nSteps Involved in Fine-Tuning Using Autotrain\\nFollowing are the steps involved in fine-tuning LLMs using Autotrain. Figure 10.1 represents the visual\\nworkflow.\\n• Dataset Upload and Model Selection:\\n78\\nFigure 10.1: Overview of the Autotrain Workflow. This diagram illustrates the step-by-step process\\nwithin the Autotrain system, beginning with the upload of datasets and model selection by users. The\\nworkflow then moves to data preparation and model configuration, followed by automated hyperpa-\\nrameter tuning to optimise model performance. The fine-tuning phase adjusts the model based on the\\nprovided datasets, culminating in the deployment of the fully fine-tuned model for practical use.\\n– Users begin by uploading their datasets to the Autotrain platform.\\n– They then select a pre-trained model from the extensive HuggingFace Model Hub.\\n• Data Preparation:\\n– Autotrain automatically processes the uploaded data, including tasks like tokenization to\\nconvert text into a format the LLM can understand.\\n• Model Configuration:\\n– The platform configures the model for fine-tuning, setting up the training environment and\\nnecessary parameters.\\n• Automated Hyperparameter Tuning:\\n– Autotrain explores various hyperparameter configurations (such as learning rate, batch size,\\nand sequence length) and selects the best-performing ones.\\n• Fine-Tuning:\\n– The model is fine-tuned on the prepared data with the optimised hyperparameters.\\n• Deployment:\\n– Once fine-tuning is complete, the model is ready for deployment in various NLP applications,\\nsuch as text generation, completion, and language translation.\\n79\\n10.1.2\\nBest Practices of Using Autotrain\\n• Data Quality: Ensure high-quality, well-labelled data for better model performance.\\n• Model Selection: Choose pre-trained models that are well-suited to your specific task to minimize\\nfine-tuning effort.\\n• Hyperparameter Optimisation: Leverage Autotrain’s automated hyperparameter tuning to\\nachieve optimal performance without manual intervention.\\n10.1.3\\nChallenges of Using Autotrain\\n• Data Privacy: Ensuring the privacy and security of sensitive data during the fine-tuning process.\\n• Resource Constraints: Managing computational resources effectively, especially in environments\\nwith limited access to powerful hardware.\\n• Model Overfitting: Avoiding overfitting by ensuring diverse and representative training data\\nand using appropriate regularization techniques.\\n10.1.4\\nWhen to Use Autotrain\\n1. Lack of Deep Technical Expertise: Ideal for individuals or small teams without extensive\\nmachine learning or LLM background who need to fine-tune models quickly and effectively.\\n2. Quick Prototyping and Deployment: Suitable for rapid development cycles where time is\\ncritical, such as proof-of-concept projects or MVPs.\\n3. Resource-Constrained Environments: Useful for scenarios with limited computational re-\\nsources or where a quick turnaround is necessary.\\nIn summary, Autotrain is an excellent tool for quick, user-friendly fine-tuning of LLMs for standard NLP\\ntasks, especially in environments with limited resources or expertise. However, it may not be suitable\\nfor highly specialised applications or those requiring significant customisation and scalability.\\n10.1.5\\nTutorials\\n1. How To Create HuggingFace Custom AI Models Using AutoTrain\\n2. Finetune models with HuggingFace AutoTrain\\n10.2\\nTransformers Library and Trainer API\\nThe Transformers Library by HuggingFace stands out as a pivotal tool for fine-tuning large language\\nmodels (LLMs) such as BERT, GPT-3, and GPT-4. This comprehensive library offers a wide array of\\npre-trained models tailored for various LLM tasks, making it easier for users to adapt these models to\\nspecific needs with minimal effort. Whether you’re fine-tuning for tasks like sentiment analysis, text\\nclassification, or generating customer support responses, the library simplifies the process by allowing\\nseamless model selection from the HuggingFace Model Hub and straightforward customisation through\\nits high-level APIs.\\nCentral to the fine-tuning process within the Transformers Library is the Trainer API. This API includes\\nthe Trainer class, which automates and manages the complexities of fine-tuning LLMs. After completing\\ndata preprocessing, the Trainer class streamlines the setup for model training, including data handling,\\noptimisation, and evaluation. Users only need to configure a few parameters, such as learning rate and\\nbatch size, and the API takes care of the rest. However, it’s crucial to note that running Trainer.train()\\ncan be resource-intensive and slow on a CPU. For efficient training, a GPU or TPU is recommended.\\nPlatforms like Google Colab provide free access to these resources, making it feasible for users without\\nhigh-end hardware to fine-tune models effectively.\\n80\\nThe Trainer API also supports advanced features like distributed training and mixed precision, which\\nare essential for handling the large-scale computations required by modern LLMs. Distributed training\\nallows the fine-tuning process to be scaled across multiple GPUs or nodes, significantly reducing training\\ntime. Mixed precision training, on the other hand, optimises memory usage and computation speed by\\nusing lower precision arithmetic without compromising model performance. HuggingFace’s dedication to\\naccessibility is evident in the extensive documentation and community support they offer, enabling users\\nof all expertise levels to fine-tune LLMs. This democratisation of advanced NLP technology empowers\\ndevelopers and researchers to deploy sophisticated, fine-tuned models for a wide range of applications,\\nfrom specialised language understanding to large-scale data processing.\\n10.2.1\\nLimitations of the Transformers Library and Trainer API\\n• Limited Customisation for Advanced Users: While the Trainer API simplifies many aspects\\nof training, it might not offer the deep customisation that advanced users or researchers might need\\nfor novel or highly specialised applications.\\n• Learning Curve: Despite the simplified API, there is still a learning curve associated with un-\\nderstanding and effectively using the Transformers Library and Trainer API, particularly for those\\nnew to NLP and LLM.\\n• Integration Limitations: The seamless integration and ease of use are often tied to the Hug-\\ngingFace ecosystem, which might not be compatible with all workflows or platforms outside their\\nenvironment.\\nIn summary, the Transformers Library and Trainer API provide robust, scalable solutions for fine-tuning\\nLLMs across a range of applications, offering ease of use and efficient training capabilities. However, users\\nmust be mindful of the resource requirements and potential limitations in customisation and complexity\\nmanagement.\\n10.3\\nOptimum: Enhancing LLM Deployment Efficiency\\nOptimum6 is HuggingFace’s tool designed to optimise the deployment of large language models (LLMs)\\nby enhancing their efficiency across various hardware platforms. As LLMs grow in size and complexity,\\ndeploying them in a cost-effective and performant manner becomes increasingly challenging. Optimum\\naddresses these challenges by applying a range of hardware-specific optimisations, such as quantisation,\\npruning, and model distillation, which reduce the model’s size and improve inference speed without\\nsignificantly affecting accuracy. The following are the key techniques supported by Optimum:\\n• Quantisation: Quantisation is one of the key techniques supported by Optimum. This process in-\\nvolves converting the model’s weights from high-precision floating-point numbers to lower-precision\\nformats, such as int8 or float16. This reduction in precision decreases the model’s memory foot-\\nprint and computational requirements, enabling faster execution and lower power consumption,\\nespecially on edge devices and mobile platforms. Optimum automates the quantisation process,\\nmaking it accessible to users who may not have expertise in low-level hardware optimisation.\\n• Pruning: Pruning is another critical optimisation strategy offered by Optimum. It involves iden-\\ntifying and removing less significant weights from the LLM, reducing its overall complexity and\\nsize. This leads to faster inference times and lower storage needs, which are particularly beneficial\\nfor deploying models in environments with limited computational resources. Optimum’s pruning\\nalgorithms carefully eliminate these redundant weights while maintaining the model’s performance,\\nensuring that it continues to deliver high-quality results even after optimisation.\\n• Model Distillation: In addition to these techniques, Optimum supports model distillation, a\\nprocess where a smaller, more efficient model is trained to replicate the behaviour of a larger, more\\ncomplex model. This distilled model retains much of the knowledge and capabilities of the original\\nwhile being significantly lighter and faster. Optimum provides tools to facilitate the distillation\\nprocess, allowing users to create compact LLMs that are well-suited for real-time applications. By\\noffering a comprehensive suite of optimisation tools, Optimum ensures that HuggingFace’s LLMs\\ncan be deployed effectively across a wide range of environments, from powerful cloud servers to\\nresource-constrained edge devices.\\n6https://huggingface.co/docs/optimum/en/index\\n81\\n10.3.1\\nBest Practices of Using Optimum\\n• Understand Hardware Requirements: Assess the target deployment environment (e.g., edge\\ndevices, cloud servers) to optimise model configuration accordingly.\\n• Iterative Optimisation: Experiment with different optimisation techniques (quantisation levels,\\npruning thresholds) to find the optimal balance between model size, speed, and accuracy.\\n• Validation and Testing: Validate optimised models thoroughly to ensure they meet performance\\nand accuracy requirements across different use cases.\\n• Documentation and Support: Refer to HuggingFace’s resources for detailed guidance on using\\nOptimum’s tools effectively, and leverage community support for troubleshooting and best practices\\nsharing.\\n• Continuous Monitoring: Monitor deployed models post-optimisation to detect any performance\\ndegradation and adjust optimisation strategies as needed to maintain optimal performance over\\ntime.\\n10.3.2\\nTutorials\\n1. An Introduction to Using Transformers and Hugging Face\\n10.4\\nAmazon SageMaker JumpStart\\nAmazon SageMaker JumpStart is a feature within the SageMaker ecosystem designed to simplify and\\nexpedite the fine-tuning of large language models (LLMs). It provides users with a rich library of pre-\\nbuilt models and solutions that can be quickly customised for various use cases. This tool is particularly\\nvaluable for organisations looking to deploy NLP solutions efficiently without deep expertise in machine\\nlearning or the extensive computational resources typically required for training LLMs from scratch. The\\narchitecture depicted in Figure 10.2 outlines a comprehensive pipeline for the fine-tuning and deployment\\nof large language models (LLMs) Utilising AWS services.\\n10.4.1\\nSteps Involved in Using JumpStart\\n• Data Preparation and Preprocessing:\\n– Data Storage: Begin by securely storing raw datasets in Amazon S3, AWS’s scalable object\\nstorage service.\\n– Preprocessing: Utilise the EMR Serverless framework with Apache Spark for efficient data\\npreprocessing. This step refines and prepares the raw data for subsequent model training and\\nevaluation.\\n– Data Refinement: Store the processed dataset back into Amazon S3 after preprocessing,\\nensuring accessibility and readiness for the next stages.\\n• Model Fine-Tuning with SageMaker JumpStart:\\n– Model Selection: Choose from a variety of pre-built models and solutions available through\\nSageMaker JumpStart’s extensive library, tailored for tasks such as sentiment analysis, text\\ngeneration, or customer support automation.\\n– Fine-Tuning Execution: Utilise Amazon SageMaker’s capabilities, integrated with Sage-\\nMaker JumpStart, to fine-tune the selected model. This involves adjusting parameters and\\nconfigurations to optimise the model’s performance for specific use cases.\\n– Workflow Simplification: Leverage pre-built algorithms and model templates provided by\\nSageMaker JumpStart to streamline the fine-tuning workflow, reducing the time and effort\\nrequired for deployment.\\n• Model Deployment and Hosting:\\n82\\nFigure 10.2: A step-by-step workflow illustrating the Amazon SageMaker JumpStart process, starting\\nfrom data preprocessing using EMR Serverless Spark to the fine-tuning of LLMs, and ending with model\\ndeployment on Amazon SageMaker Endpoints. (adapted from [85])\\n– Deployment Setup: Deploy the fine-tuned model using Amazon SageMaker’s endpoint\\ndeployment capabilities. This setup ensures that the model is hosted in a scalable environment\\ncapable of handling real-time predictions efficiently.\\n– Scalability: Benefit from AWS’s infrastructure scalability, allowing seamless scaling of re-\\nsources to accommodate varying workloads and operational demands.\\n– Efficiency and Accessibility: Ensure that the deployed model is accessible via SageMaker\\nendpoints, enabling efficient integration into production applications for real-time inference\\ntasks.\\n10.4.2\\nBest Practices for Using JumpStart\\n• Robust Data Management: Maintain secure and organised data storage practices in Amazon\\nS3, facilitating efficient data access and management throughout the pipeline.\\n• Cost-Effective Processing: Utilise serverless computing frameworks like EMR Serverless with\\nApache Spark for cost-effective and scalable data preprocessing.\\n• Optimised Fine-Tuning: Capitalise on SageMaker JumpStart’s pre-built models and algorithms\\nto expedite and optimise the fine-tuning process, ensuring optimal model performance without\\n83\\nextensive manual configuration.\\n• Continuous Monitoring and Optimisation: Implement robust monitoring mechanisms post-\\ndeployment to track model performance metrics. This allows for timely optimisations and adjust-\\nments to maintain accuracy and efficiency over time.\\n• Integration with AWS Services: Leverage AWS’s comprehensive suite of services and inte-\\ngration capabilities to create end-to-end pipelines that ensure reliable and scalable deployment of\\nlarge-scale language models across diverse operational environments.\\n10.4.3\\nLimitations of Using JumpStart\\n• Limited Customisation: While JumpStart simplifies the process for common use cases, it may\\noffer limited flexibility for highly specialised or complex applications that require significant cus-\\ntomisation beyond the provided templates and workflows.\\n• Dependency on AWS Ecosystem: JumpStart is tightly integrated with AWS services, which\\nmay pose challenges for users who prefer or need to operate in multi-cloud environments or those\\nwith existing infrastructure outside of AWS.\\n• Resource Costs: Utilising SageMaker’s scalable resources for fine-tuning LLMs, especially large\\nmodels, can incur substantial costs, which might be a barrier for smaller organisations or those\\nwith limited budgets.\\n10.4.4\\nTutorials\\n1. Fine-Tuning LLaMA 2 with Amazon SageMaker JumpStart\\n2. LLM Agents Using AWS SageMaker JumpStart Foundation Models\\n10.5\\nAmazon Bedrock\\nAmazon Bedrock7 is a fully managed service designed to simplify access to high-performing foundation\\nmodels (FMs) from top AI innovators like AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability\\nAI, and Amazon. It provides a unified API that integrates these models and offers extensive capabilities\\nfor developing secure, private, and responsible generative AI applications. With Amazon Bedrock, users\\ncan effortlessly experiment with and assess leading FMs tailored to their specific needs. The service sup-\\nports private customisation of models through fine-tuning and Retrieval Augmented Generation (RAG),\\nenabling the creation of intelligent agents that leverage enterprise data and systems. Amazon Bedrock’s\\nserverless architecture allows for quick deployment, seamless integration, and secure customisation of\\nFMs without the burden of infrastructure management, Utilising AWS tools to deploy these models into\\napplications efficiently and securely.\\n10.5.1\\nSteps Involved in Using Amazon Bedrock\\nAmazon Bedrock offers a streamlined workflow for deploying and fine-tuning LLMs, making it an ideal\\nchoice for businesses looking to quickly integrate advanced AI capabilities into their operations. Here’s\\na high-level overview of how Bedrock operates:\\n• Model Selection: Users start by choosing from a curated selection of foundation models available\\nthrough Bedrock. These include models from AWS (like Amazon Titan) and third-party providers\\n(such as Anthropic Claude and Stability AI).\\n• Fine-Tuning:\\n– Once a model is selected, users can fine-tune it to better fit their specific needs. This involves\\nfeeding the model with domain-specific data or task-specific instructions to tailor its outputs.\\n7https://aws.amazon.com/bedrock/\\n84\\n– The fine-tuning process is handled via simple API calls, eliminating the need for extensive\\nsetup or detailed configuration. Users provide their custom data, and Bedrock manages the\\ntraining process in the background.\\n• Deployment:\\n– After fine-tuning, Bedrock takes care of deploying the model in a scalable and efficient manner.\\nThis means that users can quickly integrate the fine-tuned model into their applications or\\nservices.\\n– Bedrock ensures that the model scales according to demand and handles performance optimi-\\nsation, providing a seamless user experience.\\n• Integration and Monitoring:\\n– Bedrock integrates smoothly with other AWS services, allowing users to embed AI capabilities\\ndirectly into their existing AWS ecosystem.\\n– Users can monitor and manage the performance of their deployed models through AWS’s\\ncomprehensive monitoring tools, ensuring that the models continue to perform optimally.\\n10.5.2\\nLimitations of Using Amazon Bedrock\\nWhile Amazon Bedrock offers a robust suite of tools and services for addressing certain AI challenges,\\nit is not a comprehensive solution for all AI needs. One key limitation is that it does not eliminate the\\nrequirement for human expertise. Organisations still need skilled professionals who understand the in-\\ntricacies of AI technology to effectively develop, fine-tune, and optimise the models provided by Bedrock.\\nAdditionally, Amazon Bedrock is not designed to function as a standalone service. It relies on integration\\nwith other AWS services, such as Amazon S3 for data storage, AWS Lambda for serverless computing,\\nand AWS SageMaker for machine learning model development. Therefore, businesses leveraging Amazon\\nBedrock will also need to use these complementary AWS services to fully realise its potential. This\\ninterconnectedness means that while Amazon Bedrock enhances the AI capabilities within an AWS\\necosystem, it may present a steep learning curve and require significant infrastructure management for\\nthose new to AWS.\\n10.5.3\\nTutorials\\n1. Finetuning LLMs on Amazon Bedrock\\n2. Amazon Bedrock for Generative AI\\n10.6\\nOpenAI’s Fine-Tuning API\\nOpenAI’s Fine-Tuning API is a comprehensive platform that facilitates the customisation of OpenAI’s\\npre-trained LLMs to cater to specific tasks and domains. This service is designed to be user-friendly,\\nenabling a broad range of users, from businesses to individual developers, to harness the power of\\nadvanced AI without the complexities typically associated with model training and deployment.\\n10.6.1\\nSteps Involved in Using OpenAI’s Fine-Tuning API\\n• Model Selection:\\n– Choosing a Pre-Trained Model: Users begin by selecting a base model from OpenAI’s\\nextensive lineup. This includes powerful models like GPT-4, which offer a robust starting\\npoint for a wide range of language processing tasks.\\n– Customisable Base: These models come pre-trained with vast amounts of data, providing\\na solid foundation that can be further refined to suit specific requirements.\\n• Data Preparation and Upload:\\n85\\n– Curating Relevant Data: Users need to gather and prepare a dataset that reflects the\\nspecific task or domain they wish to fine-tune the model for. This data is crucial for teaching\\nthe model to perform the desired function more effectively.\\n– Uploading Data to the API: The Fine-Tuning API facilitates easy data upload. Users\\ncan feed their curated datasets into the API through straightforward commands, making the\\nprocess accessible even to those with limited technical backgrounds.\\n• Initiating Fine-Tuning:\\n– Automated Process: Once the data is uploaded, OpenAI’s infrastructure handles the fine-\\ntuning process. The API adjusts the model’s parameters based on the new data to improve\\nperformance on the specified tasks.\\n• Deploying the Fine-Tuned Model:\\n– API Integration: The fine-tuned model can be accessed and deployed via OpenAI’s API.\\nThis allows for seamless integration into various applications, such as chatbots, automated\\ncontent creation tools, or specialised customer service systems.\\n10.6.2\\nLimitations of OpenAI’s Fine-Tuning API\\n• Pricing Models: Fine-tuning and using OpenAI’s models through the API can be costly, espe-\\ncially for large-scale deployments or continuous usage. This can be a significant consideration for\\nsmaller organisations or budget-constrained projects.\\n• Data Privacy and Security: Users must upload their data to OpenAI’s servers for the fine-\\ntuning process. This raises potential concerns about data privacy and the security of sensitive or\\nproprietary information.\\n• Dependency on OpenAI Infrastructure: The reliance on OpenAI’s infrastructure for model\\nhosting and API access can lead to vendor lock-in, limiting flexibility and control over the deploy-\\nment environment.\\n• Limited Control Over Training Process: The fine-tuning process is largely automated and\\nmanaged by OpenAI, offering limited visibility and control over the specific adjustments made to\\nthe model.\\n10.6.3\\nTutorials\\n1. Fine-Tuning GPT-3 Using the OpenAI API\\n10.7\\nNVIDIA NeMo Customizer\\nNVIDIA NeMo Customiser8 is part of the NeMo framework, a suite of tools and models designed by\\nNVIDIA to facilitate the development and fine-tuning of LLM models. The Customiser focuses specifi-\\ncally on making it easier to fine-tune large language models (LLMs) for specialised tasks and domains.\\nLike other fine-tuning tools, NeMo Customiser is geared toward users who want to adapt pre-trained\\nmodels for specific applications, such as conversational AI, translation, or domain-specific text gener-\\nation. It delivers enterprise-ready models by offering accurate data curation, extensive customisation\\noptions, retrieval-augmented generation (RAG), and improved performance features. The platform sup-\\nports training and deploying generative AI models across diverse environments, including cloud, data\\ncenter, and edge locations. It provides a comprehensive package with support, security, and reliable APIs\\nas part of the NVIDIA AI Enterprise.\\n8https://developer.nvidia.com/blog/fine-tune-and-align-llms-easily-with-nvidia-nemo-customizer/\\n86\\n10.7.1\\nKey Features of NVIDIA NeMo\\nNVIDIA NeMo is designed to enhance AI projects with several standout features.[86]\\n• State-of-the-Art Training Techniques NeMo employs GPU-accelerated tools like NeMo Cu-\\nrator for preparing large-scale, high-quality datasets. These tools facilitate efficient pretraining of\\ngenerative AI models by leveraging thousands of compute cores, which significantly reduces training\\ntime and enhances the accuracy of large language models (LLMs).\\n• Advanced Customisation for LLMs The NeMo Customiser microservice allows for precise fine-\\ntuning and alignment of LLMs for specific domains. It uses model parallelism to speed up training\\nand supports scaling across multiple GPUs and nodes, enabling the fine-tuning of larger models.\\n• Optimised AI Inference with NVIDIA Triton NeMo includes NVIDIA Triton Inference Server\\nto streamline AI inference at scale. This integration accelerates generative AI inference, ensuring\\nconfident deployment of AI applications both on-premises and in the cloud.\\n• User-Friendly Tools for Generative AI NeMo features a modular, reusable architecture that\\nsimplifies the development of conversational AI models. It supports comprehensive workflows from\\ndata processing to deployment and includes pre-trained models for automatic speech recognition\\n(ASR), natural language processing (NLP), and text-to-speech (TTS), which can be fine-tuned or\\nused as-is.\\n• Best-in-Class Pretrained Models NeMo Collections offer a variety of pre-trained models and\\ntraining scripts, facilitating rapid application development or fine-tuning for specific tasks. Cur-\\nrently, NeMo supports models like Llama 2, Stable Diffusion, and NVIDIA’s Nemotron-3 8B family.\\n• Optimised Retrieval-Augmented Generation NeMo Retriever delivers high-performance, low-\\nlatency information retrieval, enhancing generative AI applications with enterprise-grade retrieval-\\naugmented generation (RAG) capabilities. This feature supports real-time business insights and\\ndata Utilisation.\\n10.7.2\\nComponents of NVIDIA NeMo\\n• NeMo Core Provides essential elements like the Neural Module Factory for training and inference,\\nstreamlining the development of conversational AI models.\\n• NeMo Collections Offers specialised modules and models for ASR, NLP, and TTS, including\\npre-trained models and training scripts, making the platform versatile.\\n• Neural Modules Serve as the building blocks of NeMo, defining trainable components such as\\nencoders and decoders, which can be connected to create comprehensive models.\\n• Application Scripts Simplify the deployment of conversational AI models with ready-to-use\\nscripts, enabling quick training or fine-tuning on specific datasets for various AI applications.\\n10.7.3\\nCustomising Large Language Models (LLMs)\\nWhile general-purpose LLMs, enhanced with prompt engineering or light fine-tuning, have enabled organ-\\nisations to achieve successful proof-of-concept projects, transitioning to production presents additional\\nchallenges.\\nFigure 10.3 illustrates NVIDIA’s detailed LLM customisation lifecycle, offering valuable\\nguidance for organisations that are preparing to deploy customised models in a production environment\\n[87].\\n1. Model Selection or Development\\nNVIDIA provides a range of pre-trained models, from 8B to 43B parameters, and supports the\\nintegration of other open-source models of any size. Alternatively, users can develop their own\\nmodels, starting with data curation, which includes selecting, labeling, cleansing, validating, and\\nintegrating data. This process, better termed data engineering, involves additional analysis, de-\\nsigning storage, evaluating model training results, and incorporating reinforcement learning with\\nhuman feedback (RLHF). While building a custom foundation model is often costly, complex, and\\ntime-consuming, most enterprises opt to start with a pre-trained model and focus on customisation.\\n87\\nFigure 10.3: Nvidia NeMo Framework for Customising and Deploying LLMs. The Nvidia NeMo frame-\\nwork is designed for end-to-end customisation and deployment of large language models (LLMs). This\\ndiagram illustrates the process from data curation and distributed training of foundation models, through\\nmodel customisation, to accelerated inference with guardrails. The platform enables AI developers to\\nintegrate in-domain, secure, and cited responses into enterprise applications, ensuring that LLMs are\\neffectively tailored for specific tasks and industries. The NeMo framework, supported by Nvidia AI En-\\nterprise, also offers robust support for various pre-trained foundation models like OpenAI’s GPT family,\\nensuring scalability and reliability in AI deployments. (adapted from [87])\\n2. Model Customisation\\nModel customisation involves optimising performance with task-specific datasets and adjusting\\nmodel weights. NeMo offers recipes for customisation, and enterprises can choose models already\\ntailored to specific tasks and then fine-tune them with proprietary data.\\n3. Inference\\nInference refers to running models based on user queries. This phase involves considering hardware,\\narchitecture, and performance factors that significantly impact usability and cost in production.\\n4. Guardrails\\nNVIDIA employs guardrails as intermediary services between models and applications.\\nThese\\nservices review incoming prompts for policy compliance, execute arbitration or orchestration steps,\\nand ensure model responses adhere to policies. Guardrails help maintain relevance, accuracy, safety,\\nprivacy, and security.\\n5. Applications\\nNVIDIA’s framework presents enterprise applications as LLM-ready, though this is not always\\nthe case.\\nExisting applications may be connected to LLMs to enable new features.\\nHowever,\\ncreating assistants for knowledge access or task execution often involves designing new applications\\nspecifically for natural language interfaces.\\n10.7.4\\nTutorials\\n1. Introduction to NVIDIA NeMo — Tutorial and Example\\n2. How to fine-tune a Riva NMT Bilingual model with Nvidia NeMo\\n88\\nChapter 11\\nMultimodal LLMs and their\\nFine-tuning\\nA multimodal model is a machine learning model that can process information from various modalities,\\nsuch as images, videos, and text. For instance, Google’s multimodal model, Gemini[88], can analyse a\\nphoto of a plate of cookies and produce a written recipe in response, and it can perform the reverse as well.\\nThe difference between Generative AI and Multimodal AI is that generative AI refers to the use of\\nmachine learning models to create new content, such as text, images, music, audio, and videos, typically\\nfrom a single type of input. Multimodal AI extends these generative capabilities by processing informa-\\ntion from multiple modalities, including images, videos, and text. This enables the AI to understand\\nand interpret different sensory modes, allowing users to input various types of data and receive a diverse\\nrange of content types in return.\\nFigure 11.1: Timeline of Multimodal Model Developments — This figure illustrates the progression\\nof significant multimodal models, highlighting key releases from major tech companies and research\\ninstitutions from December 2023 to March 2024. The timeline showcases models like Google’s TinyGPT-\\nV and Gemini Nano, along with other innovations such as MoE-LLAVA, DeepSeek-VL, and LLAVA-\\nGemma, indicating the rapid advancement in multimodal AI technologies (adapted from [89]).\\n89\\n11.1\\nVision Language Model (VLMs)\\nVision language models encompass multimodal models capable of learning from both images and text\\ninputs. They belong to the category of generative models that utilise image and text data to produce\\ntextual outputs. These models, especially at larger scales, demonstrate strong zero-shot capabilities,\\nexhibit robust generalisation across various tasks, and effectively handle diverse types of visual data such\\nas documents and web pages. Typical applications include conversational interactions involving images,\\nimage interpretation based on textual instructions, answering questions related to visual content, under-\\nstanding documents, generating captions for images, and more. Certain advanced vision language models\\ncan also understand spatial attributes within images. They can generate bounding boxes or segmentation\\nmasks upon request to identify or isolate specific subjects, localise entities within images, or respond to\\nqueries regarding their relative or absolute positions. The landscape of large vision language models is\\ncharacterised by considerable diversity in training data, image encoding techniques, and consequently,\\ntheir functional capabilities.\\n11.1.1\\nArchitecture\\nVision-language models adeptly integrate both visual and textual information, leveraging three funda-\\nmental components:\\n• Image Encoder: This component translates visual data (images) into a format that the model\\ncan process.\\n• Text Encoder: Similar to the image encoder, this component converts textual data (words and\\nsentences) into a format the model can understand.\\n• Fusion Strategy: This component combines the information from both the image and text en-\\ncoders, merging the two data types into a unified representation.\\nThese elements work collaboratively, with the model’s learning process (loss functions) specifically tai-\\nlored to the architecture and learning strategy employed. Although the concept of vision-language mod-\\nels is not new, their construction has evolved significantly. Early models used manually crafted image\\ndescriptions and pre-trained word vectors. Modern models, however, utilise transformers—an advanced\\nneural network architecture—for both image and text encoding. These encoders can learn features either\\nindependently or jointly.\\nA crucial aspect of these models is pre-training. Before being applied to specific tasks, the models are\\ntrained on extensive datasets using carefully selected objectives. This pre-training equips them with the\\nfoundational knowledge required to excel in various downstream applications. Following is one of the\\nexample architectures of VLMs.\\n11.1.2\\nContrastive Learning\\nContrastive learning is a technique that focuses on understanding the differences between data points. It\\ncomputes a similarity score between instances and aims to minimise contrastive loss, making it particu-\\nlarly useful in semi-supervised learning where a limited number of labelled samples guide the optimisation\\nprocess to classify unseen data points.\\nHow it works\\nFor instance, to recognise a cat, contrastive learning compares a cat image with a similar cat image and\\na dog image. The model learns to distinguish between a cat and a dog by identifying features such as\\nfacial structure, body size, and fur. By determining which image is closer to the ”anchor” image, the\\nmodel predicts its class.\\nCLIP is a model that utilises contrastive learning to compute similarity between text and image embed-\\ndings through textual and visual encoders. It follows a three-step process for zero-shot predictions:\\n• Pre-training: Trains a text and image encoder to learn image-text pairs.\\n• Caption Conversion: Converts training dataset classes into captions.\\n• Zero-Shot Prediction: Estimates the best caption for a given input image based on learned\\nsimilarities.\\n90\\nFigure 11.2: Workflow of Contrastive Pre-Training for Multimodal Models. This figure illustrates the\\nprocess of contrastive pre-training where text and image encoders are trained to align representations\\nfrom both modalities. Step 1 involves contrastive pre-training by pairing text and image data, while\\nStep 2 showcases the creation of a dataset classifier using label text encoded by the text encoder. Step\\n3 demonstrates the model’s application for zero-shot prediction by leveraging the pre-trained text and\\nimage encoders. This method enables the model to generalise across various tasks without requiring\\ntask-specific fine-tuning (adopted from [90]).\\n11.2\\nFine-tuning of multimodal models\\nFor fine-tuning a Multimodal Large Language Model (MLLM), PEFT techniques such as LoRA and\\nQLoRA can be utilised. The process of fine-tuning for multimodal applications is analogous to that for\\nlarge language models, with the primary difference being the nature of the input data. In addition to\\nLoRA, which employs matrix factorisation techniques to reduce the number of parameters, other tools\\nsuch as LLM-Adapters and (IA)³[91] can be effectively used. LLM-Adapters integrate various adapter\\nmodules into the pre-trained model’s architecture, enabling parameter-efficient fine-tuning for diverse\\ntasks by updating only the adapter parameters while keeping the base model parameters fixed. (IA)³,\\nor Infused Adapters by Inhibiting and Amplifying Inner Activations, enhances performance by learn-\\ning vectors to weight model parameters through activation multiplications, supporting robust few-shot\\nperformance and task mixing without manual adjustments. Moreover, dynamic adaptation techniques\\nlike DyLoRA[92] allow for the training of low-rank adaptation blocks across different ranks, optimising\\nthe learning process by sorting the representations during training. LoRA-FA[93], a variant of LoRA,\\noptimises the fine-tuning process by freezing the first low-rank matrix after initialisation and using it as a\\nrandom projection while training the other, thereby reducing the number of parameters by half without\\ncompromising performance.\\nThe Efficient Attention Skipping (EAS)[94] module introduces a novel parameter and computation-\\nefficient tuning method for MLLMs, aiming to maintain high performance while reducing parameter and\\ncomputation costs for downstream tasks. However, MemVP[95] critiques this approach, noting that it\\nstill increases the input length of language models. To address this, MemVP integrates visual prompts\\nwith the weights of Feed Forward Networks, thereby injecting visual knowledge to decrease training time\\nand inference latency, ultimately outperforming previous PEFT methods.\\n11.2.1\\nFull-parameter Fine-Tuning\\nMethods such as those introduced by LOMO[96] and MeZO[97] provide alternative solutions by focusing\\non memory efficiency.\\nLOMO utilises a low-memory optimisation technique derived from Stochastic\\nGradient Descent (SGD), reducing memory consumption typically associated with the ADAM optimiser.\\nMeZO, on the other hand, offers a memory-efficient optimiser that requires only two forward passes\\nto compute gradients, enabling comprehensive fine-tuning of large models with a memory footprint\\nequivalent to inference [89].\\n91\\n11.2.2\\nCase study of fine-tuning MLLMs for Medical domain\\nThe following section provides a case study on fine-tuning MLLMs for the Visual Question Answering\\n(VQA) task. In this example, we present a PEFT for fine-tuning MLLM specifically designed for Med-\\nVQA applications. To ensure accurate performance measurement, human evaluations were conducted,\\ndemonstrating that the model achieves an overall accuracy of 81.9% and surpasses the GPT-4v model\\nby a substantial margin of 26% in absolute accuracy on closed-ended questions.\\nThe model consists of three components: the vision encoder, a pre-trained Large Language Model (LLM)\\nfor handling multimodal inputs and generating responses, and a single linear layer for projecting embed-\\ndings from the visual encoding space to the LLM space, as shown in figure 11.3.\\nThe Vision Transformer (ViT) type backbone, EVA, encodes image tokens into visual embeddings,\\nwith model weights remaining frozen during the fine-tuning process. The technique from MiniGPT-v2\\nis utilised, grouping four consecutive tokens into one visual embedding to efficiently reduce resource\\nconsumption by concatenating on the embedding dimension.\\nThese grouped visual tokens are then processed through the projection layer, resulting in embeddings\\n(length 4096) in the LLM space. A multimodal prompt template integrates both visual and question\\ninformation, which is input into the pre-trained LLM, LLaMA2-chat(7B), for answer generation. The\\nlow-rank adaptation (LoRA) technique is applied for efficient fine-tuning, keeping the rest of the LLM\\nfrozen during downstream fine-tuning. A beam search with a width of 1 is utilised.\\nFigure 11.3: Overview of Med VQA architecture integrating LoRA and a pre-trained LLM with a Vision\\nEncoder for medical visual question answering tasks. The architecture includes stages for processing\\nimages and generating contextually relevant responses, demonstrating the integration of vision and lan-\\nguage models in a medical setting (adopted from [98]).\\nThe multimodal prompt includes input images, questions, and a specific token for VQA tasks, following\\nthe MiniGPT-v2 template. In Figure 11.3, the image features derived from linear projection are labelled\\nas ImageFeature, with the corresponding questions serving as text instructions. The special token [VQA]\\nis used as the task identifier, forming the complete multimodal instructional template:\\n92\\n[INST]<img><ImageFeature></img>[VQA] Instruction [/INST].\\nModel Training\\nWeights from MiniGPT-v2, pre-trained on general domain datasets, are further fine-tuned using multi-\\nmodal medical datasets in two stages. The LoRA technique is employed for efficient fine-tuning, updating\\nonly a small portion of the entire model, as detailed below:\\n• Fine-tuning with image captioning: During this stage, the model is fine-tuned using the ROCO\\nmedical image-caption dataset, which contains medical image-caption pairs of varying lengths. The\\nprompt template used is <Img><ImageHere></Img>[caption] <instruction>, with the instruc-\\ntion prompt randomly selected from a pool of four candidates, such as “Briefly describe this image.”\\nDuring training, only the linear projection layer and the LoRA layer in the LLM are fine-tuned,\\nwhile other parts of the model remain frozen.\\n• Fine-tuning on VQA: In the second stage, the model is fine-tuned on the Med-VQA dataset,\\nVQA-RAD, which contains triplets of images, questions, and answers. Following the instruction\\ntemplate proposed in MiniGPT-v2, the template used is: “[INST] <img><ImageFeature></img>[VQA]\\nInstruction [/INST]”, where the instruction prompt is: “Based on the image, respond to this\\nquestion with a short answer: question,” with question signifying the question corresponding to\\nthe given medical image. The motivation for generating short answers is to validate against the\\nexisting labelled data in VQA-RAD, where the answers are typically short in both open-ended and\\nclosed-ended QA pairs. Similar to the first stage, the vision encoder and the LLM remain frozen\\nwhile only the linear projection and LoRA layers in the LLM are updated.\\n11.3\\nApplications of Multimodal models\\n1. Gesture Recognition - These models interpret and recognise human gestures, which is crucial\\nfor sign language translation. Multimodal models facilitate inclusive communication by processing\\ngestures and converting them into text or speech.\\n2. Video Summarisation - Multimodal models can summarise lengthy videos by extracting key vi-\\nsual and audio elements. This capability streamlines content consumption, enables efficient content\\nbrowsing, and enhances video content management platforms.\\n3. DALL-E is a notable example of multimodal AI that generates images from textual descriptions.\\nThis technology expands creative possibilities in content creation and visual storytelling, with\\napplications in art, design, advertising, and more.\\n4. Educational Tools - Multimodal models enhance learning experiences by providing interactive\\neducational content that responds to both visual and verbal cues from students. They are integral\\nto adaptive learning platforms that adjust content and difficulty based on student performance and\\nfeedback.\\n5. Virtual Assistants - Multimodal models power virtual assistants by understanding and respond-\\ning to voice commands while processing visual data for comprehensive user interaction. They are\\nessential for smart home automation, voice-controlled devices, and digital personal assistants.\\n11.4\\nAudio or Speech LLMs Or Large Audio Models\\nAudio or speech LLMs are models designed to understand and generate human language based on audio\\ninputs. They have applications in speech recognition, text-to-speech conversion, and natural language\\nunderstanding tasks. These models are typically pre-trained on large datasets to learn generic language\\npatterns, which are then fine-tuned on specific tasks or domains to enhance performance.\\nAudio and Speech Large Language Models (LLMs) represent a significant advancement in the integration\\nof language processing with audio signals. These models leverage a robust Large Language Model as a\\nfoundational backbone, which is enhanced to handle multimodal data through the inclusion of custom\\naudio tokens. This transformation allows the models to learn and operate within a shared multimodal\\nspace, where both text and audio signals can be effectively processed.\\n93\\nUnlike text, which is inherently discrete, audio signals are continuous and need to be discretized into\\nmanageable audio tokens. Techniques like HuBERT[99] and wav2vec[100] are employed for this purpose,\\nconverting audio into a tokenized format that the LLM can process alongside text. The model, typically\\nautoregressive and decoder-based, is pre-trained using a combination of self-supervised tasks, such as\\npredicting masked tokens in interleaved text and audio, and supervised fine-tuning for specific tasks like\\ntranscription or sentiment analysis. This capability to handle and generate audio and text simultane-\\nously allows for a wide range of applications, from audio question answering to speech-based sentiment\\ndetection, making Audio and Speech LLMs a versatile tool in multimodal AI. The figure 11.4 illustrates\\nan example of a multimodal Audio LM architecture. In this setup, a prompt provides instructions in\\nboth text and audio formats. The audio is tokenized using an audio tokenizer. The multimodal model\\nthen combines these text and audio tokens and generates spoken speech through a vocoder (also known\\nas a voice decoder).\\nFigure 11.4: Multimodal Audio-Text Language Model architecture that integrates text and audio in-\\nputs for advanced multimodal processing.\\nThe architecture utilises text tokenizers and audio en-\\ncoders/tokenizers to convert inputs into tokens, which are then processed by the audio-text LM. This\\nmodel supports both discrete and continuous speech processing and enables tasks such as sentiment anal-\\nysis and response generation in natural language. The audio tokens are further refined using a vocoder,\\nwhile text tokens are detokenized to produce coherent text outputs (adapted from [101]).\\n94\\nAudio and speech LLMs like AudioPaLM[102], AudioLM[103], and various adaptations of models like\\nWhisper and LLaMA, integrate capabilities for understanding and generating audio data, including\\nspeech-to-text (STT), text-to-speech (TTS), and speech-to-speech (STS) translation.\\nThese models\\nhave shown that LLMs, initially designed for text, can be effectively adapted for audio tasks through\\nsophisticated tokenization and fine-tuning techniques.\\n11.4.1\\nTokenization and Preprocessing\\nA key aspect of adapting LLMs for audio is the tokenization of audio data into discrete representations\\nthat the model can process. For instance, AudioLM and AudioPaLM utilise a combination of acoustic\\nand semantic tokens. Acoustic tokens capture the high-quality audio synthesis aspect, while semantic\\ntokens help maintain long-term structural coherence in the generated audio. This dual-token approach\\nallows the models to handle both the intricacies of audio waveforms and the semantic content of speech.\\n11.4.2\\nFine-Tuning Techniques\\nFine-tuning audio and speech LLMs typically involve several key strategies:\\n• Full Parameter Fine-Tuning: This involves updating all the model’s parameters during fine-\\ntuning. For instance, LauraGPT and SpeechGPT fine-tune all parameters to adapt pre-trained\\ntext LLMs to various audio tasks, although this can be computationally expensive.\\n• Layer-Specific Fine-Tuning: Techniques like LoRA (Low-Rank Adaptation) update only spe-\\ncific layers or modules of the model. This method significantly reduces computational requirements\\nwhile still allowing effective adaptation. Models like Qwen-Audio leverage LoRA to fine-tune pre-\\ntrained components for enhanced performance on speech recognition tasks.\\n• Component-Based Fine-Tuning: Recent models, such as those integrating the Whisper en-\\ncoder, freeze certain parts of the model (like the speech encoder) and only fine-tune a linear\\nprojector or specific adapters to align the speech and text modalities. This approach simplifies the\\ntraining process and enhances efficiency[104].\\n• Multi-Stage Fine-Tuning: Models like AudioPaLM perform multi-stage fine-tuning, starting\\nwith a text-based pre-training phase, followed by fine-tuning on a mixture of tasks that include\\nboth text and audio data. This staged approach leverages the strengths of pre-trained text models\\nwhile adapting them for multimodal tasks.\\n11.4.3\\nFine-Tuning Whisper for Automatic Speech Recognition (ASR)\\nWhisper1 is an advanced Automatic Speech Recognition (ASR) model developed by OpenAI, designed\\nto convert spoken language into text. Built upon the powerful Transformer architecture, Whisper excels\\nat capturing and transcribing diverse speech patterns across various languages and accents.\\nUnlike\\ntraditional ASR models that require extensive labelled data, Whisper leverages a vast dataset and self-\\nsupervised learning, enabling it to perform robustly in noisy environments and handle a wide range of\\nspeech variations. Its versatility and high accuracy make it an ideal choice for applications such as voice\\nassistants, transcription services, and multilingual speech recognition systems.\\nWhy Fine-Tune Whisper?\\nFine-tuning Whisper for specific ASR tasks can significantly enhance its performance in specialised\\ndomains. Although Whisper is pre-trained on a large and diverse dataset, it might not fully capture\\nthe nuances of specific vocabularies or accents present in niche applications. Fine-tuning allows Whisper\\nto adapt to particular audio characteristics and terminologies, leading to more accurate and reliable\\ntranscriptions. This process is especially beneficial in industries with domain-specific jargon, like medical,\\nlegal, or technical fields, where the generic model might struggle with specialised vocabulary.\\n1https://openai.com/index/whisper/\\n95\\nSteps to Fine-Tune Whisper\\n• Data Collection and Preparation: Gather a sizable dataset that matches the target domain or\\ntask. Ensure the dataset includes diverse examples with clear transcriptions. Clean and preprocess\\nthe audio files and transcripts, ensuring they are in a consistent format and aligned correctly. Tools\\nlike FFmpeg2 can help standardise audio formats and sample rates.\\n• Data Augmentation: To improve robustness, augment the dataset with variations such as dif-\\nferent noise levels, accents, or speeds. Techniques like adding background noise, altering pitch, or\\nchanging the tempo can help the model generalise better to real-world conditions.\\n• Preprocessing: Convert the audio files into a format suitable for Whisper, typically into mel\\nspectrograms or another time-frequency representation. This transformation is crucial as Whisper\\nrelies on such representations to learn and transcribe speech effectively.\\n• Model Configuration: Initialise the Whisper model with pre-trained weights. Configure the\\nmodel to accommodate the target language or domain-specific adjustments. This includes setting\\nappropriate hyperparameters, like learning rate and batch size, tailored to the dataset’s size and\\ncomplexity.\\n• Training: Fine-tune the Whisper model on the prepared dataset using a framework like PyTorch\\nor TensorFlow. Ensure to monitor the model’s performance on a validation set to avoid overfitting.\\nTechniques like gradient clipping, learning rate scheduling, and early stopping can help maintain\\ntraining stability and efficiency.\\n• Evaluation and Testing: After training, evaluate the model’s performance on a separate test\\nset to assess its accuracy and generalisability. Metrics like Word Error Rate (WER) or Character\\nError Rate (CER) provide insights into how well the model transcribes audio compared to ground\\ntruth transcriptions.\\n11.4.4\\nCase Studies and Applications\\n1. Medical Transcription: Fine-tuning speech LLMs on medical data has led to significant im-\\nprovements in transcribing doctor-patient interactions. Models like Whisper have been fine-tuned\\non medical terminologies, resulting in more accurate and reliable transcriptions.\\n2. Legal Document Processing: Legal firms have employed fine-tuned audio LLMs to transcribe\\ncourt proceedings and legal discussions.\\nDomain-specific fine-tuning has enhanced the models’\\nability to recognise and accurately transcribe legal jargon.\\n3. Customer Service Automation: Companies are using fine-tuned speech models to automate\\ncustomer service interactions. These models are trained on customer support data to understand\\nand respond to queries more effectively, providing a more seamless user experience.\\n2https://ffmpeg.org/ffmpeg.html\\n96\\nChapter 12\\nOpen Challenges and Research\\nDirections\\n12.1\\nScalability Issues\\nThe fine-tuning of Large Language Models (LLMs) such as GPT-4, PaLM1 , and T52 has become a critical\\narea of research, presenting several significant challenges and opening up new avenues for exploration,\\nparticularly in scaling these processes efficiently. This discussion focuses on the two main aspects: the\\nchallenges in scaling fine-tuning processes and potential research directions for scalable solutions.\\n12.1.1\\nChallenges in Scaling Fine-Tuning Processes\\n1. Computational Resources: Large-scale models such as GPT-3 and PaLM require enormous\\ncomputational resources for fine-tuning. For instance, fine-tuning a 175-billion parameter model\\nlike GPT-3 necessitates high-performance GPUs or TPUs capable of handling vast amounts of data\\nand complex operations. The sheer volume of parameters translates to extensive computational\\ndemands. Even a relatively smaller model, such as BERT-large with 340 million parameters, can\\nbe computationally intensive to fine-tune.\\n2. Memory Requirements: The memory footprint for fine-tuning LLMs is staggering. Each pa-\\nrameter in the model requires storage, and during training, additional memory is needed to store\\nintermediate computations, gradients, and optimiser states. For example, loading a 7 billion pa-\\nrameter model (e.g., LLaMA 2) in FP32 (4 bytes per parameter) requires approximately 28 GB\\nof GPU memory, while fine-tuning demands around 112 GB of GPU memory[105]. This memory\\ndemand is beyond the capability of most consumer-grade hardware, making fine-tuning accessible\\nprimarily to well-funded organisations or research institutions.\\n3. Data Volume: LLMs typically require vast amounts of training data to achieve state-of-the-art\\nperformance during fine-tuning. This data needs to be loaded, preprocessed, and fed into the model\\nat high speeds to maintain efficient training. Managing large datasets can become a bottleneck,\\nespecially if the data is stored in a distributed fashion across multiple systems or if it needs to be\\nfetched from remote storage.\\n4. Throughput and Bottlenecks: High throughput is essential to keep GPUs or TPUs fully\\nutilised. However, data pipelines can become bottlenecks if not properly optimised. For exam-\\nple, shuffling large datasets or loading them into memory quickly enough to keep up with the\\ntraining process can be challenging. Techniques like data packing, where multiple small examples\\nare combined into larger batches, help improve throughput but add complexity to data handling\\nroutines.[106]\\n5. Efficient Use of Resources: The financial and environmental costs of fine-tuning large models\\nare significant. Large-scale fine-tuning involves not just the direct cost of computational resources\\nbut also the indirect costs associated with energy consumption and infrastructure maintenance.\\n1https://ai.google/discover/palm2/\\n2https://huggingface.co/docs/transformers/en/model_doc/t5\\n97\\nTechniques such as mixed-precision training and gradient checkpointing can reduce these costs by\\noptimising memory and computational efficiency.\\nThe challenges in scaling the fine-tuning processes of LLMs are multifaceted and complex, involving sig-\\nnificant computational, memory, and data handling constraints. Innovations in PEFT, data throughput\\noptimisation, and resource-efficient training methods are critical for overcoming these challenges. As\\nLLMs continue to grow in size and capability, addressing these challenges will be essential for making\\nadvanced AI accessible and practical for a wider range of applications.\\n12.1.2\\nResearch Directions for Scalable Solutions\\nAdvanced PEFT Techniques and Sparse Fine-Tuning\\nRecent advancements in PEFT techniques, like LoRA and its variant, Quantised LoRA, are revolu-\\ntionising the scalability of LLMs. LoRA reduces the computational burden by updating only a low-rank\\napproximation of the parameters, significantly lowering memory and processing requirements. Quantised\\nLoRA further optimises resource usage by applying quantisation to these low-rank matrices, maintaining\\nhigh model performance while minimising the need for extensive hardware. This has enabled efficient\\nfine-tuning of massive models, such as in Meta’s LLaMA project, where adapting a smaller set of influ-\\nential parameters allowed the models to perform robustly across various tasks with less computational\\nstrain.\\nSparse fine-tuning techniques, such as SpIEL [107] complement these efforts by selectively updating\\nonly the most impactful parameters. SpIEL fine-tunes models by only changing a small portion of the\\nparameters, which it tracks with an index. The process includes updating the parameters, removing the\\nleast important ones, and adding new ones based on their gradients or estimated momentum using an\\nefficient optimiser.\\nData Efficient Fine-Tuning (DEFT)\\nTo address the scalability challenges, recently the concept of DEFT has emerged. This novel approach\\nintroduces data pruning as a mechanism to optimise the fine-tuning process by focusing on the most\\ncritical data samples.\\nDEFT aims to enhance the efficiency and effectiveness of fine-tuning LLMs by selectively pruning the\\ntraining data to identify the most influential and representative samples. This method leverages few-shot\\nlearning principles, enabling LLMs to adapt to new data with minimal samples while maintaining or even\\nexceeding performance levels achieved with full datasets [108].\\nKey Components of DEFT\\nHigh Accuracy Through Influence Score: DEFT introduces the concept of an influence score to\\nevaluate and rank the importance of each data sample in the context of LLM fine-tuning. The influence\\nscore estimates how removing a specific sample would impact the overall performance of the model. This\\napproach allows for the selection of a small subset of data that is highly representative and influential,\\nthereby enabling the model to maintain high accuracy with significantly fewer samples.\\nHigh Efficiency Through Effort Score and Surrogate Models: To address the cost and complexity\\nof evaluating large datasets, DEFT employs a surrogate model—a smaller, computationally less intensive\\nmodel—to approximate the influence scores. This surrogate model helps estimate the impact of each\\nsample without the heavy computational burden associated with directly using the LLM. Additionally,\\nDEFT introduces an effort score to identify and prioritise more challenging samples that may require\\nspecial attention from the LLM. This dual-score system ensures that the fine-tuning process remains\\nboth efficient and effective.\\nPractical Implications and Use Cases\\n• Few-Shot Fine-Tuning for Rapid Adaptation: DEFT is particularly beneficial for applica-\\ntions where models need to quickly adapt to new data with minimal samples. In scenarios such as\\n98\\npersonalised recommendations or adapting to sudden changes in user behaviour, DEFT allows for\\nrapid fine-tuning, maintaining high performance with a fraction of the data typically required.\\n• Reducing Computational Costs in Large-Scale Deployments: By focusing on the most\\ninfluential data samples and using surrogate models, DEFT significantly reduces the computational\\nresources needed for fine-tuning. This makes it feasible to maintain high-performing LLMs even in\\nlarge-scale deployments where data volumes are substantial.\\nFuture Directions\\nThe DEFT introduces a data pruning task for fine-tuning large language models (LLMs), setting the\\nstage for new research into efficient LLM-based recommendation systems and presenting numerous op-\\nportunities for future exploration. Key areas for further investigation include:\\n• Applying the proposed DEALRec[109] approach to a broader range of LLM-based recommender\\nmodels across diverse cross-domain datasets, thereby enhancing fine-tuning performance within\\nresource constraints.\\n• Addressing the limited context window of LLMs by selectively focusing on the most informative\\nitems in user interaction sequences for fine-tuning purposes.\\n12.1.3\\nHardware and Algorithm Co-Design\\nCo-designing hardware and algorithms tailored for LLMs can lead to significant improvements in the\\nefficiency of fine-tuning processes. Custom hardware accelerators optimised for specific tasks or types of\\ncomputation can drastically reduce the energy and time required for model training and fine-tuning.\\n• Custom Accelerators: Developing hardware accelerators specifically for the sparse and low-\\nprecision computations often used in LLM fine-tuning can enhance performance. These accelerators\\nare designed to efficiently handle the unique requirements of LLMs, such as the high memory\\nbandwidth and extensive matrix multiplications involved in transformer architectures.\\n• Algorithmic Optimisation: Combining hardware innovations with algorithmic optimisation\\ntechniques, such as those that minimise data movement or leverage hardware-specific features\\n(e.g., tensor cores for mixed-precision calculations), can further enhance the efficiency of fine-tuning\\nprocesses.\\n• Example: NVIDIA’s TensorRT3 is an example of hardware and algorithm co-design in action.\\nIt optimises deep learning models for inference by leveraging NVIDIA GPUs’ capabilities, signifi-\\ncantly speeding up the process while reducing the resource requirements. TensorRT’s optimisations\\ninclude support for mixed-precision and sparse tensor operations, making it highly suitable for fine-\\ntuning large models.\\nAs the scale of language models continues to grow, addressing the challenges of fine-tuning them efficiently\\nbecomes increasingly critical. Innovations in PEFT, sparse fine-tuning, data handling, and the integration\\nof advanced hardware and algorithmic solutions present promising directions for future research. These\\nscalable solutions are essential not only to make the deployment of LLMs feasible for a broader range of\\napplications but also to push the boundaries of what these models can achieve.\\n12.2\\nEthical Considerations in Fine-Tuning LLMs\\n12.2.1\\nBias and Fairness\\nWhen fine-tuning LLMs, the goal is often to optimise their performance for specific tasks or datasets.\\nHowever, these datasets may inherently carry biases that get transferred to the model during the fine-\\ntuning process. Biases can arise from various sources, including historical data, imbalanced training\\nsamples, and cultural prejudices embedded in language. For instance, an LLM fine-tuned on a dataset\\nprimarily sourced from English-speaking countries might underperform or make biased predictions when\\n3https://docs.nvidia.com/tensorrt/index.html\\n99\\napplied to text from other linguistic or cultural backgrounds. Google AI’s Fairness Indicators tool4 is a\\npractical solution that allows developers to evaluate the fairness of their models by analysing performance\\nmetrics across different demographic groups. This tool can be integrated into the fine-tuning pipeline to\\nmonitor and address bias in real-time.\\nAddressing Bias and Fairness\\n• Diverse and Representative Data: Ensuring that fine-tuning datasets are diverse and repre-\\nsentative of all user demographics can help mitigate bias.\\n• Fairness Constraints: Incorporating fairness constraints, as suggested by the FairBERTa frame-\\nwork5, ensures that fine-tuned models maintain equitable performance across different groups.\\n• Example Application: In healthcare, an LLM fine-tuned to assist in diagnosing conditions might\\ninitially be trained on data from predominantly white patients. Such a model could produce less\\naccurate diagnoses for patients from other racial backgrounds. By using fairness-aware fine-tuning\\ntechniques, healthcare providers can develop models that perform more equitably across diverse\\npatient populations.\\n12.2.2\\nPrivacy Concerns\\nFine-tuning often involves using sensitive or proprietary datasets, which poses significant privacy risks. If\\nnot properly managed, fine-tuned models can inadvertently leak private information from their training\\ndata. This issue is especially critical in domains like healthcare or finance, where data confidentiality is\\nparamount.\\nEnsuring Privacy During Fine-Tuning\\n• Differential Privacy6: Implementing differential privacy techniques during fine-tuning can pre-\\nvent models from leaking sensitive information.\\n• Federated Learning7: Utilising federated learning frameworks allows models to be fine-tuned\\nacross decentralised data sources, which enhances privacy by keeping data localised.\\n• Example Application: In customer service applications, companies might fine-tune LLMs using\\ncustomer interaction data. Employing differential privacy ensures that the model learns from these\\ninteractions without memorising and potentially leaking personal information, thus maintaining\\ncustomer confidentiality.\\n12.2.3\\nSecurity Risks\\n• Security Vulnerabilities in Fine-Tuned Models: Fine-tuned LLMs are susceptible to secu-\\nrity vulnerabilities, particularly from adversarial attacks. These attacks involve inputs designed to\\nexploit model weaknesses, causing them to produce erroneous or harmful outputs. Such vulnera-\\nbilities can be more pronounced in fine-tuned models due to their specialised training data, which\\nmay not cover all possible input scenarios.\\n• Recent Research and Industry Practices: Microsoft’s Adversarial ML Threat Matrix pro-\\nvides a comprehensive framework for identifying and mitigating adversarial threats during model\\ndevelopment and fine-tuning. This matrix helps developers understand the potential attack vectors\\nand implement defensive strategies accordingly.\\n• Enhancing Security in Fine-Tuning:\\n– Adversarial Training: Exposing models to adversarial examples during fine-tuning can\\nenhance their robustness against attacks.\\n– Security Audits: Regularly conducting security audits on fine-tuned models can help iden-\\ntify and address potential vulnerabilities.\\n4https://research.google/blog/fairness-indicators-scalable-infrastructure-for-fair-ml-systems/\\n5https://huggingface.co/facebook/FairBERTa\\n6https://privacytools.seas.harvard.edu/differential-privacy\\n7https://research.ibm.com/blog/what-is-federated-learning\\n100\\n12.3\\nAccountability and Transparency\\n12.3.1\\nThe Need for Accountability and Transparency\\nFine-tuning can significantly alter an LLM’s behaviour, making it crucial to document and understand\\nthe changes and their impacts.\\nThis transparency is essential for stakeholders to trust the model’s\\noutputs and for developers to be accountable for its performance and ethical implications.\\n12.3.2\\nRecent Research and Industry Practices\\nMeta’s Responsible AI framework8 underscores the importance of documenting the fine-tuning process\\nand its effects on model behaviour. This includes maintaining detailed records of the data used, the\\nchanges made during fine-tuning, and the evaluation metrics applied.\\n12.3.3\\nPromoting Accountability and Transparency\\n• Comprehensive Documentation: Creating detailed documentation of the fine-tuning process\\nand its impact on model performance and behaviour.\\n• Transparent Reporting: Utilising frameworks like Model Cards9 to report on the ethical and\\noperational characteristics of fine-tuned models.\\n• Example Application: In content moderation systems, LLMs fine-tuned to identify and filter\\nharmful content need clear documentation and reporting. This ensures that platform users and\\nregulators understand how the model operates and can trust its moderation decisions.\\n12.3.4\\nProposed frameworks/techniques for Ethical Fine-Tuning\\nFrameworks for Mitigating Bias\\nBias-aware fine-tuning frameworks aim to incorporate fairness into the model training process. Fair-\\nBERTa, introduced by Facebook, is an example of such a framework that integrates fairness constraints\\ndirectly into the model’s objective function during fine-tuning. This approach ensures that the model’s\\nperformance is balanced across different demographic groups.\\nOrganisations can adopt fairness-aware frameworks to develop more equitable AI systems. For instance,\\nsocial media platforms can use these frameworks to fine-tune models that detect and mitigate hate speech\\nwhile ensuring fair treatment across various user demographics.\\nTechniques for Privacy Preservation\\nDifferential privacy and federated learning are key techniques for preserving privacy during fine-tuning.\\nTensorFlow Privacy10, developed by Google, provides built-in support for differential privacy, allowing\\ndevelopers to fine-tune models securely without compromising data confidentiality.\\nLLMs are highly effective but face challenges when applied in sensitive areas where data privacy is cru-\\ncial. To address this, researchers focus on enhancing Small Language Models (SLMs) tailored to specific\\ndomains. Existing methods often use LLMs to generate additional data or transfer knowledge to SLMs,\\nbut these approaches struggle due to differences between LLM-generated data and private client data. In\\nresponse, a new Federated Domain-specific Knowledge Transfer (FDKT)[110] framework is introduced.\\nFDKT leverages LLMs to create synthetic samples that mimic clients’ private data distribution using\\ndifferential privacy. This approach significantly boosts SLMs’ performance by approximately 5% while\\nmaintaining data privacy with a minimal privacy budget, outperforming traditional methods relying\\nsolely on local private data.\\nIn healthcare, federated fine-tuning can allow hospitals to collaboratively train models on patient data\\nwithout transferring sensitive information. This approach ensures data privacy while enabling the de-\\nvelopment of robust, generalisable AI systems.\\n8https://ai.meta.com/responsible-ai/\\n9https://huggingface.co/docs/hub/en/model-cards\\n10https://www.tensorflow.org/responsible_ai/privacy/guide\\n101\\nFrameworks for Enhancing Security\\nAdversarial training and robust security measures[111] are essential for protecting fine-tuned models\\nagainst attacks. The adversarial training approach involves training models with adversarial examples\\nto improve their resilience against malicious inputs. Microsoft Azure’s adversarial training tools provide\\npractical solutions for integrating these techniques into the fine-tuning process, helping developers create\\nmore secure and reliable models.\\nIn cybersecurity, fine-tuned LLMs used for threat detection can benefit from adversarial training to\\nenhance their ability to identify and respond to sophisticated attacks, thereby improving organisational\\nsecurity.\\nFrameworks for Ensuring Transparency\\nTransparency and accountability frameworks, such as Model Cards and AI FactSheets11, provide struc-\\ntured ways to document and report on the fine-tuning process and the resulting model behaviours. These\\nframeworks promote understanding and trust among stakeholders by clearly outlining the model’s capa-\\nbilities, limitations, and ethical considerations.\\nIn government applications, where AI systems might be used for decision-making or public services,\\nmaintaining transparent documentation through frameworks like AI FactSheets ensures that these sys-\\ntems are accountable and their decisions can be audited and trusted by the public.\\nFine-tuning LLMs introduces several ethical challenges, including bias, privacy risks, security vulnera-\\nbilities, and accountability concerns. Addressing these requires a multifaceted approach that integrates\\nfairness-aware frameworks, privacy-preserving techniques, robust security measures, and transparency\\nand accountability mechanisms.\\nBy leveraging recent advancements in these areas, researchers and\\npractitioners can develop and deploy LLMs that are not only powerful but also ethically sound and\\ntrustworthy.\\n12.4\\nIntegration with Emerging Technologies\\nIntegrating LLMs with emerging technologies such as IoT (Internet of Things) and edge computing\\npresents numerous opportunities and challenges, reflecting advancements and insights from recent re-\\nsearch and industry developments.\\n12.4.1\\nOpportunities\\n• Enhanced Decision-Making and Automation: LLMs have the capability to analyse and derive\\ninsights from vast amounts of unstructured data generated by IoT devices. This data can range\\nfrom sensor readings in manufacturing plants to environmental data in smart cities. By processing\\nthis data in real-time, LLMs can optimise decision-making processes and automate tasks that\\ntraditionally required human intervention. For example:\\n– Industrial Applications: Predictive maintenance can be enhanced by LLMs analysing sen-\\nsor data to predict equipment failures before they occur, thereby reducing downtime and\\nmaintenance costs.\\n– Smart Cities: LLMs can analyse traffic patterns and environmental data from IoT sensors\\nto optimise city infrastructure and improve urban planning decisions.\\n• Personalised User Experiences: Integration with edge computing allows LLMs to process\\ndata locally on devices rather than relying solely on cloud-based servers. This enables LLMs to\\ndeliver highly personalised services based on real-time data and user preferences, enhancing user\\nexperiences across various domains:\\n– Healthcare: LLMs can provide personalised healthcare recommendations by analysing data\\nfrom wearable devices and integrating it with medical records securely stored on edge devices.\\n11https://aifs360.res.ibm.com/\\n102\\n• Improved Natural Language Understanding: IoT data integration enriches LLMs’ ability to\\nunderstand context and respond more intelligently to natural language queries. This can signifi-\\ncantly improve user interactions with smart environments:\\n– Smart Homes: LLMs integrated with IoT devices can understand and respond to voice\\ncommands more accurately, adjusting smart home settings based on real-time sensor data\\n(e.g., adjusting lighting and temperature based on occupancy and environmental conditions).\\n12.4.2\\nChallenges\\n• Data Complexity and Integration: Integrating data from diverse IoT devices poses challenges\\nrelated to data quality, interoperability, and scalability.\\nLLMs need to effectively process and\\ninterpret this heterogeneous data to derive meaningful insights:\\n– Data Integration: Ensuring seamless integration of data streams from different IoT plat-\\nforms and devices without compromising data integrity or performance.\\n– Data Preprocessing: Cleaning and preprocessing IoT data to ensure consistency and reli-\\nability before feeding it into LLMs for analysis.\\n• Privacy and Security: Edge computing involves processing sensitive data locally on devices,\\nraising concerns about data privacy and security:\\n– Data Privacy: Implementing robust encryption techniques and access control mechanisms\\nto protect sensitive data processed by LLMs on edge devices.\\n– Secure Communication: Ensuring secure communication channels between IoT devices\\nand LLMs to prevent data breaches or unauthorised access.\\n• Real-Time Processing and Reliability: LLMs deployed in edge computing environments must\\noperate with low latency and high reliability to support real-time applications:\\n– Latency: Optimising algorithms and processing capabilities of LLMs to handle real-time\\ndata streams efficiently without delays.\\n– Reliability: Ensuring the accuracy and consistency of insights generated by LLMs in dynamic\\nand unpredictable IoT environments.\\n12.5\\nFuture Research Areas\\n• Federated Learning and Edge Computing: Exploring federated learning techniques where\\nLLMs can be trained collaboratively across edge devices without centralised data aggregation.\\nThis approach addresses privacy concerns and reduces communication overhead.\\n• Real-Time Decision Support Systems: Developing LLM-based systems capable of real-time\\ndecision-making by integrating with edge computing infrastructure. This includes optimising algo-\\nrithms for low-latency processing and ensuring reliability under dynamic environmental conditions.\\n• Ethical and Regulatory Implications: Investigating the ethical implications of integrating\\nLLMs with IoT and edge computing, particularly regarding data ownership, transparency, and\\nfairness. This area requires frameworks for ethical AI deployment and governance.\\n103\\nGlossary\\nLLM Large Language Model – A type of AI model, typically with billions of parameters, trained on vast\\namounts of text data to understand and generate human-like text. They are primarily designed\\nfor tasks in natural language processing (NLP).\\nNLP Natural Language Processing – A field of artificial intelligence that focuses on the interaction\\nbetween computers and humans through natural language, including tasks like language generation,\\ntranslation, and sentiment analysis.\\nLoRA Low-Rank Adaptation – A parameter-efficient fine-tuning technique that adjusts only small low-\\nrank matrices to adapt pre-trained models to specific tasks, thus preserving most of the original\\nmodel’s parameters.\\nDoRA Weight-Decomposed Low-Rank Adaptation – A technique that decomposes model weights into\\nmagnitude and direction components, facilitating fine-tuning while maintaining inference efficiency.\\nQLoRA Quantised Low-Rank Adaptation – A variation of LoRA, specifically designed for quantised\\nmodels, allowing for efficient fine-tuning in resource-constrained environments.\\nPPO Proximal Policy Optimisation – A reinforcement learning algorithm that adjusts policies by bal-\\nancing the exploration of new actions and exploitation of known rewards, designed for stability and\\nefficiency in training.\\nDPO Direct Preference Optimisation – A method that directly aligns language models with human\\npreferences through preference optimisation, bypassing reinforcement learning models like PPO.\\nMoE Mixture of Experts – A model architecture that employs multiple specialised subnetworks, called\\nexperts, which are selectively activated based on the input to improve model performance and\\nefficiency.\\nMoA Mixture of Agents – A multi-agent framework where several agents collaborate during training\\nand inference, leveraging the strengths of each agent to improve overall model performance.\\nPEFT Parameter-Efficient Fine-Tuning – A fine-tuning approach for large models that involves adjust-\\ning only a subset of model parameters, improving efficiency in scenarios with limited computational\\nresources. This includes techniques like LoRA, QLoRA, and adapters.\\nAdapters Small, trainable modules introduced into the layers of pre-trained language models, allowing\\nefficient task-specific fine-tuning without modifying the core parameters of the original model.\\nTechniques such as **AdapterFusion** and **AdapterSoup** fall under this category, facilitating\\nthe combination of multiple adapters for complex multitasking.\\nSoft Prompt Tuning (SPT) A fine-tuning technique where a set of trainable prompt tokens are added\\nto the input sequence to guide a pre-trained model towards task-specific performance without\\nmodifying internal model weights.\\nPrefix-Tuning A variation of soft prompt tuning where a fixed sequence of trainable vectors is prepended\\nto the input layer at every layer of the model, enhancing task-specific adaptation.\\nQuantisation The process of reducing the precision of model weights and activations, often from 32-bit\\nto lower-bit representations like 8-bit or 4-bit, to reduce memory usage and improve computational\\nefficiency.\\n104\\nQuantised LLMs Large Language Models that have undergone quantisation, a process that reduces\\nthe precision of model weights and activations, often from 32-bit to 8-bit or lower, to enhance\\nmemory and computational efficiency.\\nPruning A model optimisation technique that reduces the complexity of large language models by\\nremoving less significant parameters, enabling faster inference and lower memory usage.\\nHalf Fine-Tuning (HFT) A fine-tuning method where half of the model’s parameters are kept frozen\\nwhile the other half are updated, helping to maintain pre-trained knowledge while adapting the\\nmodel to new tasks.\\nStructured Masking A technique that masks entire layers, heads, or other structural components of\\na model to reduce complexity while fine-tuning for specific tasks.\\nUnstructured Masking A technique where certain parameters of the model are masked out randomly\\nor based on a pattern during fine-tuning, allowing for the identification of the most important\\nmodel weights.\\nGLUE General Language Understanding Evaluation – A benchmark used to evaluate the performance\\nof NLP models across a variety of language understanding tasks, such as sentiment analysis and\\nnatural language inference.\\nSuperGLUE Super General Language Understanding Evaluation – A more challenging extension of\\nGLUE, consisting of harder tasks designed to test the robustness and adaptability of NLP models.\\nTruthfulQA A benchmark designed to measure the truthfulness of a language model’s output, focusing\\non factual accuracy and resistance to hallucination.\\nIFEval Instruction Following Evaluation – A benchmark that assesses a model’s ability to follow explicit\\ninstructions across tasks, usually in the context of fine-tuning large models for adherence to specific\\ninstructions.\\nBBH Big Bench Hard – A subset of the Big Bench dataset, which consists of particularly difficult tasks\\naimed at evaluating the advanced reasoning abilities of large language models.\\nMATH A dataset created to evaluate a model’s ability to solve high-school level mathematical problems,\\npresented in formal formats like LaTeX.\\nGPQA General-Purpose Question Answering – A challenging dataset that features knowledge-based\\nquestions crafted by experts to assess deep reasoning and factual recall.\\nMuSR Multimodal Structured Reasoning – A dataset that involves complex problems requiring lan-\\nguage models to integrate reasoning across modalities, often combining text with other forms of\\ndata such as images or graphs.\\nMMLU Massive Multitask Language Understanding – A benchmark that evaluates a language model’s\\nability to perform various tasks across diverse domains, such as humanities, STEM, social sciences,\\nand others, typically requiring high-level reasoning.\\nMMLU-PRO A refined version of the MMLU dataset with a focus on more challenging, multi-choice\\nproblems, typically requiring the model to parse long-range context.\\nARC AI2 Reasoning Challenge – A benchmark for evaluating a language model’s reasoning capabilities\\nusing a dataset of multiple-choice science questions.\\nCOQA Conversational Question Answering – A benchmark that evaluates how well a language model\\ncan understand and engage in back-and-forth conversation, especially in a question-answer format.\\nDROP Discrete Reasoning Over Paragraphs – A benchmark that tests a model’s ability to perform\\ndiscrete reasoning over text, especially in scenarios requiring arithmetic, comparison, or logical\\nreasoning.\\nSQuAD Stanford Question Answering Dataset – A popular dataset for evaluating a model’s ability to\\nunderstand and answer questions based on passages of text.\\n105\\nTREC Text REtrieval Conference – A benchmark that evaluates models on various text retrieval tasks,\\noften focusing on information retrieval and document search.\\nWMT Workshop on Machine Translation – A dataset and benchmark for evaluating the performance\\nof machine translation systems across different language pairs.\\nXNLI Cross-lingual Natural Language Inference – A dataset designed to evaluate a model’s ability to\\nunderstand and infer meaning across multiple languages.\\nPiQA Physical Interaction Question Answering – A dataset that measures a model’s understanding of\\nphysical interactions and everyday tasks.\\nWinogrande A large-scale dataset aimed at evaluating a language model’s ability to handle common-\\nsense reasoning, typically through tasks that involve resolving ambiguous pronouns in sentences.\\nRLHF Reinforcement Learning from Human Feedback – A method where language models are fine-\\ntuned based on human-provided feedback, often used to guide models towards preferred behaviours\\nor outputs.\\nRAFT Retrieval-Augmented Fine-Tuning – A method combining retrieval techniques with fine-tuning\\nto enhance the performance of language models by allowing them to access external information\\nduring training or inference.\\n106')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_txt_folder = \"../input/inputtxt\"\n",
    "\n",
    "# Import opportunity file \n",
    "for filename in os.listdir(input_txt_folder):\n",
    "    if filename.endswith('opportunityfileexample.txt'):\n",
    "        file_path = os.path.join(input_txt_folder, filename)\n",
    "        print(f\"Processing {filename}...\")\n",
    "\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                print(type(content))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "documents = [Document(page_content=content)]\n",
    "display(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da27c04",
   "metadata": {},
   "source": [
    "## LLM and Graph Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "baf10ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"llama3.2:3b\")\n",
    "graph_transformer = LLMGraphTransformer(llm=llm, ignore_tool_usage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6841729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = await graph_transformer.aconvert_to_graph_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7149fbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GraphDocument(nodes=[], relationships=[], source=Document(metadata={}, page_content='Chapter 1\\nIntroduction\\n1.1\\nBackground of Large Language Models (LLMs)\\nLarge Language Models (LLMs) represent a significant leap in computational systems capable of under-\\nstanding and generating human language. Building on traditional language models (LMs) like N-gram\\nmodels [1], LLMs address limitations such as rare word handling, overfitting, and capturing complex\\nlinguistic patterns. Notable examples, such as GPT-3 and GPT-4 [2], leverage the self-attention mecha-\\nnism within Transformer architectures to efficiently manage sequential data and understand long-range\\ndependencies. Key advancements include in-context learning for generating coherent text from prompts\\nand Reinforcement Learning from Human Feedback (RLHF) [3] for refining models using human re-\\nsponses. Techniques like prompt engineering, question-answering, and conversational interactions have\\nsignificantly advanced the field of natural language processing (NLP) [4].\\n1.2\\nHistorical Development and Key Milestones\\nLanguage models are fundamental to natural language processing (NLP), leveraging mathematical tech-\\nniques to generalise linguistic rules and knowledge for tasks involving prediction and generation. Over\\nseveral decades, language modelling has evolved from early statistical language models (SLMs) to to-\\nday’s advanced large language models (LLMs). This rapid advancement has enabled LLMs to process,\\ncomprehend, and generate text at a level comparable to human capabilities [5, 6].\\nFigure 1.1 shows the evolution of large language models from early statistical approaches to current\\nadvanced models.\\n1.3\\nEvolution from Traditional NLP Models to State-of-the-Art\\nLLMs\\nUnderstanding LLMs requires tracing the development of language models through stages such as Statis-\\ntical Language Models (SLMs), Neural Language Models (NLMs), Pre-trained Language Models (PLMs),\\nand LLMs.\\n1.3.1\\nStatistical Language Models (SLMs)\\nEmerging in the 1990s, SLMs analyse natural language using probabilistic methods to determine the\\nlikelihood of sentences within texts.\\nFor instance, the probability P(S) of the sentence “I am very\\nhappy” is given by:\\nP(S) = P(ω1, ω2, ω3, ω4) = P(I, am, very, happy)\\n(1.1)\\nThis probability can be calculated using conditional probabilities:\\nP(I, am, very, happy) = P(I) · P(am | I) · P(very | I, am) · P(happy | I, am, very)\\n(1.2)\\nConditional probabilities are estimated using Maximum Likelihood Estimation (MLE):\\n6\\nFigure 1.1: A chronological timeline showcasing the evolution of Large Language Models (LLMs) from\\n1990 to 2023. This progression begins with early statistical models such as N-grams, transitions through\\nneural language models like Word2Vec and RNN/LSTM, and advances into the era of pre-trained mod-\\nels with the introduction of transformers and attention mechanisms. The figure highlights significant\\nmilestones, including the development of BERT, GPT series, and recent innovations such as GPT-4 and\\nChatGPT, demonstrating the rapid advancements in LLM technology over time. (adapted from [6])\\nP(ωi | ω1ω2 · · · ωi−1) =\\nC(ω1ω2 · · · ωi)\\nC(ω1ω2 · · · ωi−1)\\n(1.3)\\n1.3.2\\nNeural Language Models (NLMs)\\nNLMs leverage neural networks to predict word sequences, overcoming SLM limitations. Word vectors\\nenable computers to understand word meanings. Tools like Word2Vec [7] represent words in a vector\\nspace where semantic relationships are reflected in vector angles. NLMs consist of interconnected neurons\\norganised into layers, resembling the human brain’s structure. The input layer concatenates word vectors,\\nthe hidden layer applies a non-linear activation function, and the output layer predicts subsequent words\\nusing the Softmax function to transform values into a probability distribution.\\nFigure 1.2 illustrates the structure of Neural Language Models, highlighting the layers and connections\\nused to predict subsequent words.\\n1.3.3\\nPre-trained Language Models (PLMs)\\nPLMs are initially trained on extensive volumes of unlabelled text to understand fundamental language\\nstructures (pre-training). They are then fine-tuned on a smaller, task-specific dataset. This ”pre-training\\nand fine-tuning” paradigm, exemplified by GPT-2 [8] and BERT [9], has led to diverse and effective model\\narchitectures.\\n1.3.4\\nLarge Language Models (LLMs)\\nLLMs like GPT-3, GPT-4, PaLM [10], and LLaMA [11] are trained on massive text corpora with tens of\\nbillions of parameters. LLMs undergo a two-stage process: initial pre-training on a vast corpus followed\\n7\\nFigure 1.2: A schematic representation of Neural Language Models, showcasing the layered architecture\\nwhere the input layer processes sequential data, the hidden layer captures dependencies, and the output\\nlayer generates predictions. The figure emphasises the flow of information through concatenation and\\nmatrix multiplications, culminating in a probability distribution via the softmax function. (adopted from\\n[6])\\nby alignment with human values. This approach enables LLMs to understand human commands and\\nvalues better.\\n1.4\\nOverview of Current Leading LLMs\\nLLMs are powerful tools in NLP, capable of performing tasks such as translation, summarisation, and\\nconversational interaction. Advances in transformer architectures, computational power, and extensive\\ndatasets have driven their success. These models approximate human-level performance, making them\\ninvaluable for research and practical implementations. LLMs’ rapid development has spurred research\\ninto architectural innovations, training strategies, extending context lengths, fine-tuning techniques, and\\nintegrating multi-modal data. Their applications extend beyond NLP, aiding in human-robot interactions\\nand creating intuitive AI systems. This highlights the importance of comprehensive reviews consolidating\\nthe latest developments [12].\\nFigure 1.3 provides an overview of current leading LLMs, highlighting their capabilities and applications.\\n1.5\\nWhat is Fine-Tuning?\\nFine-tuning uses a pre-trained model, such as OpenAI’s GPT series, as a foundation.\\nThe process\\ninvolves further training on a smaller, domain-specific dataset. This approach builds upon the model’s\\npre-existing knowledge, enhancing performance on specific tasks with reduced data and computational\\nrequirements.\\nFine-tuning transfers the pre-trained model’s learned patterns and features to new tasks, improving\\nperformance and reducing training data needs. It has become popular in NLP for tasks like text classi-\\nfication, sentiment analysis, and question-answering.\\n8\\nFigure 1.3: Mind map depicting various dimensions of Large Language Models (LLMs), covering aspects\\nfrom pre-training and fine-tuning methodologies to efficiency, evaluation, inference, and application do-\\nmains. Each dimension is linked to specific techniques, challenges, and examples of models that exemplify\\nthe discussed characteristics. This diagram serves as an overview of the multifaceted considerations in\\nthe development and deployment of LLMs. (adapted from [13])\\n1.6\\nTypes of LLM Fine-Tuning\\n1.6.1\\nUnsupervised Fine-Tuning\\nThis method does not require labelled data. Instead, the LLM is exposed to a large corpus of unla-\\nbelled text from the target domain, refining its understanding of language. This approach is useful for\\nnew domains like legal or medical fields but is less precise for specific tasks such as classification or\\nsummarisation.\\n1.6.2\\nSupervised Fine-Tuning (SFT)\\nSFT involves providing the LLM with labelled data tailored to the target task. For example, fine-tuning\\nan LLM for text classification in a business context uses a dataset of text snippets with class labels.\\nWhile effective, this method requires substantial labelled data, which can be costly and time-consuming\\nto obtain.\\n9\\n1.6.3\\nInstruction Fine-Tuning via Prompt Engineering\\nThis method relies on providing the LLM with natural language instructions, useful for creating spe-\\ncialised assistants. It reduces the need for vast amounts of labelled data but depends heavily on the\\nquality of the prompts.\\n1.7\\nPre-training vs Fine-tuning\\nTable 1.1 provides a comparison between pre-training and fine-tuning, highlighting their respective char-\\nacteristics and processes.\\nAspect\\nPre-training\\nFine-tuning\\nDefinition\\nTraining on a vast amount of\\nunlabelled text data\\nAdapting a pre-trained model to\\nspecific tasks\\nData Requirement\\nExtensive\\nand\\ndiverse\\nunla-\\nbelled text data\\nSmaller,\\ntask-specific labelled\\ndata\\nObjective\\nBuild general linguistic knowl-\\nedge\\nSpecialise\\nmodel\\nfor\\nspecific\\ntasks\\nProcess\\nData\\ncollection,\\ntraining\\non\\nlarge\\ndataset,\\npredict\\nnext\\nword/sequence\\nTask-specific\\ndata\\ncollection,\\nmodify last layer for task, train\\non new dataset, generate output\\nbased on tasks\\nModel Modification\\nEntire model trained\\nLast layers adapted for new task\\nComputational Cost\\nHigh (large dataset,\\ncomplex\\nmodel)\\nLower (smaller dataset,\\nfine-\\ntuning layers)\\nTraining Duration\\nWeeks to months\\nDays to weeks\\nPurpose\\nGeneral language understand-\\ning\\nTask-specific performance im-\\nprovement\\nExamples\\nGPT, LLaMA 3\\nFine-tuning LLaMA 3 for sum-\\nmarisation\\nTable 1.1: A Comparative Overview of Pre-training and Fine-tuning in Large Language Models (LLMs).\\nThe table outlines key differences between the pre-training and fine-tuning phases across various aspects\\nsuch as definition, data requirements, objectives, processes, model modification, computational costs,\\ntraining duration, and their respective purposes, with examples highlighting specific models and tasks.\\nPre-training involves extensive training on vast amounts of unlabelled data to build general linguistic\\nknowledge, while fine-tuning adapts the pre-trained models to specialised tasks using smaller, labelled\\ndatasets, focusing on task-specific performance improvements.\\n1.8\\nImportance of Fine-Tuning LLMs\\n1. Transfer Learning: Fine-tuning leverages the knowledge acquired during pre-training, adapting\\nit to specific tasks with reduced computation time and resources.\\n2. Reduced Data Requirements: Fine-tuning requires less labelled data, focusing on tailoring\\npre-trained features to the target task.\\n3. Improved Generalisation: Fine-tuning enhances the model’s ability to generalise to specific\\ntasks or domains, capturing general language features and customising them.\\n4. Efficient Model Deployment: Fine-tuned models are more efficient for real-world applications,\\nbeing computationally efficient and well-suited for specific tasks.\\n5. Adaptability to Various Tasks: Fine-tuned LLMs can adapt to a broad range of tasks, per-\\nforming well across various applications without task-specific architectures.\\n6. Domain-Specific Performance: Fine-tuning allows models to excel in domain-specific tasks by\\nadjusting to the nuances and vocabulary of the target domain.\\n10\\n7. Faster Convergence: Fine-tuning usually achieves faster convergence, starting with weights that\\nalready capture general language features.\\n1.9\\nRetrieval Augmented Generation (RAG)\\nA popular method to utilise your own data is by incorporating it into the prompt when querying the LLM\\nmodel. This approach, known as Retrieval-Augmented Generation (RAG), involves retrieving relevant\\ndata and using it as additional context for the LLM. Instead of depending solely on knowledge from the\\ntraining data, a RAG workflow pulls pertinent information, connecting static LLMs with real-time data\\nretrieval. With RAG architecture, organisations can deploy any LLM model and enhance it to return\\nrelevant results by providing a small amount of their own data (see Figure1.4 for visual workflow). This\\nprocess avoids the costs and time associated with fine-tuning or pre-training the model.\\nFigure 1.4: An illustration of the Traditional Retrieval-Augmented Generation (RAG) pipeline steps,\\ndepicting the sequential process from client query to response generation.\\nThe pipeline starts with\\nthe client’s question, followed by semantic search in a vector database, contextually enriching the data\\nbefore generating a prompt for the large language model (LLM). The final response is post-processed\\nand returned to the client.\\n1.9.1\\nTraditional RAG Pipeline and Steps\\n1. Data Indexing: Organise data efficiently for quick retrieval. This involves processing, chunking,\\nand storing data in a vector database using indexing strategies like search indexing, vector indexing,\\nand hybrid indexing.\\n2. Input Query Processing: Refine user queries to improve compatibility with indexed data. This\\ncan include simplification or vector transformation of queries for enhanced search efficiency.\\n3. Searching and Ranking: Retrieve and rank data based on relevance using search algorithms\\nsuch as TF-IDF, BM25, and deep learning models like BERT to interpret the query’s intent and\\ncontext.\\n4. Prompt Augmentation: Incorporate relevant information from the search results into the origi-\\nnal query to provide the LLM with additional context, enhancing response accuracy and relevance.\\n11\\n5. Response Generation: Use the augmented prompt to generate responses that combine the LLM’s\\nknowledge with current, specific data, ensuring high-quality, contextually grounded answers.\\n1.9.2\\nBenefits of Using RAG\\n• Up-to-Date and Accurate Responses: Enhances the LLM’s responses with current external\\ndata, improving accuracy and relevance.\\n• Reducing Inaccurate Responses: Grounds the LLM’s output in relevant knowledge, reducing\\nthe risk of generating incorrect information.\\n• Domain-Specific Responses: Delivers contextually relevant responses tailored to an organisa-\\ntion’s proprietary data.\\n• Efficiency and Cost-Effectiveness: Offers a cost-effective method for customising LLMs without\\nextensive model fine-tuning.\\n1.9.3\\nChallenges and Considerations in Serving RAG\\n1. User Experience: Ensuring rapid response times suitable for real-time applications.\\n2. Cost Efficiency: Managing the costs associated with serving millions of responses.\\n3. Accuracy: Ensuring outputs are accurate to avoid misinformation.\\n4. Recency and Relevance: Keeping responses and content current with the latest data.\\n5. Business Context Awareness: Aligning LLM responses with specific business contexts.\\n6. Service Scalability: Managing increased capacity while controlling costs.\\n7. Security and Governance: Implementing protocols for data security, privacy, and governance.\\n1.9.4\\nUse Cases and Examples\\n1. Question and Answer Chatbots: Integrate LLMs with chatbots to generate accurate answers\\nfrom company documents, enhancing customer support.\\n2. Search Augmentation: Enhance search engines with LLM-generated answers for more accurate\\ninformational queries.\\n3. Knowledge Engine: Use LLMs to answer questions related to internal functions, such as HR\\nand compliance, using company data.\\n1.9.5\\nConsiderations for Choosing Between RAG and Fine-Tuning\\nWhen considering external data access, RAG is likely a superior option for applications needing to access\\nexternal data sources. Fine-tuning, on the other hand, is more suitable if you require the model to ad-\\njust its behaviour, and writing style, or incorporate domain-specific knowledge. In terms of suppressing\\nhallucinations and ensuring accuracy, RAG systems tend to perform better as they are less prone to gen-\\nerating incorrect information. If you have ample domain-specific, labelled training data, fine-tuning can\\nresult in a more tailored model behaviour, whereas RAG systems are robust alternatives when such data\\nis scarce. RAG systems provide an advantage with dynamic data retrieval capabilities for environments\\nwhere data frequently updates or changes. Additionally, it is crucial to ensure the transparency and\\ninterpret ability of the model’s decision-making process. In that case, RAG systems offer insight that is\\ntypically not available in models that are solely fine-tuned. Figure1.5 illustrates the visual representation\\nalongside example use cases.\\n12\\nFigure 1.5: Graph comparing the model adaptation required versus the level of external knowledge needed\\nacross different scenarios, highlighting the roles of Retrieval-Augmented Generation (RAG), Fine-Tuning,\\nand their hybrid applications in various contexts such as Q&A systems, customer support automation,\\nand summarisation tasks. (adapted from [14])\\n1.10\\nObjectives of the Report\\n1.10.1\\nGoals and Scope\\nThe primary goal of this report is to conduct a comprehensive analysis of fine-tuning techniques for LLMs.\\nThis involves exploring theoretical foundations, practical implementation strategies, and challenges. The\\nreport examines various fine-tuning methodologies, their applications, and recent advancements.\\n1.10.2\\nKey Questions and Issues Addressed\\nThis report addresses critical questions surrounding fine-tuning LLMs, starting with foundational in-\\nsights into LLMs, their evolution, and significance in NLP. It defines fine-tuning, distinguishes it from\\npre-training, and emphasises its role in adapting models for specific tasks. Key objectives include en-\\nhancing model performance for targeted applications and domains.\\nThe report outlines a structured fine-tuning process, featuring a high-level pipeline with visual rep-\\nresentations and detailed stage explanations. It covers practical implementation strategies, including\\nmodel initialisation, hyperparameter definition, and fine-tuning techniques such as Parameter-Efficient\\nFine-Tuning (PEFT) and Retrieval-Augmented Generation (RAG). Industry applications, evaluation\\nmethods, deployment challenges, and recent advancements are also explored.\\n1.10.3\\nOverview of the Report Structure\\nThe rest of the report provides a comprehensive understanding of fine-tuning LLMs. The main chapters\\ninclude an in-depth look at the fine-tuning pipeline, practical applications, model alignment, evaluation\\nmetrics, and challenges. The concluding sections discuss the evolution of fine-tuning techniques, highlight\\nongoing research challenges, and provide insights for researchers and practitioners.\\n13\\nChapter 2\\nSeven Stage Fine-Tuning Pipeline\\nfor LLM\\nFine-tuning a Large Language Model (LLM) is a comprehensive process divided into seven distinct\\nstages, each essential for adapting the pre-trained model to specific tasks and ensuring optimal per-\\nformance. These stages encompass everything from initial dataset preparation to the final deployment\\nand maintenance of the fine-tuned model. By following these stages systematically, the model is refined\\nand tailored to meet precise requirements, ultimately enhancing its ability to generate accurate and\\ncontextually appropriate responses. The seven stages include Dataset Preparation, Model Initialisation,\\nTraining Environment Setup, Fine-Tuning, Evaluation and Validation, Deployment, and Monitoring and\\nMaintenance.\\nFigure 2.1 illustrates the comprehensive pipeline for fine-tuning LLMs, encompassing all necessary stages\\nfrom dataset preparation to monitoring and maintenance.\\n2.1\\nStage 1: Dataset Preparation\\nFine-tuning a Large Language Model (LLM) starts with adapting the pre-trained model for specific tasks\\nby updating its parameters using a new dataset. This involves cleaning and formatting the dataset to\\nmatch the target task, such as instruction tuning, sentiment analysis, or topic mapping. The dataset is\\ncomposed of < input, output > pairs, demonstrating the desired behaviour for the model.\\nFor example, in instruction tuning, the dataset may look like:\\n###Human: $<Input Query>$\\n###Assistant: $<Generated Output>$\\nHere, the ’Input Query’ is what the user asks, and the ’Generated Output’ is the model’s response. The\\nstructure and style of these pairs can be adjusted based on the specific needs of the task.\\n2.2\\nStage 2: Model Initialisation\\nModel initialisation is the process of setting up the initial parameters and configurations of the LLM\\nbefore training or deploying it. This step is crucial for ensuring the model performs optimally, trains\\nefficiently, and avoids issues such as vanishing or exploding gradients.\\n2.3\\nStage 3: Training Environment Setup\\nSetting up the training environment for LLM fine-tuning involves configuring the necessary infrastructure\\nto adapt a pre-existing model for specific tasks. This includes selecting relevant training data, defining the\\nmodel’s architecture and hyperparameters, and running training iterations to adjust the model’s weights\\nand biases.\\nThe aim is to enhance the LLM’s performance in generating accurate and contextually\\nappropriate outputs tailored to specific applications, like content creation, translation, or sentiment\\nanalysis. Successful fine-tuning relies on careful preparation and rigorous experimentation.\\n14\\nFigure 2.1: A comprehensive pipeline for fine-tuning Large Language Models (LLMs), illustrating the\\nseven essential stages: Dataset Preparation, Model Initialisation, Training Environment Setup, Fine-\\nTuning, Evaluation and Validation, Deployment, and Monitoring and Maintenance. Each stage plays\\na crucial role in adapting the pre-trained model to specific tasks and ensuring optimal performance\\nthroughout its lifecycle.\\n2.4\\nStage 4: Partial or Full Fine-Tuning\\nThis stage involves updating the parameters of the LLM using a task-specific dataset. Full fine-tuning up-\\ndates all parameters of the model, ensuring comprehensive adaptation to the new task. Alternatively, Half\\nfine-tuning (HFT) [15] or Parameter-Efficient Fine-Tuning (PEFT) approaches, such as using adapter\\nlayers, can be employed to partially fine-tune the model. This method attaches additional layers to the\\npre-trained model, allowing for efficient fine-tuning with fewer parameters, which can address challenges\\nrelated to computational efficiency, overfitting, and optimisation.\\n2.5\\nStage 5: Evaluation and Validation\\nEvaluation and validation involve assessing the fine-tuned LLM’s performance on unseen data to ensure\\nit generalises well and meets the desired objectives. Evaluation metrics, such as cross-entropy, measure\\nprediction errors, while validation monitors loss curves and other performance indicators to detect issues\\nlike overfitting or underfitting.\\nThis stage helps guide further fine-tuning to achieve optimal model\\nperformance.\\n15\\n2.6\\nStage 6: Deployment\\nDeploying an LLM means making it operational and accessible for specific applications. This involves\\nconfiguring the model to run efficiently on designated hardware or software platforms, ensuring it can\\nhandle tasks like natural language processing, text generation, or user query understanding. Deployment\\nalso includes setting up integration, security measures, and monitoring systems to ensure reliable and\\nsecure performance in real-world applications.\\n2.7\\nStage 7: Monitoring and Maintenance\\nMonitoring and maintaining an LLM after deployment is crucial to ensure ongoing performance and\\nreliability.\\nThis involves continuously tracking the model’s performance, addressing any issues that\\narise, and updating the model as needed to adapt to new data or changing requirements.\\nEffective\\nmonitoring and maintenance help sustain the model’s accuracy and effectiveness over time.\\n16\\nChapter 3\\nStage 1: Data Preparation\\n3.1\\nSteps Involved in Data Preparation\\n3.1.1\\nData Collection\\nThe first step in data preparation is to collect data from various sources. These sources can be in any\\nformat such as CSV, web pages, SQL databases, S3 storage, etc. Python provides several libraries to\\ngather the data efficiently and accurately. Table 3.1 presents a selection of commonly used data formats\\nalong with the corresponding Python libraries used for data collection.\\n3.1.2\\nData Preprocessing and Formatting\\nData preprocessing and formatting are crucial for ensuring high-quality data for fine-tuning. This step\\ninvolves tasks such as cleaning the data, handling missing values, and formatting the data to match the\\nspecific requirements of the task. Several libraries assist with text data processing and Table 3.2 contains\\nsome of the most commonly used data preprocessing libraries in python.\\n3.1.3\\nHandling Data Imbalance\\nHandling imbalanced datasets is crucial for ensuring balanced performance across all classes. Several\\ntechniques and strategies are employed:\\n1. Over-sampling and Under-sampling:\\nTechniques like SMOTE (Synthetic Minority Over-\\nsampling Technique) generate synthetic examples to achieve balance.\\nPython Library: imbalanced-learn\\nDescription: imbalanced-learn provides various methods to deal with imbalanced datasets, in-\\ncluding oversampling techniques like SMOTE.\\n2. Adjusting Loss Function: Modify the loss function to give more weight to the minority class,\\nsetting class weights inversely proportional to the class frequencies.\\n3. Focal Loss: A variant of cross-entropy loss that adds a factor to down-weight easy examples and\\nfocus training on hard negatives.\\nPython Library: focal loss\\nDescription: The focal loss package provides robust implementations of various focal loss func-\\ntions, including BinaryFocalLoss and SparseCategoricalFocalLoss.\\n4. Cost-sensitive Learning: Incorporating the cost of misclassifications directly into the learning\\nalgorithm, assigning a higher cost to misclassifying minority class samples.\\n5. Ensemble Methods: Using techniques like bagging and boosting to combine multiple models\\nand handle class imbalance.\\nPython Library: sklearn.ensemble\\nDescription: scikit-learn provides robust implementations of various ensemble methods, including\\nbagging and boosting.\\n17\\nData Format\\nPython\\nLi-\\nbrary\\nDescription\\nLibrary Link\\nCSV Files\\npandas\\npandas is a powerful library for data ma-\\nnipulation and analysis. It provides the\\nread csv function for easy and efficient\\nreading of CSV files into DataFrame ob-\\njects.\\nIt also supports reading data in\\nExcel, JSON, and more.\\npandas documenta-\\ntion\\nWeb Pages\\nBeautifulSoup\\nand requests\\nBeautifulSoup is a library for parsing\\nHTML and XML documents. Combined\\nwith requests for sending HTTP re-\\nquests, it enables data extraction from\\nweb pages, essential for web scraping\\ntasks.\\nBeautifulSoup\\ndocumentation,\\nrequests documen-\\ntation\\nSQL Databases\\nSQLAlchemy\\nSQLAlchemy\\nis\\na\\nSQL\\ntoolkit\\nand\\nObject-Relational Mapping (ORM) li-\\nbrary for Python, providing a full suite\\nof enterprise-level persistence patterns.\\nSQLAlchemy docu-\\nmentation\\nS3 Storage\\nboto3\\nboto3 is the Amazon Web Services\\n(AWS) SDK for Python, allowing devel-\\nopers to use services like Amazon S3 and\\nEC2. It enables interaction with AWS\\nservices, including uploading, download-\\ning, and managing S3 bucket files.\\nboto3\\ndocumenta-\\ntion\\nData\\nIntegra-\\ntion\\nRapidMiner\\nRapidMiner is a comprehensive envi-\\nronment for data preparation, machine\\nlearning, and predictive analytics, allow-\\ning efficient processing and transforma-\\ntion of raw data into actionable insights.\\nRapidMiner\\ndocu-\\nmentation\\nData Cleaning\\nTrifacta\\nWran-\\ngler\\nTrifacta Wrangler focuses on simplify-\\ning and automating data wrangling pro-\\ncesses, transforming raw data into clean\\nand structured formats.\\nTrifacta\\nWrangler\\ndocumentation\\nTable 3.1: Python libraries and tools for data collection and integration in various formats, providing\\nan overview of commonly used libraries, their functions, and links to their official documentation for\\nefficient data management and processing.\\n6. Stratified Sampling: Ensuring that each mini-batch during training contains an equal or pro-\\nportional representation of each class.\\nPython Library: sklearn.model selection.StratifiedShuffleSplit\\nDescription: scikit-learn offers tools for stratified sampling, ensuring balanced representation\\nacross classes.\\n7. Data Cleaning: Removing noisy and mislabelled data, which can disproportionately affect the\\nminority class.\\nPython Library: pandas.DataFrame.sample\\nDescription: pandas provides methods for sampling data from DataFrames, useful for data clean-\\ning and preprocessing.\\n8. Using Appropriate Metrics: Metrics like Precision-Recall AUC, F1-score, and Cohen’s Kappa\\nare more informative than accuracy when dealing with imbalanced datasets.\\nPython Library: sklearn.metrics\\nDescription: scikit-learn offers a comprehensive set of tools for evaluating the performance of\\nclassification models, particularly with imbalanced datasets.\\n18\\nLibrary Name\\nData Preprocessing Options\\nLink\\nspaCy\\nspaCy provides robust capabilities for text prepro-\\ncessing, including tokenization, lemmatization, and\\nefficient sentence boundary detection.\\nspaCy documentation\\nNLTK\\nNLTK offers a comprehensive set of tools for data\\npreprocessing, such as tokenization, stemming, and\\nstop word removal.\\nNLTK documentation\\nHuggingFace\\nHuggingFace provides extensive capabilities for\\ntext preprocessing through its transformers library,\\nincluding functionalities for tokenization and sup-\\nport for various pre-trained models.\\nHuggingFace documentation\\nKNIME\\nKNIME Analytics Platform allows visual workflow\\ndesign for data integration, preprocessing, and ad-\\nvanced manipulations like text mining and image\\nanalysis.\\nKNIME documentation\\nTable 3.2: Outline of Python libraries commonly used for text data preprocessing, including spaCy,\\nNLTK, HuggingFace, and KNIME. It details the specific preprocessing options offered by each library\\nand provides links to their official documentation for users seeking more in-depth guidance on their use.\\n3.1.4\\nSplitting Dataset\\nSplitting the dataset for fine-tuning involves dividing it into training and validation sets, typically using\\nan 80:20 ratio. Different techniques include:\\n1. Random Sampling: Selecting a subset of data randomly to create a representative sample.\\nPython Library: sklearn.model selection.train test split\\n2. Stratified Sampling: Dividing the dataset into subgroups and sampling from each to maintain\\nclass balance.\\nPython Library: sklearn.model selection.StratifiedShuffleSplit\\n3. K-Fold Cross Validation: Splitting the dataset into K folds and performing training and vali-\\ndation K times.\\nPython Library: sklearn.model selection.KFold\\n4. Leave-One-Out Cross Validation: Using a single data point as the validation set and the rest\\nfor training, repeated for each data point.\\nPython Library: sklearn.model selection.LeaveOneOut\\nFurther details can be found in scikit-learn’s documentation on model selection.\\n3.2\\nExisting and Potential Research Methodologies\\n3.2.1\\nData Annotation\\nData annotation involves labelling or tagging textual data with specific attributes relevant to the model’s\\ntraining objectives.\\nThis process is crucial for supervised learning tasks and greatly influences the\\nperformance of the fine-tuned model. Recent research highlights various approaches to data annotation:\\n• Human Annotation: Manual annotation by human experts remains a gold standard due to its\\naccuracy and context understanding. However, it is time-consuming and costly for large datasets\\n[16]. Tools like Excel, Prodigy1, and Innodata2 facilitate this process.\\n• Semi-automatic Annotation: Combining machine learning algorithms with human review to\\ncreate labelled datasets more efficiently. This approach balances efficiency and accuracy. Tools\\nlike Snorkel3 use weak supervision to generate initial labels, which are then refined by human\\nannotators [17].\\n1https://prodi.gy\\n2https://innodata.com/\\n3https://snorkel.ai/\\n19\\n• Automatic Annotation: Fully automated annotation leverages machine learning algorithms to\\nlabel data without human intervention, offering scalability and cost-effectiveness.\\nServices like\\nAmazon SageMaker Ground Truth4 utilise machine learning to automate data labelling, al-\\nthough the accuracy may vary depending on the complexity of the task [18].\\n3.2.2\\nData Augmentation\\nData Augmentation (DA) techniques expand training datasets artificially to address data scarcity and\\nimprove model performance. Advanced techniques often used in NLP include:\\n• Word Embeddings: Using word embeddings like Word2Vec and GloVe to replace words with\\ntheir semantic equivalents, thereby generating new data instances [19, 20].\\n• Back Translation: Translating text to another language and then back to the original language\\nto create paraphrased data. This technique helps in generating diverse training samples [21]. Tools\\nlike Google Translate API5 are commonly used for this purpose.\\n• Adversarial Attacks: Generating augmented data through adversarial examples that slightly\\nmodify the original text to create new training samples while preserving the original meaning [22].\\nLibraries like TextAttack6 provide frameworks for such augmentations.\\n• NLP-AUG7: This library offers a variety of augmenters for character, word, sentence, audio, and\\nspectrogram augmentation, enhancing dataset diversity.\\n3.2.3\\nSynthetic Data Generation using LLMs\\nLarge Language Models (LLMs) can generate synthetic data through innovative techniques such as:\\n• Prompt Engineering: Crafting specific prompts to guide LLMs like GPT-3 in generating relevant\\nand high-quality synthetic data [23].\\n• Multi-Step Generation: Employing iterative generation processes where LLMs generate initial\\ndata that is refined through subsequent steps [24]. This method can produce high-quality synthetic\\ndata for various tasks, including summarising and bias detection.\\nIt is crucial to verify the accuracy and relevance of synthetic data generated by LLMs before using\\nthem for fine-tuning processes [25].\\n3.3\\nChallenges in Data Preparation for Fine-Tuning LLMs\\nKey challenges in data preparation include:\\n1. Domain Relevance: Ensuring that the data is relevant to the specific domain for accurate model\\nperformance. Mismatched domain data can lead to poor generalisation and inaccurate outputs\\n[26].\\n2. Data Diversity: Including diverse and well-balanced data to prevent model biases and improve\\ngeneralisation. A lack of diversity can cause the model to perform poorly on underrepresented\\nscenarios [27].\\n3. Data Size: Managing and processing large datasets, with at least 1000 samples recommended for\\neffective fine-tuning. However, large datasets pose challenges in terms of storage, computational\\nrequirements, and processing time.\\n4. Data Cleaning and Preprocessing: Removing noise, errors, and inconsistencies are critical for\\nproviding clean inputs to the model. Poorly preprocessed data can degrade model performance\\nsignificantly.\\n4https://aws.amazon.com/sagemaker/groundtruth/\\n5https://translate.google.com/?sl=auto&tl=en&op=translate\\n6https://github.com/QData/TextAttack\\n7https://github.com/makcedward/nlpaug\\n20\\n5. Data Annotation: Ensuring precise and consistent labelling is essential for tasks requiring la-\\nbelled data. Inconsistent annotation can lead to unreliable model predictions.\\n6. Handling Rare Cases: Adequately representing rare but important instances in the dataset to\\nensure the model can generalise to less frequent but critical scenarios.\\n7. Ethical Considerations: Scrutinising data for harmful or biased content to prevent unintended\\nconsequences. Ethical data handling includes removing biases and ensuring privacy [28].\\n3.4\\nAvailable LLM Fine-Tuning Datasets\\nFor a comprehensive list of datasets suitable for fine-tuning LLMs, refer to resources like LLMXplorer,\\nwhich provides domain and task-specific datasets.\\n3.5\\nBest Practices\\n3.5.1\\nHigh-Quality Data Collection\\nEnsuring high-quality, diverse, and representative data is critical. Leveraging curated sources and en-\\nsuring comprehensive coverage across different scenarios enhances model robustness [29].\\nTools like\\nDataRobot Paxata8 and KNIME Analytics Platform9 offer robust data profiling and transforma-\\ntion capabilities.\\n3.5.2\\nEffective Data Preprocessing\\nProper data preprocessing is essential for model performance. Utilising libraries like spaCy, NLTK, and\\nHuggingFace Transformers can streamline preprocessing tasks. Platforms like Trifacta Wrangler\\nand RapidMiner automate data cleaning tasks, improving efficiency and ensuring consistency [30].\\n3.5.3\\nManaging Data Imbalance\\nAddressing data imbalance is crucial.\\nTechniques like over-sampling, under-sampling, and SMOTE\\nhelp balance datasets. Libraries like imbalanced-learn and ensemble methods in scikit-learn provide\\nrobust tools for managing imbalanced datasets [31].\\n3.5.4\\nAugmenting and Annotating Data\\nData augmentation and annotation improve model robustness. Tools like NLP-AUG, TextAttack,\\nand Snorkel offer sophisticated capabilities for creating diverse and well-labelled datasets [32, 33].\\n3.5.5\\nEthical Data Handling\\nEnsuring ethical data handling involves thorough scrutiny for biases and privacy concerns. Implement-\\ning privacy-preserving techniques and filtering harmful content is critical. Services like Amazon Sage-\\nMaker Ground Truth ensure scalable and secure data annotation [34].\\n3.5.6\\nRegular Evaluation and Iteration\\nContinuous evaluation and iteration of the data preparation pipeline help maintain data quality and\\nrelevance. Leveraging feedback loops and performance metrics ensures ongoing improvements and adap-\\ntation to new data requirements.\\nBy integrating these best practices, researchers and practitioners can enhance the effectiveness of LLM\\nfine-tuning, ensuring robust and reliable model performance.\\n8https://www.datarobot.com/platform/preparation/\\n9https://www.knime.com/\\n21\\nChapter 4\\nStage 2: Model Initialisation\\n4.1\\nSteps Involved in Model Initialisation\\nFigure 4.1: Sequential steps involved in Initialising a Large Language Model (LLM), illustrating the\\nprocess from setting up the environment to executing tasks. Each step is critical for ensuring that the\\nLLM is correctly configured and ready for operation. This includes installing necessary dependencies,\\nimporting libraries, selecting and downloading the appropriate language model from a repository, and\\nfinally, loading the model to perform specific tasks.\\n1. Set Up the Environment: Configure your environment, such as setting up GPU/TPU usage if\\navailable, which can significantly speed up model loading and inference.\\n2. Install the Dependencies: Ensure that all necessary software and libraries are installed. This\\ntypically includes package managers like pip and frameworks like PyTorch or TensorFlow.\\n22\\n3. Import the Libraries: Import the required libraries in your script or notebook. Common libraries\\ninclude transformers from Hugging Face, torch for PyTorch, and other utility libraries.\\n4. Choose the Language Model: Select the appropriate pre-trained language model based on your\\ntask requirements. This could be models like BERT, GPT-3, or others available on platforms like\\nHugging Face’s Model Hub.\\n5. Download the Model from the Repository: Use the chosen framework’s functions to download\\nthe pre-trained model from an online repository. For instance, using transformers, you might use\\nAutoModel.from pretrained(’model name’).\\n6. Load the Model in the Memory: Load the model into memory, ready for inference or further\\nfine-tuning. This step ensures the model weights are initialised and ready for use.\\n7. Execute Tasks: Perform the desired tasks using the loaded model. This could involve making\\npredictions, generating text, or fine-tuning the model on a new dataset.\\n4.2\\nTools and Libraries for Model Initialisation\\nPython offers a wide range of libraries for Initialising large language models, providing access to both\\nopen and closed-source models. Here are some notable libraries:\\n1. Python Library: HuggingFace\\nDescription: HuggingFace is renowned for its support of numerous pre-trained large language\\nmodels, ranging from Phi-3 mini to Llama-3 70B. The transformers library, part of HuggingFace,\\nenables users to access these models via classes such as AutoModelForCausalLM. This library\\nsupports loading fine-tuned models as well as 4-bit quantised models. Additionally, the transformers\\nlibrary includes the ”pipeline” feature, making it easy to use pre-trained models for various tasks\\n[35].\\n2. Python Framework: PyTorch\\nDescription: PyTorch offers comprehensive tools and libraries for Initialising and fine-tuning\\nlarge language models. It provides a flexible and efficient platform for building and deploying deep\\nlearning models. HuggingFace’s transformers library bridges the gap between PyTorch and other\\nframeworks, enhancing its usability for state-of-the-art language models [36].\\n3. Python Framework: TensorFlow\\nDescription: TensorFlow also provides extensive tools and libraries for Initialising and fine-tuning\\nlarge language models. Similar to PyTorch, it benefits from the HuggingFace transformers library,\\nwhich provides a versatile and user-friendly API and interface for working with the latest advance-\\nments in large language models [37].\\n23\\n4.3\\nChallenges in Model Initialisation\\nChallenge\\nDescription\\nAlignment with the\\nTarget Task\\nIt’s essential that the pre-trained model closely aligns with your specific\\ntask or domain. This initial alignment serves as a solid foundation for\\nfurther fine-tuning efforts, leading to improved efficiency and results [38].\\nUnderstanding the\\nPre-trained Model\\nBefore making a selection, it’s crucial to thoroughly comprehend the\\narchitecture, capabilities, limitations, and the tasks the model was orig-\\ninally trained on. Without this understanding, fine-tuning efforts may\\nnot yield the desired outcomes [23].\\nAvailability and\\nCompatibility\\nCareful consideration of a model’s documentation, license, maintenance,\\nand update frequency is necessary to avoid potential issues and ensure\\nsmooth integration into your application.\\nModel Architecture\\nNot all models excel at every task.\\nEach model architecture has its\\nstrengths and weaknesses, so selecting one aligned with your specific\\ntask is essential for favourable outcomes [39].\\nResource Constraints\\nLoading pre-trained LLMs is resource-heavy and requires more compu-\\ntation. These models need high-performance CPUs and GPUs and a\\nsignificant amount of disk space. For instance, the Llama 3 8B model\\nrequires a minimum of 16GB of memory to load and run the inference.\\nPrivacy\\nPrivacy and confidentiality are crucial factors when selecting a large lan-\\nguage model (LLM). Many businesses prefer not to share their data\\nwith external LLM providers.\\nIn such instances, hosting an LLM on\\nlocal servers or using pre-trained LLMs available through private cloud\\nproviders can be viable solutions. These approaches ensure that data\\nremains within the company’s premises, thereby preserving privacy and\\nconfidentiality.\\nCost and Maintenance\\nHosting LLMs on local servers entails significant time and expense for\\nsetup and ongoing maintenance. Conversely, utilising cloud vendors al-\\nleviates concerns about resource maintenance but incurs monthly billing\\ncosts. These charges are typically based on factors such as model size\\nand the volume of requests per minute.\\nModel Size and\\nQuantisation\\nutilising a pre-trained model with high memory consumption can still be\\nviable by employing its quantised version. Through quantisation, pre-\\ntrained weights can be loaded with reduced precision, typically 4-bit or\\n8-bit floating point, substantially diminishing parameter volume while\\nmaintaining considerable accuracy [40].\\nPre-training Datasets\\nExamine the datasets used for pre-training to gauge the model’s under-\\nstanding of language. These are important as there are models available\\nspecifically for performing code generation, and we do not want to use\\nthose models for finance text classification [41].\\nBias Awareness\\nBe vigilant regarding potential biases in pre-trained models, especially if\\nunbiased predictions are required. The bias awareness can be evaluated\\nby testing different models and backtracking the datasets used for pre-\\ntraining [42].\\nTable 4.1: Comprehensive Overview of Challenges in Initialising a Large Language Model (LLM). This\\ntable highlights critical considerations, such as the importance of aligning pre-trained models with specific\\ntasks, understanding model architecture and compatibility, managing resource constraints, and ensuring\\ndata privacy. Additionally, it discusses the challenges related to cost, maintenance, and the complexities\\nof model size, quantisation, and bias awareness. Each challenge is associated with specific references to\\nensure thorough understanding and proper model deployment.\\n4.4\\nTutorials\\n1. Summarisation using Llama 3\\n24\\n2. HuggingFace tutorial for getting started with LLMs\\n3. PyTorch tutorial for fine-tuning models\\n4. TensorFlow tutorial for transformer models\\n25\\nChapter 5\\nStage 3: Training Setup\\n5.1\\nSteps Involved in Training Setup\\n1. Setting up the training environment: When setting up the environment for training an LLM,\\nit is crucial to configure high-performance hardware, such as GPUs or TPUs, and ensure proper\\ninstallation of necessary software components like CUDA, cuDNN, and deep learning frameworks\\nsuch as PyTorch or TensorFlow. Verify hardware recognition and compatibility with the software to\\nleverage computational power effectively, reducing training time and improving model performance.\\n2. Defining the Hyper-parameters: When defining hyperparameters for fine-tuning an LLM, it is\\nessential to carefully tune key parameters such as learning rate, batch size, and epochs to optimise\\nthe model’s performance.\\n3. Initialising Optimisers and Loss Functions: When initialising optimisers and loss functions\\nfor fine-tuning an LLM, it is crucial to select the appropriate optimiser to efficiently update the\\nmodel’s weights and the correct loss function to measure model performance [43].\\n5.2\\nSetting up Training Environment\\nWhen fine-tuning a large language model (LLM), the computational environment plays a crucial role in\\nensuring efficient training. To achieve optimal performance, it’s essential to configure the environment\\nwith high-performance hardware such as GPUs (Graphics Processing Units) or TPUs (Tensor Processing\\nUnits). GPUs, such as the NVIDIA A100 or V100, are widely used for training deep learning models\\ndue to their parallel processing capabilities. For larger-scale operations, TPUs offered by Google Cloud\\ncan provide even greater acceleration [44].\\nFirst, ensure that your system or cloud environment has the necessary hardware installed. For GPUs,\\nthis involves setting up CUDA1 (Compute Unified Device Architecture) and cuDNN2 (CUDA Deep Neu-\\nral Network library) from NVIDIA, which are essential for enabling GPU acceleration. For TPU usage,\\nyou would typically set up a Google Cloud environment with TPU instances, which includes configuring\\nthe TPU runtime in your training scripts.\\nVerify that your hardware is correctly recognised and utilised by your deep learning frameworks. In\\nPyTorch, for instance, you can check GPU availability with torch.cuda.is available(). Properly setting\\nup and testing the hardware ensures that the training process can leverage the computational power\\neffectively, reducing training time and improving model performance [36].\\nWhen fine-tuning an LLM, both software and hardware considerations are paramount to ensure a smooth\\nand efficient training process. On the software side, you need a compatible deep learning framework like\\nPyTorch or TensorFlow. These frameworks have extensive support for LLMs and provide utilities for\\nefficient model training and evaluation. Installing the latest versions of these frameworks, along with\\nany necessary dependencies, is crucial for leveraging the latest features and performance improvements\\n1https://developer.nvidia.com/cuda-toolkit\\n2https://developer.nvidia.com/cudnn\\n26\\n[45].\\nAdditionally, use libraries like Hugging Face’s transformers to simplify the process of loading pre-trained\\nmodels and tokenizers. This library is particularly well-suited for working with various LLMs and offers\\na user-friendly interface for model fine-tuning. Ensure that all software components, including libraries\\nand dependencies, are compatible with your chosen framework and hardware setup [35].\\nOn the hardware side, consider the memory requirements of the model and your dataset. LLMs typ-\\nically require substantial GPU memory, so opting for GPUs with higher VRAM (e.g., 16GB or more)\\ncan be beneficial. If your model is exceptionally large or if you are training with very large datasets,\\ndistributed training across multiple GPUs or TPUs might be necessary. This requires a careful setup of\\ndata parallelism or model parallelism techniques to efficiently utilise the available hardware [46].\\nLastly, ensure robust cooling and power supply for your hardware, as training LLMs can be resource-\\nintensive, generating significant heat and requiring consistent power. Proper hardware setup not only\\nenhances training performance but also prolongs the lifespan of your equipment [47].\\n5.3\\nDefining Hyperparameters\\nKey hyperparameters like learning rate, batch size, epochs are crucial for enhancing the model’s perfor-\\nmance and obtaining superior outcomes. This process entails adjusting hyperparameters and training\\nsettings to align with your particular use case. Below are the key hyperparameters:\\n1. Learning Rate: Fine-tuning an LLM involves using optimisation algorithms like stochastic gradi-\\nent descent (SGD). This technique estimates the error gradient for the model’s current state using\\nsamples from the training dataset and subsequently updates the model’s weights via the backprop-\\nagation of errors algorithm. The learning rate dictates the speed at which the model adapts to the\\nproblem. Smaller learning rates necessitate more training due to the minimal weight adjustments\\nper update, while larger learning rates lead to quicker changes to weights [48].\\n2. Batch Size: A batch refers to a subset of the training data used to update a model’s weights\\nduring the training process. Batch training involves dividing the entire training set into smaller\\ngroups, updating the model after processing each batch. The batch size is a hyperparameter that\\ndetermines the number of samples processed before the model parameters are updated.\\n3. Epochs: Epoch refers to a full pass through the entire training dataset. This involves a complete\\nforward and backward pass through the dataset. The dataset can be processed as a single batch\\nor divided into multiple smaller batches. An epoch is considered complete once the model has\\nprocessed all batches and updated its parameters based on the calculated loss.\\n5.3.1\\nMethods for Hyperparameter Tuning\\nLLM hyperparameter tuning involves adjusting various hyperparameters during the training process\\nto identify the optimal combination that yields the best output. This process often entails significant\\ntrial and error, meticulously tracking each hyperparameter adjustment, and recording the resulting\\nperformance. Conducting this manually can be highly time-consuming. To address this, automated\\nhyperparameter tuning methods have been developed to streamline the process. The three most common\\nmethods of automated hyperparameter tuning are random search, grid search, and Bayesian optimisation:\\n1. Random Search: This method randomly selects and evaluates combinations of hyperparameters\\nfrom a specified range. It is a straightforward and efficient approach capable of exploring a large\\nparameter space. However, it may not always find the optimal combination of hyperparameters\\nand can be computationally expensive [49].\\n2. Grid Search: Unlike random search, grid search exhaustively evaluates every possible combination\\nof hyperparameters from a given range.\\nAlthough resource-intensive, this systematic approach\\nensures that the optimal set of hyperparameters is found [50].\\n27\\n3. Bayesian Optimisation: This method uses a probabilistic model to predict the performance of\\ndifferent hyperparameters and selects the best ones accordingly. It is an efficient method that can\\nhandle large parameter spaces better and is less resource-intensive than grid search. However, it is\\nmore complex to set up and may be less reliable in identifying the optimal set of hyperparameters\\ncompared to grid search.\\n4. Automated hyperparameter tuning: This facilitates the development of multiple language\\nmodels, each with a unique combination of hyperparameters. By training these models on the same\\ndataset, it becomes possible to compare their outputs and determine which configuration is best\\nsuited for the desired use case. Additionally, models tuned with different sets of hyperparameters\\ncan be tailored to various specific applications.\\n5.4\\nInitialising Optimisers and Loss Functions\\nChoosing the right optimiser and loss function is crucial for training and fine-tuning LLMs.\\nBelow\\nare descriptions of some commonly used optimisation algorithms, their advantages, disadvantages, and\\nappropriate use cases:\\n5.4.1\\nGradient Descent\\nGradient Descent is a fundamental optimisation algorithm used to minimise cost functions in machine\\nlearning models. It aims to find the optimal parameters for a neural network.\\nHow it Works: Gradient Descent iteratively updates model parameters in the direction of the\\nnegative gradient of the cost function. It calculates gradients for each parameter and applies updates\\nacross all data points until convergence. This method utilises the entire dataset to calculate gradients,\\noften requiring a fixed learning rate and being sensitive to the scale of data and learning rate choice.\\nPros:\\n• Simple and easy to implement.\\n• Intuitive and easy to understand.\\n• Converges to the global minimum for convex functions.\\n• Suitable for small-scale problems.\\nCons:\\n• Computationally expensive on large datasets.\\n• May get stuck in local minima.\\n• Requires a large number of iterations.\\n• Sensitive to the choice of learning rate.\\nWhen to Use: Gradient Descent is best used for small datasets where gradient computation is\\ncheap and simplicity and clarity are preferred.\\n5.4.2\\nStochastic Gradient Descent (SGD)\\nStochastic Gradient Descent (SGD) is a variant of Gradient Descent that focuses on reducing computation\\nper iteration.\\nHow it Works: SGD updates parameters using a single or few data points at each iteration, intro-\\nducing randomness in updates. It reduces the computational burden per iteration and often converges\\nfaster than batch Gradient Descent. However, it requires a smaller learning rate due to higher variance\\nand benefits from momentum to stabilise updates.\\nPros:\\n• Fast and handles large datasets well.\\n• Efficient memory usage.\\n28\\n• Simple and easy to implement.\\n• Can escape local minima due to noise.\\nCons:\\n• High variance in updates can lead to instability.\\n• Can overshoot the minimum.\\n• Sensitive to the choice of learning rate.\\n• Can be slower to converge compared to batch methods.\\nWhen to Use: SGD is ideal for large datasets, incremental learning scenarios, and real-time learning\\nenvironments where computational resources are limited.\\n5.4.3\\nMini-batch Gradient Descent\\nMini-batch Gradient Descent combines the efficiency of SGD and the stability of batch Gradient Descent,\\noffering a compromise between batch and stochastic approaches.\\nHow it Works: It splits data into small batches and updates parameters using gradients averaged\\nover each mini-batch. This reduces variance compared to SGD and is more efficient than batch Gradient\\nDescent, helping in generalising the updates.\\nPros:\\n• Balances between efficiency and stability.\\n• More generalisable updates.\\n• Reduces the variance of parameter updates.\\n• Provides a compromise between SGD and batch.\\nCons:\\n• Requires tuning of batch size.\\n• Can still be computationally expensive for very large datasets.\\n• More complex implementation.\\n• Can require more iterations than full-batch Gradient Descent.\\nWhen to Use: Mini-batch Gradient Descent is suitable for most deep learning tasks, especially\\nwhen working with moderate to large datasets.\\n5.4.4\\nAdaGrad\\nAdaptive Gradient Algorithm (AdaGrad) is designed for sparse data and high-dimensional models, ad-\\njusting learning rates to improve performance on sparse data.\\nHow it Works: AdaGrad adapts the learning rate for each parameter based on historical gradi-\\nent information, accumulating squared gradients. This approach prevents large updates for frequent\\nparameters and helps in dealing with sparse features.\\nPros:\\n• Adapts learning rate for each parameter.\\n• Good for sparse data.\\n• No need to manually tune learning rates.\\n• Works well with high-dimensional data.\\nCons:\\n• Learning rate can diminish to zero, stopping learning.\\n29\\n• May require more tuning for convergence.\\n• Accumulation of squared gradients can lead to overly small learning rates.\\n• Can slow down significantly.\\nWhen to Use: AdaGrad is useful for sparse datasets like text and images where learning rates need\\nto adapt to feature frequency.\\n5.4.5\\nRMSprop\\nRoot Mean Square Propagation (RMSprop) is an adaptive learning rate method designed to perform\\nbetter on non-stationary and online problems.\\nHow it Works: RMSprop modifies AdaGrad by using a moving average of squared gradients to\\nadapt learning rates based on recent gradient magnitudes. It maintains a running average of squared\\ngradients to help in maintaining steady learning rates.\\nPros:\\n• Addresses the diminishing learning rate problem of AdaGrad.\\n• Adapts learning rate based on recent gradients.\\n• Effective for recurrent neural networks.\\n• More robust against non-stationary targets.\\nCons:\\n• Can still get stuck in local minima on non-convex problems.\\n• Requires hyperparameter tuning.\\n• Requires careful tuning of the decay rate.\\n• Can be sensitive to the initial learning rate.\\nWhen to Use: RMSprop is best for non-convex optimisation problems, training RNNs and LSTMs,\\nand dealing with noisy or non-stationary objectives.\\n5.4.6\\nAdaDelta\\nAdaptive Delta (AdaDelta) improves on AdaGrad and RMSprop, focusing on adaptive learning rates\\nwithout diminishing too quickly.\\nHow it Works: AdaDelta eliminates the need for a default learning rate by using a moving window\\nof gradient updates. It adapts learning rates based on recent gradient magnitudes to ensure consistent\\nupdates even with sparse gradients.\\nPros:\\n• Eliminates the need to set a default learning rate.\\n• Addresses the diminishing learning rate issue.\\n• Does not require manual tuning of the learning rate.\\n• Handles gradient sparsity well.\\nCons:\\n• More complex than RMSprop and AdaGrad.\\n• Can have slower convergence initially.\\n• Can require more iterations to converge.\\n• Implementation can be more complex.\\nWhen to Use: AdaDelta is suitable for scenarios similar to RMSprop but is preferred when avoiding\\nmanual learning rate setting.\\n30\\n5.4.7\\nAdam\\nAdaptive Moment Estimation (Adam) combines the advantages of AdaGrad and RMSprop, making it\\nsuitable for problems with large datasets and high-dimensional spaces.\\nHow it Works: Adam uses running averages of both gradients and their squared values to com-\\npute adaptive learning rates for each parameter. It includes bias correction and often achieves faster\\nconvergence than other methods.\\nPros:\\n• Combines advantages of AdaGrad and RMSprop.\\n• Adaptive learning rates.\\n• Includes bias correction.\\n• Fast convergence.\\n• Works well with large datasets and high-dimensional spaces.\\nCons:\\n• Requires tuning of hyperparameters (though it often works well with defaults).\\n• Computationally intensive.\\n• Can lead to overfitting if not regularised properly.\\n• Requires more memory.\\nWhen to Use: Adam is widely used in most deep learning applications due to its efficiency and\\neffectiveness, particularly in complex neural network architectures.\\n5.4.8\\nAdamW\\nAdamW is an extension of Adam that includes weight decay regularisation to address overfitting issues\\npresent in Adam.\\nHow it Works: AdamW integrates L2 regularisation directly into the parameter updates, decoupling\\nweight decay from the learning rate. This improves generalisation and is suitable for fine-tuning large\\nmodels.\\nPros:\\n• Includes weight decay for better regularisation.\\n• Combines Adam’s adaptive learning rate with L2 regularisation.\\n• Improves generalisation.\\n• Reduces overfitting compared to Adam.\\nCons:\\n• Slightly more complex than Adam.\\n• Requires careful tuning of the weight decay parameter.\\n• Slightly slower than Adam due to additional computations.\\n• Requires more memory.\\nWhen to Use: AdamW is ideal for scenarios where regularisation is needed, such as preventing\\noverfitting in large models and fine-tuning pre-trained models.\\nA comprehensive collection of optimisation algorithms implemented within the PyTorch library can be\\nfound in here. The Hugging Face Transformers package also offers a variety of optimisers for initialising\\nand fine-tuning language models, available here.\\n31\\n5.5\\nChallenges in Training Setup\\n1. Ensuring compatibility and proper configuration of high-performance hardware like GPUs or TPUs\\ncan be complex and time-consuming.\\n2. Managing dependencies and versions of deep learning frameworks and libraries to avoid conflicts\\nand leverage the latest features.\\n3. Selecting an appropriate learning rate is critical, as too high a rate can cause suboptimal conver-\\ngence, while too low a rate can make the training process excessively slow.\\n4. Determining the optimal batch size that balances memory constraints and training efficiency, es-\\npecially given the large memory requirements of LLMs.\\n5. Choosing the right number of epochs to avoid underfitting or overfitting the model, requiring careful\\nmonitoring and validation.\\n6. Selecting the most suitable optimiser for the specific training task to efficiently update the model’s\\nweights.\\n7. Choosing the correct loss function to accurately measure model performance and guide the opti-\\nmisation process.\\n5.6\\nBest Practices\\n• Optimal Learning Rate: Use a lower learning rate, typically between 1e-4 to 2e-4, to ensure\\nstable convergence. A learning rate schedule, such as learning rate warm-up followed by a linear\\ndecay, can also be beneficial. This helps in initially stabilising the training and then allowing the\\nmodel to converge more accurately.\\n• Batch Size Considerations: Opt for a batch size that balances memory constraints and training\\nefficiency.\\nSmaller batch sizes can help in achieving faster convergence but may require more\\nfrequent updates. Conversely, larger batch sizes can be more memory-intensive but may lead to\\nmore stable updates. Experiment with different batch sizes to find the optimal balance for your\\nspecific use case.\\n• Save Checkpoints Regularly: Regularly save model weights at various intervals across 5-8\\nepochs to capture optimal performance without overfitting. Implement early stopping mechanisms\\nto halt training once the model performance starts to degrade on the validation set, thereby pre-\\nventing overfitting [51].\\n• Hyperparameter Tuning: Utilise hyperparameter tuning methods like grid search, random\\nsearch, and Bayesian optimisation to find the optimal set of hyperparameters.\\nTools such as\\nOptuna, Hyperopt, and Ray Tune can automate this process and help in efficiently exploring the\\nhyperparameter space [49].\\n• Data Parallelism and Model Parallelism: For large-scale training, consider using data paral-\\nlelism or model parallelism techniques to distribute the training workload across multiple GPUs or\\nTPUs. Libraries like Horovod and DeepSpeed can facilitate efficient distributed training, helping\\nto reduce training time and manage memory usage effectively [52, 53].\\n• Regular Monitoring and Logging: Implement robust monitoring and logging to track training\\nmetrics, resource usage, and potential bottlenecks. Tools like TensorBoard, Weights & Biases, and\\nMLflow can provide real-time insights into the training process, allowing for timely interventions\\nand adjustments.\\n• Handling Overfitting and Underfitting: Ensure that your model generalises well by imple-\\nmenting techniques to handle overfitting and underfitting. regularisation techniques such as L2\\nregularisation, dropout, and data augmentation can help prevent overfitting. Conversely, if your\\nmodel is underfitting, consider increasing the model complexity or training for more epochs.\\n32\\n• Use Mixed Precision Training: Mixed precision training involves using both 16-bit and 32-bit\\nfloating-point types to reduce memory usage and increase computational efficiency. This technique\\ncan significantly speed up training and reduce the required memory footprint, especially when\\nusing large models. NVIDIA’s Apex and TensorFlow’s mixed precision API provide support for\\nimplementing mixed precision training [54].\\n• Evaluate and Iterate: Continuously evaluate the model performance using a separate validation\\nset and iterate on the training process based on the results. Regularly update your training data\\nand retrain the model to keep it current with new data trends and patterns.\\n• Documentation and Reproducibility: Maintain thorough documentation of your training\\nsetup, including the hardware configuration, software environment, and hyperparameters used.\\nEnsure reproducibility by setting random seeds and providing detailed records of the training\\nprocess. This practice not only aids in debugging and further development but also facilitates\\ncollaboration and sharing of results with the broader research community.\\n33\\nChapter 6\\nStage 4: Selection of Fine-Tuning\\nTechniques and Appropriate Model\\nConfigurations\\nThis chapter focuses on selecting appropriate fine-tuning techniques and model configurations that suit\\nthe specific requirements of various tasks. Fine-tuning is a crucial stage where pre-trained models are\\nadapted to specific tasks or domains.\\n6.1\\nSteps Involved in Fine-Tuning\\nThe following steps outline the fine-tuning process, integrating advanced techniques and best practices.\\n1. Initialise the Pre-Trained Tokenizer and Model: Begin by loading the pre-trained tokenizer\\nand model. The tokenizer ensures that the input text is converted into a format the model can\\nprocess, while the pre-trained model serves as the foundation for further adaptation. Depending\\non the task, select a model that has been pre-trained on relevant data to provide a strong starting\\npoint.\\n2. Modify the Model’s Output Layer: Adjust the model’s output layer to align with the specific\\nrequirements of the target task. This may involve modifying existing layers or adding new layers.\\nFor instance, tasks like classification may require a softmax layer with the appropriate number of\\nclasses, while text generation tasks might involve changes in the decoding mechanism.\\n3. Choose an Appropriate Fine-Tuning Strategy: Select the fine-tuning strategy that best fits\\nthe task and the model architecture. Some Options include:\\n• Task-Specific Fine-Tuning: For tasks such as text summarisation, code generation, classi-\\nfication, and question answering, adapt the model using relevant datasets.\\n• Domain-Specific Fine-Tuning: Tailor the model to comprehend and generate text relevant\\nto specific domains, such as medical, financial, or legal fields.\\n• Parameter-Efficient Fine-Tuning (PEFT): Techniques like LoRA, QLoRA, and adapters\\nallow for fine-tuning with reduced computational costs by updating a small subset of model\\nparameters.\\n• Half Fine-Tuning (HFT): Balance between retaining pre-trained knowledge and learning\\nnew tasks by updating only half of the model’s parameters during each fine-tuning round.\\n4. Set Up the Training Loop: Establish the training loop, incorporating the selected fine-tuning\\nstrategy. The loop should include data loading, loss computation, backpropagation, and parameter\\nupdates.\\nWhen using PEFT methods, ensure that only the relevant parameters are updated\\nto maximise efficiency. Implement techniques like dynamic learning rates and early stopping to\\nenhance the training process.\\n34\\n5. Incorporate Techniques for Handling Multiple Tasks: If fine-tuning for multiple tasks,\\nconsider strategies like fine-tuning with multiple adapters or leveraging Mixture of Experts (MoE)\\narchitectures. These methods allow a single model to handle various tasks by utilising specialised\\nsub-networks or adapters for each task.\\n6. Monitor Performance on a Validation Set: Regularly evaluate the model’s performance on\\na validation set to ensure it generalises well to unseen data.\\nAdjust hyperparameters such as\\nlearning rate, batch size, and dropout rates based on the validation performance. Utilise advanced\\nmonitoring tools to track metrics like accuracy, loss, and overfitting.\\n7. Optimise Model Using Advanced Techniques: Employ techniques such as Proximal Policy\\nOptimisation (PPO) for reinforcement learning scenarios, or Direct Preference Optimisation (DPO)\\nfor aligning model outputs with human preferences. These techniques are particularly useful in\\nfine-tuning models for tasks requiring nuanced decision-making or human-like responses.\\n8. Prune and optimise the Model (if necessary): To deploy the model in resource-constrained\\nenvironments, consider pruning techniques to reduce its size and complexity. This involves removing\\nunnecessary parameters or components without significantly affecting performance. Utilise dynamic\\npruning methods during inference to optimise the model on-the-fly for different scenarios.\\n9. Continuous Evaluation and Iteration: Continuously evaluate the model’s performance across\\nvarious tasks using appropriate benchmarks. Iterate on the fine-tuning process, making adjustments\\nbased on performance metrics and real-world testing. This iterative approach helps in refining the\\nmodel to meet specific performance criteria.\\n6.2\\nFine-Tuning Strategies for LLMs\\n6.2.1\\nTask-Specific Fine-Tuning\\nTask-specific fine-tuning adapts large language models (LLMs) for particular downstream tasks using\\nappropriately formatted and cleaned data. Below is a summary of key tasks suitable for fine-tuning\\nLLMs, including examples of LLMs tailored to these tasks.\\nTask\\nDescription\\nKey Models\\nText Summarisation\\nCondensing long texts into coherent sum-\\nmaries while retaining key information. Ap-\\nproaches include Extractive (selecting key\\nsentences) and Abstractive summarisation\\n(generating new sentences).\\nBERTSUM, GPT-3, T5\\nCode Generation\\nAutomatically generating programming code\\nbased on natural language descriptions, par-\\ntial code snippets, or structured data inputs.\\nCodex, GPT-3, CodeBERT\\nClassification\\nCategorising text into predefined labels such\\nas Sentiment Analysis, Topic Classification,\\nand Entity Classification.\\nBERT, RoBERTa, GPT-4\\nQ&A\\nUnderstanding and generating accurate, con-\\ntextually relevant answers to natural lan-\\nguage questions.\\nBERT, GPT-3, T5\\nTable 6.1: Overview of tasks such as text summarisation, code generation, classification, and Q&A, along\\nwith their key LLMs and descriptions.\\n6.2.2\\nDomain-Specific Fine-Tuning\\nDomain-specific fine-tuning focuses on tailoring the model to comprehend and produce text relevant to\\na specific domain or industry. By fine-tuning the model on a dataset derived from the target domain,\\nit enhances the model’s contextual understanding and expertise in domain-specific tasks. Below are\\nexamples of domain-specific LLMs.\\n35\\nMedical Domain\\nModel Description: Med-PaLM 2 is trained on meticulously curated medical datasets and is capable\\nof accurately answering medical questions, achieving performance comparable to that of medical profes-\\nsionals [55].\\nBase Model: PaLM 2\\nFine-tuned Model Parameters: Not Known\\nFine-Tuning Techniques Used: Instruction fine-tuning\\nDatasets Used:\\n• MedQA\\n• MedMCQA\\n• LiveQA\\n• MedicationQA\\n• HealthSearchQA\\nResults: Med-PaLM 2 outperformed GPT-4 in several key medical benchmarks, demonstrating superior\\nperformance in handling complex medical knowledge and reasoning tasks.\\nFinance Domain\\nModel Description: FinGPT, an open-source LLM tailored for the financial sector, enhances financial\\nresearch and cooperation by promoting data accessibility and handling finance-specific issues like data\\nacquisition and quality [56].\\nBase Model: LlaMA, ChatGLM, and other Transformer Models\\nFine-tuned Model Parameters: Not Known\\nFine-Tuning Techniques Used: LoRA, Reinforcement Learning on Stock Prices (RLSP)\\nDatasets Used:\\n• Financial News (Reuters, CNBC, Yahoo Finance)\\n• Social Media (Twitter, Facebook, Reddit, Weibo)\\n• Regulatory Filings (e.g., SEC filings)\\n• Trends (Seeking Alpha, Google Trends)\\n• Academic Datasets\\nResults: Not Applicable\\nLegal Domain\\nModel Description: LAWGPT, the first open-source model specifically designed for Chinese legal\\napplications, demonstrates superior capability in handling Chinese legal tasks [57].\\nBase Model: Chinese Alpaca Plus 7B base model\\nFine-tuned Model Parameters: Not Known\\nFine-Tuning Techniques Used: LoRA with Alpaca template\\nDatasets Used:\\n• Open-source dataset: 200,000 examples containing crime type prediction and crime consultation\\ntasks.\\n• JEC-QA dataset: 20,000 examples containing legal question answering tasks.\\n• Constructed legal dataset: 80,000 examples, refined from open-source and JEC-QA datasets using\\nChatGPT.\\nResults: LAWGPT demonstrates notable performance improvements over the LLaMA 7B model in\\nvarious legal tasks, but still trails behind proprietary models like GPT-3.5 Turbo and GPT-4.\\n36\\nPharmaceutical Domain\\nModel Description: PharmaGPT, a suite of domain-specific large language models tailored to the\\nbiopharmaceutical and chemical industries, sets a new benchmark for precision in these fields [58].\\nBase Model: LlaMA series\\nFine-tuned Model Parameters: 13B and 70B\\nFine-Tuning Techniques Used: Instruction fine-tuning and RLHF\\nDatasets Used:\\n• Specific-domain data from academic papers and clinical reports\\n• Text data from NLP dataset formats (e.g., question answering, summarisation, dialogue)\\n• Instruction fine-tuning dataset for multitask learning\\n• RLHF dataset with human preference expert-annotated instructions\\nResults: PharmaGPT models demonstrated impressive performance on various pharmaceutical bench-\\nmarks, consistently outperforming GPT-3.5 Turbo.\\nFinance Domain\\nModel Description: Palmyra-Fin-70B-32K, developed by Writer, is a leading large language model\\nspecifically designed for the financial sector. [59]\\nBase Model: LlaMA\\nFine-tuned Model Parameters: 70B\\nFine-Tuning Techniques Used: Not Known\\nDatasets Used: Not Known\\nResults: Palmyra-Fin-70B-32K exhibits state-of-the-art performance, achieving leading results across\\nvarious financial datasets and excelling in financial document analysis, market trend prediction, and risk\\nassessment.\\n6.3\\nParameter-Efficient Fine-Tuning (PEFT) Techniques\\nParameter Efficient Fine Tuning (PEFT) is an impactful NLP technique that adeptly adapts pre-trained\\nlanguage models to various applications with remarkable efficiency. PEFT methods fine-tune only a\\nsmall subset of (additional) model parameters while keeping most of the pre-trained LLM parameters\\nfrozen, thereby significantly reducing computational and storage costs. This approach mitigates the issue\\nof catastrophic forgetting, a phenomenon where neural networks lose previously acquired knowledge and\\nexperience a significant performance decline on previously learned tasks when trained on new datasets.\\nPEFT methods have demonstrated superior performance compared to full fine-tuning, particularly in\\nlow-data scenarios, and exhibit better generalisation to out-of-domain contexts. This technique is appli-\\ncable to various modalities, such as financial sentiment classification and machine translation of medical\\nterminologies. A taxonomy of PEFT-based fine-tuning approaches is provided in Figure6.1. We will\\nfurther discuss a few key PEFT-based approaches in the following sections.\\n6.3.1\\nAdapters\\nAdapter-based methods introduce additional trainable parameters after the attention and fully connected\\nlayers of a frozen pre-trained model, aiming to reduce memory usage and accelerate training. The specific\\napproach varies depending on the adapter; it might involve adding an extra layer or representing the\\nweight updates delta (W) as a low-rank decomposition of the weight matrix. Regardless of the method,\\nadapters are generally small yet achieve performance comparable to fully fine-tuned models, allowing for\\nthe training of larger models with fewer resources.\\nHuggingFace supports adapter configurations through the PEFT library. During fine-tuning, new adapters\\nare integrated into the model using LoraConfig 1. HuggingFace uses PeftConfig to load existing pre-\\ntrained models and apply PEFT techniques. Additionally, HuggingFace provides built-in support to\\n1https://huggingface.co/docs/peft/en/package_reference/lora\\n37\\nFigure 6.1: Comprehensive Taxonomy of Parameter-Efficient Fine-Tuning (PEFT) Methods for Large\\nLanguage Models (LLMs). This figure categorises various PEFT techniques, highlighting their distinct\\napproaches, from additive and selective fine-tuning to reparameterised and hybrid methods. It details\\nspecific strategies within each category, such as Adapter-Based Fine-Tuning, Soft Prompt-Based Fine-\\nTuning, and their respective sub-techniques like LoRA and its derivatives, showcasing the diverse and\\nevolving landscape of LLM fine-tuning. (adapted from [60])\\nrun the fine-tuning process across any distributed configuration using Accelerate2, making large-scale\\ntraining and inference simple, efficient, and adaptable.\\n6.3.2\\nLow-Rank Adaptation (LoRA)\\nLow-Rank Adaptation (LoRA)[62] is a technique designed for fine-tuning large language models, which\\nmodifies the fine-tuning process by freezing the original model weights and applying changes to a separate\\nset of weights, added to the original parameters. LoRA transforms the model parameters into a lower-\\nrank dimension, reducing the number of trainable parameters, speeding up the process, and lowering\\ncosts. This method is particularly useful in scenarios where multiple clients require fine-tuned models\\nfor different applications, allowing for the creation of specific weights for each use case without the\\nneed for separate models. By employing low-rank approximation methods, LoRA effectively reduces\\ncomputational and resource requirements while preserving the pre-trained model’s adaptability to specific\\ntasks or domains.\\nBenefits of Using LoRA\\n1. Parameter Efficiency: LoRA significantly reduces the number of parameters that need to be\\ntrained by focusing only on the low-rank matrices, resulting in lower memory and storage require-\\nments compared to full fine-tuning.\\n2. Efficient Storage: The storage of the trained model is more efficient as it only requires storing\\nthe low-rank matrices instead of the full model weights.\\n2https://huggingface.co/docs/accelerate/en/index\\n38\\nFigure 6.2: Schematic representation of the Adapter Architecture used in LLMs. The diagram showcases\\nthe integration of adapters within the Transformer architecture, including the feed-forward up and down\\nlayers and their role in enabling efficient model adaptation by inserting additional parameters while\\nmaintaining the model’s core structure (adapted from [61])\\n3. Reduced Computational Load: Training with low-rank matrices requires fewer computational\\nresources, making it faster and more scalable.\\n4. Lower Memory Footprint: Since fewer parameters are being updated, the memory footprint\\nduring training is reduced, enabling the use of larger batch sizes or more complex models within\\nthe same hardware constraints.\\n5. Flexibility: LoRA can be easily integrated with existing pre-trained models without extensive\\nmodifications to the model architecture.\\n6. Compatibility: It can be used alongside other fine-tuning techniques, such as adapter layers or\\nprompt-tuning, to further enhance performance.\\n7. Comparable Results: Despite the reduction in the number of trainable parameters, LoRA has\\nbeen shown to achieve performance comparable to full fine-tuning in many tasks.\\n8. Task-Specific Adaptation: It effectively adapts the pre-trained model to specific tasks, leverag-\\ning the knowledge already embedded in the original model.\\n9. Avoiding Overfitting: By focusing on low-rank updates, LoRA can help in mitigating overfitting,\\nespecially when dealing with smaller task-specific datasets.\\nLimitations\\nWhile LoRA demonstrates considerable power, it also presents challenges:\\n• Fine-tuning Scope: LoRA may face difficulties when applied to tasks demanding substantial\\nalterations to the pre-trained model’s internal representations.\\n• Hyperparameter Optimisation: Tuning the rank parameter ‘r’ requires meticulous adjustment\\nfor optimal performance.\\n• Ongoing Research: Despite its promise, LoRA is still in active research stages, and its long-term\\nimplications remain to be fully explored.\\n39\\nFigure 6.3: A comparison between weight updates in regular fine-tuning and LoRA fine-tuning.\\nIn\\nregular fine-tuning, the entire weight update matrix (∆W) is applied to the pre-trained weights. In\\ncontrast, LoRA fine-tuning introduces two low-rank matrices (A and B) that approximate the weight\\nupdate matrix (∆W), significantly reducing the number of trainable parameters by leveraging the inner\\ndimension (r), which is a hyperparameter.\\nThis method is more efficient in terms of memory and\\ncomputation, making it ideal for fine-tuning large models. (adapted from [63])\\nDespite these challenges, LoRA stands as a pioneering technique with vast potential to democratise access\\nto the capabilities of LLMs. Continued research and development offer the prospect of overcoming current\\nlimitations and unlocking even greater efficiency and adaptability.\\nTutorial for Fine-Tuning LLM Using LoRA\\nAn open-source template for fine-tuning LLMs using the LoRA method with the Hugging Face library\\ncan be found here. This template is designed specifically for adapting LLMs for instruction fine-tuning\\nprocesses.\\n6.3.3\\nQLoRA\\nQLoRA[64] is an extended version of LoRA designed for greater memory efficiency in large language mod-\\nels (LLMs) by quantising weight parameters to 4-bit precision. Typically, LLM parameters are stored\\nin a 32-bit format, but QLoRA compresses them to 4-bit, significantly reducing the memory footprint.\\nThis allows fine-tuning on less powerful hardware, including consumer GPUs. QLoRA also quantises the\\nweights of the LoRA adapters from 8-bit to 4-bit, further decreasing memory and storage requirements\\n(see Figure 6.4). Despite the reduction in bit precision, QLoRA maintains performance levels comparable\\nto traditional 16-bit fine-tuning.\\nIt achieves this by backpropagating gradients through a frozen, 4-bit quantised pre-trained language\\nmodel into Low-Rank Adapters, making the fine-tuning process efficient while preserving model effective-\\nness. The QLoRA configuration is supported by HuggingFace via the PEFT library, utilising LoraConfig\\nand BitsAndBytesConfig for quantisation. Innovations such as an optimal 4-bit data type, double quan-\\ntisation of constants, and memory spike management enable QLoRA to reduce memory usage from 96\\nbits per parameter in traditional fine-tuning to 5.2 bits per parameter, an 18-fold reduction.\\nPerformance-wise, QLoRA outperforms naive 4-bit quantisation and matches 16-bit quantised models\\non benchmarks. Additionally, QLoRA enabled the fine-tuning of a high-quality 4-bit chatbot using a\\nsingle GPU in 24 hours, achieving quality comparable to ChatGPT.\\nThis tutorial explains the end-to-end steps of fine-tuning QLoRA on a custom dataset for the Phi-2\\nmodel.\\n40\\nFigure 6.4: Quantised Low-Rank Adaptation (QLoRA) Optimisation Workflow. This figure illustrates\\nthe QLoRA optimisation process, showing how the optimisation states, adapters, and the model interact\\nduring fine-tuning. It demonstrates the use of different bit-widths (32-bit, 16-bit, and 4-bit) to optimise\\nthe memory and computational efficiency during the fine-tuning of large language models (adapted from\\n[65]).\\n6.3.4\\nWeight-Decomposed Low-Rank Adaptation (DoRA)\\nIn the context of optimising model fine-tuning, the pattern analysis of LoRA and Full Fine-Tuning\\n(FT) reveals significant differences in learning behaviours and updates. LoRA, employing a strategy of\\nincrementally updating pre-trained weights using the product of two low-rank matrices, maintains the\\noriginal weights largely static during the fine-tuning process, which allows for efficient inference. Despite\\nits computational efficiency, previous studies have suggested that LoRA’s limited number of trainable\\nparameters might contribute to its performance discrepancies when compared to FT.\\nWeight-Decomposed Low-Rank Adaptation (DoRA) [66] is a novel fine-tuning methodology designed to\\noptimise pre-trained models by decomposing their weights into magnitude and directional components.\\nThis approach leverages the efficiency of Low-Rank Adaptation (LoRA) for directional updates, facili-\\ntating substantial parameter updates without altering the entire model architecture. DoRA addresses\\nthe computational challenges associated with traditional full fine-tuning (FT) by maintaining model\\nsimplicity and inference efficiency, while simultaneously bridging the performance gap typically observed\\nbetween LoRA and FT. Empirical and theoretical evaluations demonstrate that DoRA not only achieves\\nlearning outcomes comparable to FT across diverse tasks—including natural language processing and\\nvision-language applications—but also consistently surpasses LoRA in performance, providing a robust\\nsolution for enhancing the adaptability and efficiency of large-scale models.\\nPython Library - DoRA is facilitated via the HuggingFace LoraConfig package. To incorporate DoRA\\ninto the fine-tuning process, it is essential to specify the ’use dora = True’ parameter during the Lora\\nconfiguration. Further information on initialisation can be found here.\\nBenefits of DoRA\\n1. Enhanced Learning Capacity: DoRA achieves a learning capacity closely resembling full fine-\\ntuning (FT) by decomposing pre-trained weights into magnitude and directional components, al-\\nlowing for more nuanced updates.\\n2. Efficient Fine-Tuning: By utilising the structural advantages of Low-Rank Adaptation (LoRA)\\nfor directional updates, DoRA enables efficient fine-tuning without altering the entire model archi-\\ntecture.\\n3. No Additional Inference Latency: Despite its improved learning capabilities, DoRA does not\\nintroduce any additional inference latency over LoRA, maintaining model simplicity and efficiency.\\n4. Superior Performance: Experimental results demonstrate that DoRA consistently outperforms\\nLoRA across a wide range of tasks, including natural language processing (NLP), visual instruction\\ntuning, and image/video-text understanding. For example, it shows significant improvements in\\ncommonsense reasoning and visual instruction tuning benchmarks.\\n5. Versatility Across Backbones: DoRA has been validated across various model backbones,\\nincluding large language models (LLM) and vision-language models (LVLM), indicating its broad\\n41\\nFigure 6.5: An overview of DoRA (Decomposed Representations for Adaptation), which is a method for\\nweight decomposed low-rank adaptation. The figure illustrates how pre-trained weights are decomposed\\nand adapted for fine-tuning. In the left section, pre-trained weights are decomposed into a magnitude and\\ndirection. The right section shows how these decomposed weights are merged with trainable parameters\\nduring fine-tuning, resulting in updated weights that combine both frozen (blue) and trainable (green)\\ncomponents. The process emphasises efficient adaptation by focusing on the most significant directions\\nin the parameter space, facilitating effective fine-tuning while maintaining the integrity of the original\\nmodel (adapted from [66]).\\napplicability and robustness in different domains.\\n6. Innovative Analysis: The introduction of a novel weight decomposition analysis helps uncover\\nfundamental differences in the learning patterns of FT and various parameter-efficient fine-tuning\\n(PEFT) methods, contributing to a deeper understanding of model fine-tuning dynamics.\\nComparison between LoRA and DoRA\\nLow-Rank Adaptation (LoRA) and Weight-Decomposed Low-Rank Adaptation (DoRA) are both ad-\\nvanced techniques designed to improve the efficiency and effectiveness of fine-tuning large pre-trained\\nmodels. While they share the common goal of reducing computational overhead, they employ different\\nstrategies to achieve this (see Table6.2).\\n42\\nCriteria\\nLoRA\\n(Low-Rank\\nAdapta-\\ntion)\\nDoRA\\n(Weight-Decomposed\\nLow-Rank Adaptation)\\nObjective\\nProvide an efficient method for\\nfine-tuning pre-trained models by\\nusing low-rank matrix products\\nto update weights incrementally\\nwithout increasing inference la-\\ntency.\\nImproves\\nlearning\\ncapacity\\nby\\nclosely mimicking the learning pat-\\nterns of full fine-tuning, optimis-\\ning magnitude and direction sep-\\narately.\\nApproach\\nImplements a low-rank decompo-\\nsition where the weight update is\\nmodelled as the product of two\\nlow-rank matrices (B and A), keep-\\ning the original weights static.\\nUses weight decomposition anal-\\nysis to reparameterise the weight\\nmatrix into separate magnitude\\nand direction components for dis-\\ntinct updates.\\nModel Architecture\\nKeeps the pre-trained weight ma-\\ntrix (W0) unchanged and applies\\nupdates using low-rank matrices\\n(B and A). Matrix A is initialised\\nwith a uniform Kaiming distribu-\\ntion, while B is set to zero initially.\\nRestructures\\nthe\\nweight\\nmatrix\\ninto\\nmagnitude\\nand\\ndirectional\\ncomponents, ensuring directional\\nvectors are unit vectors for more\\ndetailed adjustments.\\nTable 6.2:\\nA detailed comparison between LoRA (Low-Rank Adaptation) and DoRA (Weight-\\nDecomposed Low-Rank Adaptation), highlighting their objectives, approaches, and the specific architec-\\ntural strategies they employ for fine-tuning large language models.\\nTutorial for Fine-Tuning LLM using DoRA\\nThis tutorial offers an in-depth guide and detailed explanation of the steps involved in implementing\\nDoRA from scratch, as well as insights into the fine-tuning process essential for optimising performance.\\n6.3.5\\nFine-Tuning with Multiple Adapters\\nDuring fine-tuning, we have explored the method of freezing the parameters of the LLM and focusing\\nsolely on fine-tuning a few million trainable parameters using LoRA. For example, fine-tuning an LLM\\nfor translation involves training a translation adapter with relevant data. This approach allows us to\\nfine-tune separate adapters for each specific task we want the LLM to perform. However, a key question\\narises: can we consolidate multiple adapters into a unified multi-task adapter? For instance, if we have\\nseparate adapters for translation and summarisation tasks, can we merge them so that the LLM can\\nproficiently handle both tasks? (Illustrated via Figure6.6).\\nThe PEFT library simplifies the process of merging adapters with its add weighted adapter function 3,\\nwhich offers three distinct methods:\\n1. Concatenation: This straightforward method concatenates the parameters of the adapters. For\\ninstance, if two adapters each have a rank of 16, the resulting adapter will have a rank of 32. This\\nmethod is highly efficient.\\n2. Linear Combination: Although less documented, this method appears to perform a weighted\\nsum of the adapters’ parameters.\\n3. SVD: The default method employs singular value decomposition through torch.linalg.svd. While\\nversatile, it is notably slower than the other methods, particularly for adapters with high ranks\\n(greater than 100), which can take several hours.\\nEach method allows for customising the combination by adjusting weights. For instance, when merging\\ntwo adapters, X and Y, assigning more weight to X ensures that the resulting adapter prioritises behaviour\\nsimilar to X over Y.\\nThis approach is particularly suited for consolidating a single LLM to handle multiple tasks rather than\\ncreating separate models for each task domain. By adopting this method, there is no longer a need to\\n3https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraModel.add_weighted_adapter\\n43\\nindividually fine-tune a model for each task. Instead, a single adapter layer can be fine-tuned for each\\ntask, allowing queries to yield the desired responses efficiently.\\nFigure 6.6: Overview of how multiple adapters can be used with a pre-trained LLM to fine-tune it for\\nvarious specific tasks, such as summarisation, proofreading, sentiment analysis, and more. (adapted from\\n[67])\\nSteps for Fine-Tuning LLM with LoRA for Multiple Tasks and Adapters\\n1. Adapter Creation: Create multiple adapters, each fine-tuned for specific tasks using different\\nprompt formats or task-identifying tags (e.g., [translate fren], [chat]).\\n2. LoRA Integration: Implement LoRA to efficiently integrate these adapters into the pre-trained\\nLLM. Utilise LoRA’s methods such as concatenation, linear combination, or singular value decom-\\nposition (SVD) to combine adapters while minimising computational overhead and maintaining\\nperformance.\\n3. Task-Specific Adaptation: Fine-tune each adapter with task-specific data to enhance perfor-\\nmance for individual tasks. Ensure adapters are trained with data relevant to their respective\\ntasks, optimising their ability to generate accurate responses.\\n4. Behaviour Adjustment: Monitor the behaviour of combined adapters to identify any undesired\\ninherited behaviours from individual adapters (e.g., short response generation from a translation\\n44\\nadapter). Adjust the combination weights or types to modify adapter behaviour as needed, ensuring\\neach adapter performs optimally for its intended task.\\n5. Evaluation and Iteration: Evaluate the performance of the combined model across multiple\\ntasks using validation datasets. Iterate on the fine-tuning process, making adjustments to adapter\\ncombinations and training parameters based on performance metrics and user feedback.\\nTherefore, for optimal performance, it is advisable to combine adapters that have been fine-tuned with\\ndistinctly varied prompt formats. However, even when using adapters with different prompt formats, the\\nresulting adapter may not exhibit desired behaviour. For example, a newly combined adapter designed for\\nchatting may only generate short responses, inheriting this tendency from an adapter that was originally\\ntrained to halt after producing a single sentence. To adjust the behaviour of the combined adapter,\\none can prioritise the influence of a specific adapter during the combination process and/or modify the\\nmethod of combination used.\\nAn illustrative tutorial demonstrating the fine-tuning of large language models (LLMs) using multiple\\nadapter layers for various tasks can be found here.\\n6.4\\nHalf Fine Tuning\\nHalf Fine-Tuning (HFT)[68] is a technique designed to balance the retention of foundational knowledge\\nwith the acquisition of new skills in large language models (LLMs). HFT involves freezing half of the\\nmodel’s parameters during each fine-tuning round while updating the other half, allowing the model to\\nretain pre-trained knowledge and enhance new task performance without altering the model architecture.\\nEach repetitive transformer layer is divided into three blocks: self-attention, feed-forward, and layernorm,\\nwith half of the parameters in each block updated and the other half frozen, varying with each round.\\nThis strategic parameter update helps maintain knowledge parity across training rounds and enhances\\nscalability in successive training sessions.\\nResearch on models like LLAMA 2-7B demonstrated that HFT could significantly restore forgotten basic\\nknowledge while preserving high general ability performance. This method’s robustness and efficiency\\nmake it applicable to various fine-tuning scenarios, including supervised fine-tuning, direct preference\\noptimisation, and continual learning. Additionally, HFT’s ability to maintain the model architecture\\nsimplifies its implementation and ensures compatibility with existing systems, further promoting its\\npractical adoption.\\n6.4.1\\nBenefits of using Half Fine tuning\\n1. Recovery of Pre-Trained Knowledge: By rolling back half of the fine-tuned parameters to their\\npre-trained state, HFT effectively recovers a portion of the original knowledge, thereby mitigating\\ncatastrophic forgetting of previously acquired capabilities.\\n2. Enhanced Performance: Research experiments shows that HFT maintains or even surpasses\\nthe performance of full fine-tuning (FFT) on downstream tasks, demonstrating its effectiveness in\\nbalancing knowledge retention with task-specific learning.\\n3. Robustness: The method is robust to different selection strategies and the number of parameters\\nchosen for updating, ensuring consistent performance across various configurations.\\n4. Simplicity and Scalability: HFT does not alter the model architecture, which simplifies im-\\nplementation and allows for scalable applications, particularly beneficial in successive fine-tuning\\nscenarios.\\n5. Versatility: The technique has proven effective across diverse fine-tuning scenarios, including\\nsupervised fine-tuning, direct preference optimisation, and continual learning.\\n45\\nFigure 6.7: Schematic illustration of the Half Fine-Tuning (HFT) method as applied to LLAMA 2’s\\narchitecture. The diagram shows multiple stages of fine-tuning, where specific model parameters are\\nselectively activated (orange) while others remain frozen (blue). This approach optimises training by\\nreducing computational requirements while still effectively adapting the model to new tasks or data.\\n(adapted from [68])\\n6.4.2\\nComparison between HFT and LoRA\\nCriteria\\nHFT\\nLoRA\\nObjective\\nThe goal is to retain the foun-\\ndational knowledge acquired dur-\\ning pre-training while learning new\\ntask-specific skills, thus balancing\\nbetween maintaining existing ca-\\npabilities and acquiring new ones.\\nLoRA aims to reduce computa-\\ntional and memory requirements\\nduring fine-tuning, making it more\\nefficient and feasible to train large\\nmodels on limited hardware re-\\nsources.\\nApproach\\nHFT involves freezing half of the\\nmodel’s parameters during each\\nfine-tuning round and updating\\nonly the other half.\\nLoRA reduces the number of train-\\nable parameters by\\nintroducing\\nlow-rank decomposition into the\\nweight matrices of the neural net-\\nwork. This involves injecting low-\\nrank matrices into the model’s lay-\\ners during fine-tuning.\\nModel Architecture\\nHFT does not alter the model’s ar-\\nchitecture or introduce new param-\\neters, making it straightforward\\nto apply without additional struc-\\ntural changes.\\nLoRA\\nmodifies\\nthe\\nmodel\\nby\\nadding low-rank matrices, which\\nchanges the training dynamics and\\nrequires additional computations\\nfor the low-rank updates.\\nPerformance\\nResearch has shown that HFT\\ncan restore forgotten basic knowl-\\nedge while maintaining high per-\\nformance in general abilities.\\nLoRA is designed to achieve com-\\npetitive performance with full fine-\\ntuning but with significantly fewer\\ntrainable\\nparameters\\nand\\nlower\\ncomputational costs.\\nTable 6.3: Comparative Analysis of Half Fine-Tuning (HFT) and Low-Rank Adaptation (LoRA).\\n46\\n6.5\\nLamini Memory Tuning\\nLamini [69] was introduced as a specialised approach to fine-tuning Large Language Models (LLMs),\\ntargeting the reduction of hallucinations. This development was motivated by the need to enhance the\\nreliability and precision of LLMs in domains requiring accurate information retrieval. Traditional training\\nmethods typically consist of running stochastic gradient descent on vast datasets, which, despite fitting\\nthe training data well, often produce models that fail to generalise effectively and are prone to such errors.\\nFoundation models often follow a training regimen similar to the Chinchilla recipe, which prescribes\\ntraining for a single epoch on a massive corpus, such as training Llama 2 7B on about one trillion\\ntokens. This approach results in substantial loss and is geared more towards enhancing generalisation\\nand creativity where a degree of randomness in token selection is permissible. However, it falls short for\\ntasks demanding high factual precision. In contrast, Lamini Memory Tuning delves deeper by analysing\\nthe loss of individual facts, significantly improving the accuracy of factual recall.\\nBy augmenting a\\nmodel with additional parameters specifically for memory (e.g., an 8B parameter model with an extra 2B\\nparameters for weights), Lamini enables the model to memorise and accurately recall a significant number\\nof facts, closely aligning performance with LLM scaling laws without compromising on generalisation.\\n6.5.1\\nLamini-1 - A model architecture based on Lamini\\nDeparting from traditional transformer-based designs, the Lamini-1 model architecture (Figure 6.8) em-\\nploys a massive mixture of memory experts (MoME). This system features a pre-trained transformer\\nbackbone augmented by adapters that are dynamically selected from an index using cross-attention\\nmechanisms. These adapters function similarly to experts in MoE architectures, and the network is\\ntrained end-to-end while freezing the backbone. This setup allows for specific facts to be stored exactly\\nin the selected experts.\\nFigure 6.8: Diagram of the Lamini-1 Model Architecture, featuring a Massive Array of Memory Experts\\n(MoME). This architecture integrates a pre-trained transformer backbone with dynamically selected\\nadapters via cross-attention mechanisms. Each adapter, functioning as a memory expert, is capable of\\nstoring specific factual data. (adopted from [69])\\nAt inference time, only the relevant experts are retrieved from the index, enabling the LLM to store a\\nlarge number of facts while maintaining low inference latency. Specialised GPU kernels written in Triton\\nare used to accelerate the lookup of experts, optimising the system for quick access to stored knowledge.\\nSystems Optimisations for Banishing Hallucinations\\nThe MoME architecture is designed to minimise the computational demand required to memorise facts.\\nDuring training, a subset of experts, such as 32 out of a million, is selected for each fact. The weights of\\nthe backbone network and the cross attention used to select the expert are frozen, and gradient descent\\nsteps are taken until the loss is sufficiently reduced to memorise the fact. This approach prevents the\\nsame expert from being selected multiple times for different facts by first training the cross attention\\n47\\nselection mechanism during a generalisation training phase, then freezing its weights.\\nThis method ensures that computation scales with the number of training examples, not the total\\nnumber of parameters, thereby significantly reducing the computation required for memory tuning.\\nThis optimised approach allows Lamini-1 to achieve near-zero loss in memory tuning on real and random\\nanswers efficiently, demonstrating its efficacy in eliminating hallucinations while improving factual recall.\\n6.6\\nMixture of Experts\\nA mixture of experts (MoE) is an architectural design for neural networks that divides the computation\\nof a layer or operation (e.g., linear layers, MLPs, or attention projection) into several specialised subnet-\\nworks, referred to as ”experts”. Each expert independently carries out its computation, and the results\\nare aggregated to produce the final output of the MoE layer. MoE architectures can be categorised as\\neither dense, where every expert is engaged for each input, or sparse, where only a subset of experts is\\nutilised for each input.\\n6.6.1\\nMixtral 8x7B Architecture and Performance\\nMixtral [70] 8x7B employs a Sparse Mixture of Experts (SMoE) architecture (Figure 6.9), mirroring the\\nstructure of Mistral 7B but incorporating eight feedforward blocks (experts) in each layer. For every\\ntoken at each layer, a router network selects two experts to process the current state and combine their\\noutputs. Although each token interacts with only two experts at a time, the selected experts can vary at\\neach timestep. Consequently, each token has access to 47 billion parameters but utilises only 13 billion\\nactive parameters during inference. Mixtral 8x7B not only matches but often surpasses Llama 2 70B\\nand GPT-3.5 across all evaluated benchmarks. Its performance is notably superior to Llama 2 70B in\\nmathematics, code generation, and multilingual tasks.\\nFigure 6.9: Diagram of the Mixtral 8x7B Mixture of Experts (MoE) model architecture. The model is\\ncomposed of a router network that dynamically selects the most relevant experts from a pool of eight\\ntransformer-based experts, each with 7 billion parameters. The experts are organised into transformer\\nblocks, where the router directs data to the appropriate expert based on the input, optimising com-\\nputational efficiency and model performance. This architecture allows for scalability and specialised\\nprocessing within large language models. (adapted from [71])\\n48\\n6.7\\nMixture of Agents\\nDespite the numerous LLMs and their notable accomplishments, they continue to encounter fundamental\\nlimitations regarding model size and training data. Scaling these models further is prohibitively expen-\\nsive, often necessitating extensive retraining on multiple trillion tokens. Simultaneously, different LLMs\\nexhibit distinct strengths and specialise in various aspects of tasks. A recent study has investigated\\nleveraging the collective expertise of multiple LLMs to develop a more capable and robust model, a\\nmethod known as Mixture of Agents (MoA) [72].\\nMoA functions using a layered architecture, where each layer comprises multiple LLM agents (Figure\\n6.10). This structure reveals a phenomenon known as the “collaborativeness of LLMs.” The innova-\\ntive MoA framework utilises the combined capabilities of several LLMs to enhance both reasoning and\\nlanguage generation proficiency. Research indicates that LLMs naturally collaborate, demonstrating im-\\nproved response quality when incorporating outputs from other models, even if those outputs are not\\nideal.\\nFigure 6.10: Illustration for Mixture of Agents (MoA) LLM configuration. The model consists of multiple\\nlayers, each incorporating several agents that process the input independently before concatenating their\\noutputs to form an intermediate result. The process continues across layers, refining the output at each\\nstage to generate the final output based on the given prompt (adapted from [72]).\\n6.7.1\\nMethodology\\nTo enhance collaboration among multiple LLMs, it is essential to understand their individual strengths\\nand classify them accordingly. The classification includes:\\n1. Proposers: These models excel at generating valuable reference responses for other models. While\\nthey may not perform exceptionally on their own, they provide useful context and varied perspec-\\ntives that improve the final output when utilised by an aggregator.\\n49\\n2. Aggregators: These models are adept at merging responses from various models into a single\\nhigh-quality result. An effective aggregator should maintain or even enhance the quality of the\\nfinal response, regardless of the quality of the individual inputs.\\nThe careful selection of LLMs for each MoA layer is crucial Performance metrics, such as average win\\nrates in a given layer, help assess the suitability of models for subsequent layers, ensuring the production\\nof higher-quality outputs. Diversity in model outputs is vital, as varied responses from different models\\ncontribute significantly more than homogeneous outputs from a single model. In MoA, given an input\\nprompt, the output of the ith MoA layer yi is calculated as follows:\\nyi =\\nn\\nM\\nj=1\\n[Ai,j(xi)] + x1, xi+1 = yi\\n(6.1)\\n6.7.2\\nAnalogy with MoE\\nMixture-of-Experts (MoE) is a well-established machine learning technique where multiple expert net-\\nworks, each with specialised skills, collaborate to address complex problems. This approach has demon-\\nstrated significant success across various applications and serves as the inspiration for the Mixture-of-\\nAgents (MoA) method. In a typical MoE design, a stack of layers, known as MoE layers, consists of\\nmultiple expert networks, a gating network, and residual connections to improve gradient flow. The\\noutput for layer yi is calculated as follows:\\nyi =\\nn\\nX\\nj=1\\nGi,j(xi)Ei,j(xi) + xi\\n(6.2)\\nThe MoA framework advances the MoE concept by operating at the model level through prompt-based\\ninteractions rather than altering internal activations or weights. Instead of relying on specialised sub-\\nnetworks within a single model, MoA utilises multiple full-fledged LLMs across different layers. In this\\napproach, the gating and expert networks’ functions are integrated within an LLM, leveraging its ability\\nto interpret prompts and generate coherent outputs without additional coordination mechanisms.\\n6.7.3\\nWhat makes MoA works well?\\n1. MoA’s Superior Performance: MoA significantly outperforms LLM-based rankers, which select\\none answer from the proposals rather than generating new responses. This suggests that MoA’s\\napproach of aggregating all generated responses provides more effective results than simply choosing\\nfrom pre-existing options.\\n2. Effective Incorporation of Proposals: The aggregator in MoA demonstrates a tendency to\\nintegrate the best proposed answers. This is supported by positive correlations between aggregator\\nresponses and various similarity metrics, such as BLEU scores, which measure n-gram overlaps. The\\nuse of alternative similarity measures also shows a consistent positive correlation with preference\\nscores, indicating that the aggregator effectively utilises the proposed responses.\\n3. Influence of Model Diversity and Proposer Count: Increasing the number of proposers\\nimproves output quality, highlighting the benefits of additional auxiliary information. Additionally,\\nusing a diverse set of LLMs as proposers consistently yields better results compared to using a single\\nLLM. This suggests that both the number and diversity of LLM agents in each MoA layer contribute\\nto enhanced performance, with potential for further improvement through scaling.\\n4. Model Specialisation: Analysis of model roles within the MoA ecosystem reveals that GPT-4o,\\nQwen, and LLaMA-3 are effective in both assisting and aggregating tasks. In contrast, WizardLM\\nexcels as a proposer but struggles with aggregating responses from other models.\\n6.8\\nProximal Policy Optimisation (PPO)\\nPPO [73] is a widely recognised reinforcement learning algorithm used for training agents to perform tasks\\nin diverse environments. This algorithm leverages policy gradient methods, where policies—represented\\n50\\nby neural networks—determine the actions taken by the agent based on the current state. PPO ef-\\nfectively handles the dynamic nature of training data generated through continuous agent-environment\\ninteractions, a feature that differentiates it from static datasets used in supervised learning.\\nThe innovation of PPO lies in its ”surrogate” objective function, optimised via stochastic gradient ascent.\\nThis approach allows for multiple updates from the same batch of data, enhancing both training efficiency\\nand stability over traditional policy gradient methods. Developed by OpenAI, PPO was designed to\\nbalance ease of implementation with the robust performance characteristics of more complex algorithms\\nlike Trust Region Policy Optimisation (TRPO), but without the associated computational complexity.\\nPPO operates by maximising expected cumulative rewards through iterative policy adjustments that\\nincrease the likelihood of actions leading to higher rewards. A key feature of PPO is its use of a clipping\\nmechanism in the objective function, which limits the extent of policy updates, thus preventing drastic\\nchanges and maintaining stability during training.\\nFigure 6.11: Schematic of Proximal Policy Optimisation (PPO) applied in the context of Reinforcement\\nLearning from Human Feedback (RLHF) for fine-tuning a Large Language Model (LLM). The process\\ninvolves using a prompt dataset to train the LLM. The PPO algorithm adjusts the LLM’s policy based\\non rewards provided by the reward model, which is fine-tuned through human feedback. (adapted from\\n[73])\\nPython Library - HuggingFace Transformer Reinforcement Learning (TRL4) package supports the\\nPPO Trainer5 for training language models from the preference data.\\nThe PPOTrainer expects to align a generated response with a query given the rewards obtained from the\\nReward model. During each step of the PPO algorithm we sample a batch of prompts from the dataset,\\nwe then use these prompts to generate the a responses from the SFT model. Next, the Reward model\\nis used to compute the rewards for the generated response. Finally, these rewards are used to optimise\\nthe SFT model using the PPO algorithm. Therefore the dataset should contain a text column which we\\ncan rename to query. Each of the other data-points required to optimise the SFT model are obtained\\nduring the training loop.\\n6.8.1\\nBenefits of PPO\\n1. Stability: Proximal Policy Optimisation (PPO) is designed to ensure stable and reliable policy\\nupdates. The clipped surrogate objective function is central to this stability, as it limits policy\\nupdates to prevent large, potentially destabilising changes. This results in smoother and more\\nconsistent learning.\\n2. Ease of Implementation: Compared to advanced algorithms TRPO, PPO is relatively straight-\\nforward to implement. It avoids the need for second-order optimisation techniques, making it more\\n4https://huggingface.co/docs/trl/en/index\\n5https://huggingface.co/docs/trl/main/en/ppo_trainer\\n51\\naccessible to less experienced practitioners.\\n3. Sample Efficiency: PPO achieves data efficiency through its use of the clipped surrogate objec-\\ntive. This mechanism regulates policy updates, ensuring stability while effectively reusing training\\ndata.\\nConsequently, PPO tends to be more sample-efficient than other reinforcement learning\\nalgorithms, performing well with fewer samples, which is advantageous in scenarios where data\\ncollection is costly or time-consuming.\\n6.8.2\\nLimitations of PPO\\n1. Complexity and Computational Cost: Proximal Policy Optimisation (PPO) involves intricate\\npolicy and value networks, necessitating substantial computational resources for training. This\\ncomplexity often results in extended training durations and increased operational expenses.\\n2. Hyperparameter Sensitivity: PPO’s performance is highly dependent on several hyperparame-\\nters, such as the clipping range, learning rate, and discount factor. Achieving optimal performance\\nrequires meticulous tuning of these parameters. Incorrect settings can lead to suboptimal policy\\noutcomes or instability during the learning process.\\n3. Stability and Convergence Issues: Although PPO is designed to enhance stability compared\\nto earlier methods, it can still encounter convergence issues, particularly in highly dynamic or\\ncomplex environments. Maintaining stable policy updates remains a significant challenge.\\n4. Reward Signal Dependence: PPO’s effectiveness is heavily reliant on a well-defined reward\\nsignal to guide the learning process. In scenarios where designing an appropriate reward function\\nis challenging or impractical, PPO may struggle to attain the desired results.\\n6.8.3\\nTutorial for training models using PPO technique\\nThe tutorial for tuning GPT2 to generate positive movie reviews based on the IMDB dataset using PPO\\ntechnique can be found here.\\n6.9\\nDirect Preference Optimisation (DPO)\\nDirect Preference Optimisation (DPO) [74] offers a streamlined approach to aligning language models\\n(LMs) with human preferences, bypassing the complexity of reinforcement learning from human feedback\\n(RLHF). Large-scale unsupervised LMs typically lack precise behavioural control, necessitating meth-\\nods like RLHF that fine-tune models using human feedback. However, RLHF is intricate, involving the\\ncreation of reward models and the fine-tuning of LMs to maximise estimated rewards, which can be\\nunstable and computationally demanding. DPO addresses these challenges by directly optimising LMs\\nwith a simple classification objective that aligns responses with human preferences. This approach elim-\\ninates the need for explicit reward modelling and extensive hyperparameter tuning, enhancing stability\\nand efficiency. DPO optimises the desired behaviours by increasing the relative likelihood of preferred\\nresponses while incorporating dynamic importance weights to prevent model degeneration. Thus, DPO\\nsimplifies the preference learning pipeline, making it an effective method for training LMs to adhere to\\nhuman preferences.\\nPython Library - HuggingFace TRL package supports the DPO Trainer6 for training language models\\nfrom the preference data. The DPO training process requires a dataset formatted in a very specific\\nmanner. If you are utilising the default DPODataCollatorWithPadding data collator, your final dataset\\nobject must include three specific entries, which should be labelled as follows:\\n• Prompt\\n• Chosen\\n• Rejected\\nHuggingFace offers datasets compatible with DPO and can be accessed here.\\n6https://huggingface.co/docs/trl/main/en/dpo_trainer\\n52\\nFigure 6.12: Direct Preference Optimisation (DPO) Process Flow. This figure illustrates the Direct\\nPreference Optimisation (DPO) technique used in fine-tuning large language models. The process begins\\nwith preference data (Yw > Yl), where Yw represents preferred outputs, and Yl represents less preferred\\noutputs. Through a maximum likelihood estimation process, this preference data is used to optimise\\nthe model’s parameters, resulting in the final large language model (LLM). The method is designed to\\nimprove the alignment of model outputs with desired user preferences, enhancing the model’s effectiveness\\nin specific tasks. (adapted from [74])\\n6.9.1\\nBenefits of DPO\\n1. Direct Alignment with Human Preferences: DPO directly optimises models to generate\\nresponses that align with human preferences, thereby producing more favourable outputs.\\n2. Minimised Dependence on Proxy Objectives: In contrast to methods that rely on next-\\nword prediction, DPO leverages explicit human preferences, resulting in responses that are more\\nreflective of human behaviour.\\n3. Enhanced Performance on Subjective Tasks: For tasks requiring subjective judgement, such\\nas dialogue generation or creative writing, DPO excels in aligning the model with human prefer-\\nences.\\n6.9.2\\nBest Practices for DPO\\n1. High-Quality Preference Data: The performance of the model is heavily influenced by the\\nquality of preference data. Ensure the dataset includes clear and consistent human preferences.\\n2. Optimal Beta Value: Experiment with various beta values to manage the influence of the\\nreference model. Higher beta values prioritise the reference model’s preferences more strongly.\\n3. Hyperparameter Tuning: optimise hyperparameters such as learning rate, batch size, and LoRA\\nconfiguration to determine the best settings for your dataset and task.\\n4. Evaluation on Target Tasks: Continuously assess the model’s performance on the target task\\nusing appropriate metrics to monitor progress and ensure the achievement of desired results.\\n5. Ethical Considerations: Pay attention to potential biases in the preference data and take steps\\nto mitigate them, preventing the model from adopting and amplifying these biases.\\n6.9.3\\nTutorial for training models using DPO technique\\nThe tutorial for DPO training, including the full source code of the training scripts for SFT and DPO,\\nis available here.\\n6.9.4\\nIs DPO Superior to PPO for LLM Alignment?\\nThe recent study on DPO superior to PPO for LLM Alignment[75] investigates the efficacy of reward-\\nbased and reward-free methods within RLHF. Reward-based methods, such as those developed by Ope-\\nnAI, utilise a reward model constructed from preference data and apply actor-critic algorithms like\\nProximal Policy Optimisation (PPO) to optimise the reward signal. Conversely, reward-free methods,\\nincluding Direct Preference Optimisation (DPO), RRHF, and PRO, forego an explicit reward function,\\n53\\nwith DPO focusing exclusively on policy optimisation through a logarithmic representation of the reward\\nfunction.\\nOne of the objectives of this study is to determine whether DPO is genuinely superior to PPO in the\\nRLHF domain. The study combines theoretical and empirical analyses to uncover the inherent limita-\\ntions of DPO and identify critical factors that enhance PPO’s practical performance in RLHF.\\nTheoretical findings suggest that DPO may yield biased solutions by exploiting out-of-distribution re-\\nsponses. Empirical results indicate that DPO’s performance is notably affected by shifts in the distri-\\nbution between model outputs and the preference dataset. Furthermore, the study highlights that while\\niterative DPO may offer improvements over static data training, it still fails to enhance performance\\nin challenging tasks such as code generation. Ablation studies on PPO reveal essential components for\\noptimal performance, including advantage normalisation, large batch sizes, and exponential moving av-\\nerage updates for the reference model’s parameters. These findings form the basis of practical tuning\\nguidelines, demonstrating PPO’s robust effectiveness across diverse tasks and its ability to achieve state-\\nof-the-art results in challenging code competition tasks. Specifically, on the CodeContest dataset, the\\nPPO model with 34 billion parameters surpasses AlphaCode-41B, showing a significant improvement in\\nperformance metrics.\\n6.10\\nOdds-Ratio Preference Optimization (ORPO)\\nOdds-Ratio Preference Optimization (ORPO) is a novel approach designed to align the output of lan-\\nguage models with desired responses by introducing a penalisation mechanism for undesirable outputs.\\nUnlike traditional supervised fine-tuning (SFT) approaches, which focus solely on maximising the likeli-\\nhood of correct responses, ORPO adds a specific odds-ratio based loss to penalise unwanted generations.\\nThis technique provides a refined method for improving preference alignment without relying on a ref-\\nerence model, making it efficient for large-scale implementations.\\nGiven an input sequence x, the log-likelihood of generating an output sequence y of length m is\\ncomputed as:\\nlog Pθ(y|x) = 1\\nm\\nm\\nX\\ni=1\\nlog Pθ(yi|x)\\nThe odds of generating the output sequence y given input x is expressed as:\\noddsθ(y|x) =\\nPθ(y|x)\\n1 −Pθ(y|x)\\nORPO introduces an odds-ratio that contrasts the likelihood of generating a preferred (chosen) re-\\nsponse yw with a less preferred (rejected) response yl, defined as:\\nORθ(yw, yl|x) = oddsθ(yw|x)\\noddsθ(yl|x)\\nThe ORPO loss function incorporates two components:\\n• Supervised Fine-tuning Loss (SFT):\\nLSF T = −1\\nM\\nM\\nX\\nk=1\\n|V |\\nX\\ni=1\\nyk\\ni log pk\\ni\\nwhere yk\\ni is a binary indicator for the i-th token in the vocabulary, and pk\\ni is its predicted probability.\\n• Odds-Ratio Loss:\\nLOR = −log σ\\n\\x12\\nlog oddsθ(yw|x)\\noddsθ(yl|x)\\n\\x13\\nwhere σ is the sigmoid function applied to stabilise the log odds ratio.\\n54\\nThus, the total ORPO objective is:\\nLORP O = LSF T + λLOR\\nwhere λ controls the strength of preference alignment.\\nThis loss function effectively guides the\\nmodel towards generating the chosen response while discouraging the rejected one, facilitating efficient\\nalignment without the need for additional reference models [76].\\nAdvantages of ORPO: ORPO’s strength lies in its ability to perform preference alignment in a\\nmonolithic manner, bypassing the need for separate phases of fine-tuning and preference optimisation.\\nThis reduces computational overhead and provides state-of-the-art performance across various models,\\nincluding LLaMA and Mistral, when evaluated on benchmark tasks such as AlpacaEval and MT-Bench\\n[77].\\n6.11\\nPruning LLMs\\nPruning LLMs involves eliminating unnecessary or redundant components from a neural network to\\nreduce its size and complexity, thereby enhancing its efficiency and performance. This process assists AI\\ndevelopers and engineers in addressing the challenges associated with deploying AI models in resource-\\nlimited environments, such as mobile devices, edge computing, or embedded systems. Pruning AI models\\ncan be achieved through various techniques, each suited to the type and structure of the neural network,\\nthe pruning objective, and the pruning criterion. The following are common approaches:\\n1. Weight Pruning: Involves removing weights or connections with minimal magnitude or impact on\\nthe output. This method reduces the number of parameters and operations in the model, although\\nit may not necessarily decrease memory footprint or latency.\\n2. Unit Pruning: Eliminates entire units or neurons with the lowest activation or contribution to\\nthe output. This technique can reduce the model’s memory footprint and latency but may require\\nretraining or fine-tuning to maintain performance.\\n3. Filter Pruning: Involves removing entire filters or channels in convolutional neural networks that\\nhave the least importance or relevance to the output. This strategy also reduces memory footprint\\nand latency, though it may necessitate retraining or fine-tuning to preserve performance [78].\\n6.11.1\\nWhen to Prune AI Models?\\nPruning AI models can be conducted at various stages of the model development and deployment cycle,\\ncontingent on the chosen technique and objective.\\n1. Pre-Training Pruning: Leverages prior knowledge or heuristics to determine the optimal network\\nstructure before training begins. This approach can save time and resources during training but\\nmay necessitate careful design and experimentation to identify the best configuration.\\n2. Post-Training Pruning: Involves using metrics or criteria to assess the importance or impact of\\neach network component after training. This method helps maintain model performance but may\\nrequire additional validation and testing to ensure quality and robustness.\\n3. Dynamic Pruning: Adjusts the network structure during inference or runtime based on feedback\\nor signals. This approach can optimise the model for different scenarios or tasks but may involve\\nhigher computational overhead and complexity to implement and execute.\\n6.11.2\\nBenefits of Pruning\\n1. Reduced Size and Complexity: Pruning decreases the size and complexity of AI models, making\\nthem easier to store, transmit, and update.\\n2. Improved Efficiency and Performance: Pruned models are faster, more energy-efficient, and\\nmore reliable.\\n3. Enhanced generalisation and Accuracy: Pruning can make models more robust, less prone\\nto overfitting, and more adaptable to new data or tasks.\\n55\\n6.11.3\\nChallenges of Pruning\\n1. Balance Between Size Reduction and Performance: Achieving the optimal balance between\\nreducing size and complexity and maintaining performance is challenging; excessive or insufficient\\npruning can degrade model quality and functionality.\\n2. Choosing Appropriate Techniques: Selecting the right pruning technique, criterion, and objec-\\ntive for the specific neural network type and structure is crucial, as different methods can produce\\nvarying effects and outcomes.\\n3. Evaluation and Validation: Pruned models need thorough evaluation and validation to ensure\\npruning has not introduced errors, biases, or vulnerabilities that could impact performance and\\nrobustness.\\n56\\nChapter 7\\nStage 5: Evaluation and Validation\\n7.1\\nSteps Involved in Evaluating and Validating Fine-Tuned\\nModels\\n1. Set Up Evaluation Metrics: Choose appropriate evaluation metrics, such as cross-entropy, to\\nmeasure the difference between the predicted and actual distributions of the data.\\n2. Interpret Training Loss Curve: Monitor and analyse the training loss curve to ensure the\\nmodel is learning effectively, avoiding patterns of underfitting or overfitting.\\n3. Run Validation Loops: After each training epoch, evaluate the model on the validation set to\\ncompute relevant performance metrics and track the model’s generalisation ability.\\n4. Monitor and Interpret Results: Consistently observe the relationship between training and\\nvalidation metrics to ensure stable and effective model performance.\\n5. Hyperparameter Tuning and Adjustments: Adjust key hyperparameters such as learning\\nrate, batch size, and number of training epochs to optimise model performance and prevent over-\\nfitting.\\n7.2\\nSetting Up Evaluation Metrics\\nCross-entropy is a key metric for evaluating LLMs during training or fine-tuning.\\nOriginating from\\ninformation theory, it quantifies the difference between two probability distributions.\\n7.2.1\\nImportance of Cross-Entropy for LLM Training and Evaluation\\nCross-entropy is crucial for training and fine-tuning LLMs. It serves as a loss function, guiding the model\\nto produce high-quality predictions by minimising discrepancies between the predicted and actual data.\\nIn LLMs, each potential word functions as a separate class, and the model’s task is to predict the next\\nword given the context. This task is inherently complex, requiring the model to understand syntax,\\nsemantics, and context deeply.\\n7.2.2\\nBeyond Cross-Entropy: Advanced LLM Evaluation Metrics\\nWhile cross-entropy remains fundamental, evaluating LLMs effectively necessitates additional metrics\\ntailored to various aspects of model performance. Here are some advanced metrics employed in LLM\\nevaluation:\\nPerplexity\\nPerplexity measures how well a probability distribution or model predicts a sample. In the context of\\nLLMs, it evaluates the model’s uncertainty about the next word in a sequence. Lower perplexity indicates\\nbetter performance, as the model is more confident in its predictions.\\n57\\nFactuality\\nFactuality assesses the accuracy of the information produced by the LLM. It is particularly important for\\napplications where misinformation could have serious consequences. Higher factuality scores correlate\\nwith higher output quality.\\nLLM Uncertainty\\nLLM uncertainty is measured using log probability, helping to identify low-quality generations. Lower\\nuncertainty indicates higher output quality. This metric leverages the log probability of each generated\\ntoken, providing insights into the model’s confidence in its responses.\\nPrompt Perplexity\\nThis metric evaluates how well the model understands the input prompt.\\nLower prompt perplexity\\nindicates a clear and comprehensible prompt, which is likely to yield better model performance.\\nContext Relevance\\nIn retrieval-augmented generation (RAG) systems, context relevance measures how pertinent the re-\\ntrieved context is to the user query. Higher context relevance improves the quality of generated responses\\nby ensuring that the model utilises the most relevant information.\\nCompleteness\\nCompleteness assesses whether the model’s response fully addresses the query based on the provided\\ncontext. High completeness ensures that all relevant information is included in the response, enhancing\\nits utility and accuracy.\\nChunk Attribution and Utilisation\\nThese metrics evaluate how effectively the retrieved chunks of information contribute to the final response.\\nHigher chunk attribution and utilisation scores indicate that the model is efficiently using the available\\ncontext to generate accurate and relevant answers.\\nData Error Potential\\nThis metric quantifies the difficulty the model faces in learning from the training data. Higher data\\nquality results in lower error potential, leading to better model performance.\\nSafety Metrics\\nSafety metrics ensure that the LLM’s outputs are appropriate and non-harmful. These are included in\\nthe final sections of the chapter.\\nIntegrating these advanced metrics provides a holistic view of LLM performance, enabling developers to\\nfine-tune and optimise models more effectively. By employing a metrics-first approach, it is possible to\\nensure that LLMs not only produce accurate and high-quality outputs but also do so consistently and\\nreliably across diverse applications1.\\n7.3\\nUnderstanding the Training Loss Curve\\nThe training loss curve plots the loss value against training epochs and is essential for monitoring model\\nperformance.\\n1https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation\\n58\\n7.3.1\\nInterpreting Loss Curves\\nAn ideal training loss curve shows a rapid decrease in loss during initial stages, followed by a gradual\\ndecline and eventual plateau. Specific patterns to look for include:\\n1. Underfitting: High loss value that does not decrease significantly over time, suggesting the model\\ncannot learn the data.\\n2. Overfitting: Decreasing training loss with increasing validation loss, indicating the model mem-\\norises the training data.\\n3. Fluctuations: Significant variations may indicate a high learning rate or noisy gradients.\\nFigure 7.1: Example training loss curve showing the decline in loss over iterations during the fine-tuning\\nof Llama2 13B on a financial Q/A dataset. The curve illustrates the effectiveness of the fine-tuning\\nprocess in reducing the loss and improving model performance.\\n7.3.2\\nAvoiding Overfitting\\nTechniques to prevent overfitting include:\\n1. Regularisation: Adds a penalty term to the loss function to encourage smaller weights.\\n2. Early Stopping: Stops training when validation performance no longer improves.\\n3. Dropout: Randomly deactivates neurons during training to reduce sensitivity to noise.\\n4. Cross-Validation: Splits data into multiple subsets for training and validation to assess model\\ngeneralisation.\\n5. Batch Normalisation: Normalises inputs to each layer during training to stabilise the learning\\nprocess.\\n6. Larger Datasets and Batch Sizes: Reduces overfitting by increasing the amount of diverse\\ndata and batch sizes.\\n59\\n7.3.3\\nSources of Noisy Gradients\\nNoisy gradients are common during the training of machine learning models, including LLMs. They arise\\nfrom variability in gradient estimates due to stochastic gradient descent and its variants. Strategies to\\nmanage noisy gradients include:\\n1. Learning Rate Scheduling: Gradually decreasing the learning rate during training can reduce\\nthe impact of noisy gradients.\\n2. Gradient Clipping: Setting a threshold for gradient values prevents large updates that can\\ndestabilise training.\\n7.4\\nRunning Validation Loops\\nValidation loops provide an unbiased evaluation of model performance. Typical steps include:\\n1. Split Data: Divide the dataset into training and validation sets.\\n2. Initialise Validation: Evaluate the model on the validation set at the end of each epoch.\\n3. Calculate Metrics: Compute relevant performance metrics, such as cross-entropy loss.\\n4. Record Results: Log validation metrics for each epoch.\\n5. Early Stopping: Optionally stop training if validation loss does not improve for a predefined\\nnumber of epochs.\\n7.5\\nMonitoring and Interpreting Results\\nMonitoring validation results involves analysing trends in validation metrics over epochs. Key aspects\\ninclude:\\n1. Consistent Improvement: Indicates good model generalisation if both training and validation\\nmetrics improve and plateau.\\n2. Divergence: Suggests overfitting if training metrics improve while validation metrics deteriorate.\\n3. Stability: Ensure validation metrics do not fluctuate significantly, indicating stable training.\\n7.6\\nHyperparameter Tuning and Other Adjustments\\nFine-tuning involves adjusting key hyperparameters to achieve optimal performance. Important hyper-\\nparameters include:\\n1. Learning Rate: Determines the step size for updating model weights. A good starting point is\\n2e-4, but this can vary.\\n2. Batch Size: Larger batch sizes lead to more stable updates but require more memory.\\n3. Number of Training Epochs: Balancing the number of epochs ensures the model learns suffi-\\nciently without overfitting or underfitting.\\n4. Optimiser: Optimisers like Paged ADAM optimise memory usage, advantageous for large models.\\nOther tunable parameters include dropout rate, weight decay, and warmup steps.\\n7.6.1\\nData Size and Quality\\nThe efficacy of LLMs is directly impacted by the quality of their training data. Ensuring that datasets\\nare clean, relevant, and adequate is crucial. Data cleanliness refers to the absence of noise, errors, and\\ninconsistencies within the labelled data. For example, having a phrase like “This article suggests. . . ”\\nmultiple times in the training data can corrupt the response of LLMs and add a bias towards using this\\nspecific phrase more often and in inappropriate situations.\\n60\\n7.7\\nBenchmarking Fine-Tuned LLMs\\nModern LLMs are assessed using standardised benchmarks such as GLUE, SuperGLUE, HellaSwag,\\nTruthfulQA, and MMLU (See Table 7.1). These benchmarks evaluate various capabilities and provide\\nan overall view of LLM performance.\\nBenchmark\\nDescription\\nReference URL\\nGLUE\\nProvides a standardised set of diverse NLP tasks to\\nevaluate the effectiveness of different language mod-\\nels\\nSource\\nSuperGLUE\\nCompares more challenging and diverse tasks with\\nGLUE, with comprehensive human baselines\\nSource\\nHellaSwag\\nEvaluates how well an LLM can complete a sentence\\nSource\\nTruthfulQA\\nMeasures truthfulness of model responses\\nSource\\nMMLU\\nEvaluates how well the LLM can multitask\\nSource\\nIFEval\\nTests a model’s ability to follow explicit instructions,\\nfocusing on formatting adherence\\nSource\\nBBH (Big Bench Hard)\\n23 challenging tasks from the BigBench dataset to\\nevaluate LLMs using objective metrics\\nSource\\nMATH\\nCompilation of high-school level competition prob-\\nlems formatted using LaTeX and Asymptote\\nSource\\nGPQA\\nChallenging\\nknowledge\\ndataset\\nwith\\nquestions\\ncrafted by PhD-level domain experts\\nSource\\nMuSR\\nDataset with complex problems requiring models to\\nintegrate reasoning with long-range context parsing\\nSource\\nMMLU-PRO\\nRefined version of MMLU with higher quality and\\nmore challenging multiple-choice questions\\nSource\\nARC\\nMeasures machine reasoning with a dataset of grade-\\nschool science questions\\nSource\\nCOQA\\nA dataset for building conversational question-\\nanswering systems\\nSource\\nDROP\\nEvaluates the ability to perform discrete reasoning\\nover paragraphs of text\\nSource\\nSQuAD\\nA reading comprehension dataset for evaluating\\nmodels’ ability to answer questions based on pas-\\nsages of text\\nSource\\nTREC\\nA benchmark for evaluating text retrieval method-\\nologies\\nSource\\nWMT\\nA dataset and benchmark for evaluating machine\\ntranslation models\\nSource\\nXNLI\\nA dataset for evaluating cross-lingual language un-\\nderstanding\\nSource\\nPiQA\\nA dataset for evaluating models’ understanding of\\nphysical interactions\\nSource\\nWinogrande\\nA large-scale dataset for evaluating commonsense\\nreasoning\\nSource\\nTable 7.1: Detailed Overview of Benchmark Datasets Used for Evaluating Language Model Performance.\\nAs LLMs evolve, so do benchmarks, with new standards such as BigCodeBench challenging current\\nbenchmarks and setting new standards in the domain. Given the diverse nature of LLMs and the tasks\\nthey can perform, the choice of benchmarks depends on the specific tasks the LLM is expected to handle.\\nFor generic applicability, various benchmarks for different downstream applications and reasoning should\\nbe utilised. For domain/task-specific LLMs, benchmarking can be limited to relevant benchmarks like\\nBigCodeBench for coding.\\n61\\n7.8\\nEvaluating Fine-Tuned LLMs on Safety Benchmark\\nThe safety aspects of Large Language Models (LLMs) are increasingly scrutinised due to their ability\\nto generate harmful content when influenced by jailbreaking prompts. These prompts can bypass the\\nembedded safety and ethical guidelines within the models, similar to code injection techniques used in\\ntraditional computer security to circumvent safety protocols. Notably, models like ChatGPT, GPT-\\n3, and InstructGPT are vulnerable to such manipulations that remove content generation restrictions,\\npotentially violating OpenAI’s guidelines. This underscores the necessity for robust safeguards to ensure\\nLLM outputs adhere to ethical and safety standards.\\nDecodingTrust [79] provides a comprehensive evaluation of the trustworthiness of LLMs, notably com-\\nparing GPT-4 with GPT-3.5 (ChatGPT). This evaluation spans several critical areas:\\n1. Toxicity: Optimisation algorithms and generative models are employed to create challenging\\nprompts that test the model’s ability to avoid generating harmful content.\\n2. Stereotype Bias: An array of demographic groups and stereotype topics are utilised to assess\\nmodel bias, helping to understand and mitigate prejudiced responses.\\n3. Adversarial Robustness: The resilience of models against adversarial attacks is tested by chal-\\nlenging them with sophisticated algorithms intended to deceive or mislead.\\n4. Out-of-Distribution (OOD) Robustness: Models are evaluated on their ability to handle\\ninputs that differ significantly from their training data, such as poetic or Shakespearean styles.\\n5. Robustness to Adversarial Demonstrations: Demonstrations that contain misleading infor-\\nmation are used to test the model’s robustness across various tasks.\\n6. Privacy: Various levels of privacy evaluation assess how well models safeguard sensitive informa-\\ntion during interactions and understand privacy-related contexts.\\n7. Hallucination Detection: Identifies instances where the model generates information not grounded\\nin the provided context or factual data. Lower hallucination rates improve the reliability and trust-\\nworthiness of the LLM’s outputs.\\n8. Tone Appropriateness: Assesses whether the model’s output maintains an appropriate tone for\\nthe given context. This is particularly important for applications in customer service, healthcare,\\nand other sensitive areas.\\n9. Machine Ethics: Ethical assessments involve testing models with scenarios that require moral\\njudgments, using datasets like ETHICS and Jiminy Cricket.\\n10. Fairness: The fairness of models is evaluated by generating tasks that vary protected attributes,\\nensuring equitable responses across different demographic groups.\\nThe dataset employed for evaluating the aforementioned eight safety dimensions can be found here.\\nIn partnership with HuggingFace, the LLM Safety Leaderboard utilises DecodingTrust’s framework to\\nprovide a unified evaluation platform for LLM safety.\\nThis allows researchers and practitioners to\\nbetter understand the capabilities, limitations, and risks associated with LLMs. Users are encouraged to\\nsubmit their models to HuggingFace for evaluation, ensuring they meet the evolving standards of safety\\nand reliability in the field.\\n7.9\\nEvaluating Safety of Fine-Tuned LLM using AI Models\\n7.9.1\\nLlama Guard\\nLlama Guard 2[80] is a safeguard model built on LLMs for managing risks in conversational AI applica-\\ntions. It effectively categorises both input prompts and responses from AI agents using a detailed safety\\nrisk taxonomy tailored to identify potential legal and policy risks in AI interactions. It utilises a detailed\\nsafety risk taxonomy designed to identify and manage potential legal and policy risks in interactions\\ninvolving conversational AI. This taxonomy enables effective classification in areas such as:\\n• Violence & Hate, addressing content that could incite violent acts or discrimination.\\n62\\n• Sexual Content, targeting sexually explicit material or behaviour, especially involving minors.\\n• Guns & Illegal Weapons, concerning the promotion or instruction of illegal armaments.\\n• Regulated or Controlled Substances, covering illegal drugs and other controlled substances.\\n• Suicide & Self-Harm, aimed at content that could encourage self-destructive behaviour.\\n• Criminal Planning, for content that could assist in planning or executing criminal activities.\\nThe core of Llama Guard 2 is its robust framework that allows for both prompt and response classifica-\\ntion, supported by a high-quality dataset that enhances its ability to monitor conversational exchanges.\\nOperating on a Llama2-7b model, Llama Guard 2 has been instruction-tuned to deliver strong perfor-\\nmance on benchmarks like the OpenAI Moderation Evaluation dataset and ToxicChat, where it matches\\nor surpasses the capabilities of existing content moderation tools.\\nThe model supports multi-class classification and generates binary decision scores. Its instruction fine-\\ntuning allows for extensive customisation of tasks and adaptation of output formats. This feature enables\\nusers to modify taxonomy categories to align with specific use cases and supports flexible prompting\\ncapabilities, including zero-shot and few-shot applications. The adaptability and effectiveness of Llama\\nGuard make it a vital resource for developers and researchers. By making its model weights publicly\\navailable, Llama Guard 2 encourages ongoing development and customisation to meet the evolving needs\\nof AI safety within the community.\\nLlama Guard 3 represents the latest advancement over Llama Guard 2, having been fine-tuned on the\\nLlama 3 8b model. The key difference between the two versions is that Llama Guard 3 expands upon\\nthe capabilities of Llama Guard 2 by introducing three new categories: Defamation, Elections, and\\nCode Interpreter Abuse.\\nPython Library: Llama Guard 3 is accessible via HuggingFace’s AutoModelForCausalLM.2 A detailed\\ntutorial is available at this link. Please note that access to the model requires submitting a request to\\nHugging Face with the user details. Additionally, the model weights can be downloaded from the Meta\\nplatform by providing user details, and the link can be found here.\\nThe prompt formats for these two models also differ, with the specific formats for Llama Guard 2 available\\nhere and Llama Guard 3 is accessible here.\\n7.9.2\\nShield Gemma\\nShieldGemma [81] is an advanced content moderation model built on the Gemma2 platform, designed\\nto enhance the safety and reliability of interactions between LLMs and users. It effectively filters both\\nuser inputs and model outputs to mitigate key harm types, including offensive language, hate speech,\\nmisinformation, and explicit content. The model’s scalability, with options ranging from 2B to 27B\\nparameters, allows for tailored applications that meet specific needs, such as reducing latency in online\\nsafety applications or enhancing performance in complex decision-making tasks.\\nA distinguishing feature of ShieldGemma is its novel approach to data curation. It leverages synthetic\\ndata generation techniques to create high-quality datasets that are robust against adversarial prompts\\nand fair across diverse identity groups. This reduces the need for extensive human annotation, streamlin-\\ning the data preparation process while ensuring the model’s effectiveness. Compared to existing content\\nmoderation tools like LlamaGuard and WildGuard, which typically offer fixed-size models and limited\\ncustomisation, ShieldGemma’s flexible architecture and advanced data handling capabilities provide a\\nmore adaptable and efficient solution.\\nThese innovations position ShieldGemma as a significant ad-\\nvancement in LLM-based content moderation, offering developers and researchers a versatile tool that\\npromotes safer and more reliable AI interactions across various platforms.\\nPython Library: The ShieldGemma series is available on HuggingFace via AutoModelForCausalLM.\\nThe models can be accessed here. A tutorial for running ShieldGemma 2B on Google Colab can be found\\nhere. Similar to Llama Guard series, ShieldGemma series also has guidelines for prompting and it can\\nbe found here.\\n7.9.3\\nWILDGUARD\\nWILDGUARD [82] is an innovative open-source tool developed to enhance the safety of interactions\\nwith large language models (LLMs).\\nThis tool addresses three critical moderation tasks: detecting\\n2https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForCausalLM\\n63\\nharmful intent in user prompts, identifying safety risks in model responses, and determining when a\\nmodel appropriately refuses unsafe requests.\\nCentral to its development is WILDGUARD MIX3, a\\nmeticulously curated dataset comprising 92,000 labelled examples that include both benign prompts and\\nadversarial attempts to bypass safety measures. The dataset is divided into WILDGUARD TRAIN, used\\nfor training the model, and WILDGUARD TEST, consisting of high-quality human-annotated examples\\nfor evaluation.\\nThe WILDGUARD model itself is fine-tuned on the Mistral-7B language model using the WILDGUARD\\nTRAIN dataset, enabling it to perform all three moderation tasks in a unified, multi-task manner. Results\\nshow that WILDGUARD surpasses existing open-source moderation tools in effectiveness, particularly\\nexcelling in handling adversarial prompts and accurately detecting model refusals. On many benchmarks,\\nWILDGUARD’s performance is on par with or exceeds that of GPT-4, a much larger, closed-source\\nmodel.\\nThe quick start guide and additional information on WILDGUARD are available in GitHub and it can\\nbe accessed here.\\n3https://huggingface.co/datasets/allenai/wildguardmix\\n64\\nChapter 8\\nStage 6: Deployment\\n8.1\\nSteps Involved in Deploying the Fine-Tuned Model\\n1. Model Export: Save the fine-tuned model in a suitable format (e.g., ONNX, TensorFlow Saved-\\nModel, PyTorch) for deployment.\\n2. Infrastructure Setup: Prepare the deployment environment, including necessary hardware, cloud\\nservices, and containerisation tools.\\n3. API Development: Create APIs to allow applications to interact with the model, facilitating\\nprediction requests and responses.\\n4. Deployment: Deploy the model to the production environment, making it accessible to end-users\\nor applications.\\n8.2\\nCloud-Based Providers for LLM Deployment\\nCloud-based large language model (LLM) inferencing frequently employs a pricing model based on the\\nnumber of tokens processed. Users are charged according to the volume of text analysed or generated\\nby the model. While this pricing structure can be cost-effective for sporadic or small-scale usage, it may\\nnot always be economical for larger or continuous workloads.\\nIn some scenarios, hosting an LLM solution in-house may offer better long-term cost savings, especially if\\nthere is consistent or high-volume usage. Managing your own infrastructure provides greater control over\\nresource allocation and allows for cost optimisation based on specific needs. Additionally, self-hosting\\noffers advantages in terms of data privacy and security, as sensitive information remains within your own\\nenvironment.\\nHowever, it is crucial to carefully evaluate the total cost of ownership when comparing cloud-based\\nsolutions with self-hosted alternatives. This evaluation should consider factors such as hardware expenses,\\nmaintenance, and operational overheads. Ultimately, the decision should be informed by a comprehensive\\ncost-benefit analysis, considering both short-term affordability and long-term sustainability.\\nSeveral companies offer deployment services for large language models (LLMs), providing a range of\\ntools and platforms to efficiently implement and manage these models. Here’s a detailed list of some\\nprominent providers and their services:\\n• Amazon Web Services (AWS)\\n– Amazon Bedrock: This service offers a suite of foundation models including Amazon Ti-\\ntan, which supports various NLP tasks such as summarisation and text generation. Bedrock\\nintegrates seamlessly with other AWS services for scalable and secure deployment.\\n– Amazon SageMaker: Provides an end-to-end machine learning service that includes tools\\nfor building, training, and deploying LLMs. SageMaker JumpStart offers pre-trained models\\nand step-by-step guides to simplify the deployment process.\\n65\\n– Tutorial: This tutorial explains the deployment of LLM Agents on Amazon Bedrock. An-\\nother tutorial explains end-to-end fine-tuning and deployment of LLMs with Sagemaker Can-\\nvas and Amazon Bedrock. General guidelines of Amazon Bedrock for LLM users can be found\\nhere.\\n• Microsoft Azure\\n– Azure OpenAI Service: This service offers access to OpenAI’s powerful models like GPT-\\n3.5 and Codex. It provides capabilities for embedding, image generation with DALL-E, and\\nspeech-to-text with Whisper. Azure’s integration with OpenAI models ensures robust deploy-\\nment options for various applications.\\n– Azure Machine Learning: Supports the deployment of custom and pre-trained models,\\noffering tools for model management, deployment, and monitoring. It integrates with Azure’s\\nbroader ecosystem for scalable and secure ML operations.\\n– Tutorial: Here is the tutorial for creating and deploying an Azure OpenAI Service in Mi-\\ncrosoft Azure platform.\\n• Google Cloud Platform (GCP)\\n– Vertex AI: This platform allows the deployment of large language models with tools for\\ntraining, tuning, and serving models. Vertex AI supports models like BERT and GPT-3,\\nproviding extensive MLOps capabilities for end-to-end management.\\n– Cloud AI API: Offers APIs for NLP tasks such as translation, sentiment analysis, and\\nentity recognition. These APIs are backed by Google’s powerful infrastructure, ensuring high\\nperformance and reliability.\\n– Tutorial: This document contains a tutorial for training and deploying an LLM in GCP.\\n• Hugging Face\\n– Inference API: This service allows users to deploy and manage LLMs hosted on Hugging\\nFace’s infrastructure. It supports various models from the Transformers library and provides\\nan easy-to-use API for integrating these models into applications.\\n– Spaces: A collaborative environment where users can deploy and share models using Hugging\\nFace’s hosting platform. It supports deploying custom models and interactive demos.\\n– Tutorial: This document contains a tutorial for training and deploying an LLM using Hug-\\ngingFace Inference API.\\n• Other Platforms\\n– OpenLLM: Provides deployment solutions here.\\n– Deepseed: Offers deployment solutions here.\\n8.3\\nTechniques for Optimising Model Performance During In-\\nference\\nOptimising model performance during inference is crucial for the efficient deployment of large language\\nmodels (LLMs). The following advanced techniques offer various strategies to enhance performance,\\nreduce latency, and manage computational resources effectively.\\n8.3.1\\nTraditional On-Premises GPU-Based Deployments\\nThis conventional approach to deploying large language models (LLMs) involves using Graphics Process-\\ning Units (GPUs) due to their parallel processing capabilities, which enable fast and efficient inference.\\nHowever, this method requires upfront hardware investment and may not be suitable for applications\\nwith fluctuating demand or limited budgets. GPU-based deployments face several challenges:\\n1. Resource utilisation may suffer during periods of low demand due to idle servers.\\n2. Scaling up or down often requires physical hardware modifications, which can be time-consuming.\\n66\\n3. Centralised servers can introduce single points of failure and scalability limitations.\\nTo mitigate these issues, strategies such as load balancing between multiple GPUs, fallback routing, model\\nparallelism, and data parallelism can be employed to achieve better results. Optimisation techniques like\\ndistributed inference using PartialState from accelerate can further enhance efficiency.\\nExample use case: Large-Scale NLP Application\\nFor instance, a large e-commerce platform implemented traditional on-premises GPU-based deployment\\nto handle millions of customer queries daily. By utilising load balancing and model parallelism, they\\nwere able to achieve a significant reduction in latency and improved customer satisfaction.\\n8.3.2\\nDistributed LLM: Torrent-Style Deployment and Parallel Forward Passes\\nAn innovative deployment strategy for large language models (LLMs) involves distributing them across\\nmultiple GPUs in a decentralised, torrent-style manner. Libraries like Petals1 can perform this task.\\nPetals functions as a decentralised pipeline designed for rapid neural network inference by partitioning\\nthe model into distinct blocks or layers, which are distributed across multiple geographically dispersed\\nservers. Users can connect their own GPUs to this network, acting as both contributors and clients who\\ncan access and apply the model to their data.\\nWhen a client request is received, the network routes it through a series of servers optimised to minimise\\nthe total forward pass time. Each server dynamically selects the most optimal set of blocks, adapting to\\nthe current bottlenecks in the pipeline. This framework leverages decentralisation principles to distribute\\ncomputational load across diverse regions, sharing computational resources and GPUs in a way that\\nreduces the financial burden on individual organisations. This collaborative approach not only optimises\\nresource utilisation but also fosters a global community dedicated to shared AI goals.\\nFigure 8.1: Conceptual Representation of Distributed LLM Deployment Using a Torrent-Style Approach.\\nThis figure illustrates the distributed deployment of a Large Language Model (LLM) using a torrent-style\\napproach, where multiple GPT model layers (stacks) are distributed across different nodes (represented\\nby chefs) and perform parallel forward passes. The process mimics the flow of orders from customers\\n(input data) through restaurants (intermediate processing layers) to chefs (model layers), highlighting\\nthe efficiency of parallel processing and distributed computing in handling large-scale language models.\\nThis approach is essential for reducing inference latency and improving the scalability of LLMs across\\ndiverse computational environments. (adapted from [83])\\n1https://github.com/bigscience-workshop/petals\\n67\\nExample use case: Global Research Collaboration\\nA consortium of research institutions implemented a distributed LLM using the Petals framework to\\nanalyse large datasets across different continents. By leveraging the decentralised nature of Petals, they\\nachieved high efficiency in processing and collaborative model development.\\n8.3.3\\nWebGPU-Based Deployment of LLM\\nThis deployment option for large language models (LLMs) involves utilising WebGPU, a web standard\\nthat provides a low-level interface for graphics and compute applications on the web platform. With\\nWebGPU, organisations can harness the power of GPUs directly within web browsers, enabling effi-\\ncient inference for LLMs in web-based applications. WebGPU enables high-performance computing and\\ngraphics rendering directly within the client’s web browser. It allows developers to utilise the client’s\\nGPU for tasks such as rendering graphics, accelerating computational workloads, and performing par-\\nallel processing, all without the need for plugins or additional software installations. This capability\\npermits complex computations to be executed efficiently on the client’s device, leading to faster and\\nmore responsive web applications.\\n8.3.4\\nLLM on WebGPU using WebLLM\\nClients can access powerful large language models and chatbots directly in their browser, leveraging\\nWebGPU acceleration. This approach eliminates server dependencies, providing users with exceptional\\nperformance and enhanced privacy. WebLLM facilitates the use of large language models directly in the\\nclient’s browser to perform tasks such as filtering out personally identifiable information (PII) or named\\nentity recognition (NER) on data without transmitting it over the network.\\nThis ensures enhanced\\nprivacy and security by retaining sensitive information on the client side.\\n68\\nFigure 8.2: WebGPU-Based Deployment of LLM: This diagram illustrates the architecture of deploying\\na large language model (LLM) using WebGPU technology. The CPU manages the distribution of prompt\\ninferencing tasks to multiple GPUs, which then process these prompts in parallel, enhancing efficiency\\nand scalability in LLM deployment across web-based platforms. (adapted from [83])\\nAdditional Use Cases for WebLLM\\n1. Language Translation: Enable real-time translation of text directly in the browser, allowing\\nusers to communicate across language barriers without transmitting their messages over the net-\\nwork.\\n2. Code Autocompletion: Develop code editors that provide intelligent autocompletion suggestions\\nbased on context, leveraging WebLLM to understand and predict code snippets.\\n3. Customer Support Chatbots: Implement chatbots on websites to provide instant customer\\nsupport and answer frequently asked questions without relying on external servers.\\n4. Data Analysis and Visualisation: Create browser-based tools for analysing and visualising\\ndata, with WebLLM assisting in data processing, interpretation, and generating insights.\\n5. Personalised Recommendations:\\nDevelop recommendation engines that offer personalised\\nproduct recommendations, content suggestions, or movie/music recommendations based on user\\npreferences and behaviour.\\n6. Privacy-Preserving Analytics: Develop analytics platforms that perform data analysis directly\\nin the browser, ensuring that sensitive information remains on the client side and reducing the risk\\nof data breaches.\\n69\\nExample use case: Privacy-Focused Web Application\\nA healthcare startup deployed an LLM using WebLLM to process patient information directly within the\\nbrowser, ensuring data privacy and compliance with healthcare regulations. This approach significantly\\nreduced the risk of data breaches and improved user trust.\\n8.3.5\\nQuantised LLMs\\nModel quantisation is a technique utilised to reduce the size of an AI model by representing its parameters\\nwith fewer bits. In traditional machine learning models, each parameter (e.g., weights and biases in neural\\nnetworks) is typically stored as a 32-bit floating-point number, necessitating significant memory and\\ncomputational resources, particularly for large models. Quantisation aims to alleviate this by reducing\\nthe precision of these parameters. For instance, instead of storing each parameter as a 32-bit floating-\\npoint number, they may be represented using fewer bits, such as 8-bit integers.\\nThis compression\\nreduces the memory footprint of the model, making it more efficient to deploy and execute, especially in\\nresource-constrained environments like mobile devices or edge devices. QLoRA is a popular example of\\nthis quantisation for LLMs and can be used to deploy LLMs locally or host them on external servers.\\nExample use case: Edge Device Deployment\\nA tech company used quantised LLMs to deploy advanced NLP models on mobile devices, enabling offline\\nfunctionality for applications such as voice recognition and translation. This deployment significantly\\nimproved app performance and user experience by reducing latency and reliance on internet connectivity.\\n8.3.6\\nvLLMs\\nThe vLLM2 system efficiently handles requests by employing a block-level memory management method\\nand preemptive request scheduling. It utilises the PagedAttention[84] algorithm to manage the key-\\nvalue (KV) cache, thereby reducing memory waste and fragmentation. By batching requests and sharing\\nphysical blocks across multiple samples, vLLM optimises memory usage and enhances throughput. Per-\\nformance tests indicate that vLLM surpasses other systems in various decoding scenarios. Consider a\\ntransformer-based model tasked with summarising a lengthy book. Traditional transformers process the\\nentire book simultaneously, which can be both computationally and memory-intensive, especially for ex-\\ntended texts. With PagedAttention, the book is divided into smaller segments or pages. The model then\\nfocuses on summarising one page at a time, rather than the entire book simultaneously. This approach\\nreduces computational complexity and memory requirements, making it more feasible to process and\\nsummarise lengthy texts efficiently.\\nExample use case: High-Volume Content Generation\\nA content marketing agency implemented vLLMs for generating large volumes of SEO-optimised content.\\nBy leveraging the efficient memory management of vLLMs, they were able to handle multiple concurrent\\nrequests, significantly increasing their content production rate while maintaining high quality.\\n8.4\\nKey Considerations for Deployment of LLMs\\nDeploying large language models (LLMs) effectively requires careful planning and consideration of various\\nfactors to ensure optimal performance, cost-efficiency, and security. Key considerations include:\\n• Infrastructure Requirements:\\n– Compute Resources: Ensure adequate CPU/GPU resources to handle the model’s compu-\\ntational demands. High-performance GPUs are typically required for efficient inference and\\ntraining.\\n– Memory: LLMs, especially those with billions of parameters, require substantial memory.\\nMemory management techniques such as quantisation and model parallelism can be employed\\nto optimise usage.\\n2https://docs.vllm.ai/en/stable/\\n70\\n• Scalability:\\n– Horizontal Scaling: Plan for horizontal scaling to distribute the load across multiple servers,\\nwhich can improve performance and handle increased demand.\\n– Load Balancing: Implement load balancing strategies to ensure even distribution of requests\\nand prevent any single point of failure.\\n• Cost Management:\\n– Token-based Pricing: Understand the cost implications of token-based pricing models of-\\nfered by cloud providers. This model charges based on the number of tokens processed, which\\ncan become expensive with high usage.\\n– Self-Hosting: Evaluate the costs and benefits of self-hosting versus cloud hosting.\\nSelf-\\nhosting might offer long-term savings for consistent, high-volume usage but requires significant\\nupfront investment in hardware and ongoing maintenance.\\n• Performance Optimisation:\\n– Latency: Minimise latency to ensure real-time performance, particularly for applications\\nrequiring instant responses like chatbots and virtual assistants.\\n– Throughput: Maximise throughput to handle a high volume of requests efficiently. Tech-\\nniques like batching and efficient memory management (e.g., PagedAttention) can help.\\n• Security and Privacy:\\n– Data Security: Implement robust security measures to protect sensitive data, including\\nencryption and secure access controls.\\n– Privacy: Ensure compliance with data privacy regulations by keeping sensitive data within\\nyour environment if self-hosting, or ensuring cloud providers comply with relevant privacy\\nstandards.\\n• Maintenance and Updates:\\n– Model Updates: Regularly update the model to incorporate new data and improve perfor-\\nmance. Automate this process if possible to reduce manual effort.\\n– System Maintenance: Plan for regular maintenance of the infrastructure to prevent down-\\ntime and ensure smooth operation.\\n• Flexibility and Customisation:\\n– Fine-Tuning:\\nAllow for model fine-tuning to adapt the LLM to specific use cases and\\ndatasets. Fine-tuning can improve accuracy and relevance in responses.\\n– API Integration: Ensure the deployment platform supports easy integration with existing\\nsystems and workflows through APIs and SDKs.\\n• User Management:\\n– Access Control: Implement role-based access control to manage who can deploy, use, and\\nmaintain the LLM.\\n– Monitoring and Logging: Set up comprehensive monitoring and logging to track usage,\\nperformance, and potential issues. This helps in proactive troubleshooting and optimisation.\\n• Compliance:\\n– Regulatory Compliance: Ensure that the deployment adheres to all relevant regulatory\\nand legal requirements, including data protection laws like GDPR, HIPAA, etc.\\n– Ethical Considerations: Implement ethical guidelines to avoid biases and ensure the re-\\nsponsible use of LLMs.\\n• Support and Documentation:\\n– Technical Support: Choose a deployment platform that offers robust technical support and\\nresources.\\n– Documentation: Provide comprehensive documentation for developers and users to facili-\\ntate smooth deployment and usage.\\n71\\nChapter 9\\nStage 7: Monitoring and\\nMaintenance\\n9.1\\nSteps Involved in Monitoring and Maintenance of Deployed\\nFine-Tuned LLMs\\nContinuous monitoring and maintenance of fine-tuned LLMs are essential to ensure their optimal per-\\nformance, accuracy, and security over time. Below are the key steps involved in this process:\\n1. Setup Initial Baselines: Establish initial performance baselines by evaluating the model on a\\ncomprehensive test dataset. Record metrics such as accuracy, latency, throughput, and error rates\\nto serve as reference points for future monitoring.\\n2. Performance Monitoring: Implement systems to continuously track key performance metrics\\nsuch as response time, server load, and token usage. Regularly compare these metrics against the\\nestablished baselines to detect any deviations.\\n3. Accuracy Monitoring: Continuously evaluate the model’s predictions against a ground truth\\ndataset. Use metrics like precision, recall, F1 score, and cross-entropy loss to ensure the model\\nmaintains high accuracy levels.\\n4. Error Monitoring: Track and analyse errors, including runtime errors and prediction errors.\\nImplement logging mechanisms to capture detailed information about each error for troubleshooting\\nand improvement.\\n5. Log Analysis: Maintain comprehensive logs for each prediction request and response, including\\ninput data, output predictions, response times, and encountered errors. Regularly review logs to\\nidentify patterns and areas for improvement.\\n6. Alerting Mechanisms: Set up automated alerting systems to notify stakeholders of any anomalies\\nor deviations from expected performance metrics. Integrate alerts with communication tools like\\nSlack, PagerDuty, or email for timely responses.\\n7. Feedback Loop: Establish a feedback loop with end-users to gather insights on model performance\\nand user satisfaction. Use this feedback to continuously refine and improve the model.\\n8. Security Monitoring: Implement robust security measures to monitor for threats, including\\nunauthorised access, data breaches, and adversarial attacks. Use encryption, access control, and\\nregular security audits to protect the model and data.\\n9. Drift Detection: Continuously monitor for data and concept drift using statistical tests and\\ndrift detectors. Regularly evaluate the model on holdout datasets to detect changes in input data\\ndistribution or model performance.\\n10. Model Versioning: Maintain version control for different iterations of the model. Track perfor-\\nmance metrics for each version to ensure that the best-performing model is in production.\\n72\\n11. Documentation and Reporting: Keep detailed documentation of monitoring procedures, met-\\nrics, and findings. Generate regular reports to provide stakeholders with insights into the model’s\\nperformance and maintenance activities.\\n12. Periodic Review and Update: Regularly assess and update the monitoring processes to incor-\\nporate new techniques, tools, and best practices, ensuring the monitoring system remains effective\\nand up-to-date.\\n9.2\\nContinuous Monitoring of Model Performance\\nWhile large language model (LLM) applications undergo some form of evaluation, continuous monitoring\\nremains inadequately implemented in most cases. This section outlines the components necessary to\\nestablish an effective monitoring programme aimed at safeguarding users and preserving brand integrity.\\n9.2.1\\nFunctional Monitoring\\nInitially, it is crucial to monitor fundamental metrics consistently. This includes tracking metrics such\\nas request volume, response times, token utilisation, costs incurred, and error rates.\\n9.2.2\\nPrompt Monitoring\\nFollowing functional metrics, attention should be directed towards monitoring user-generated prompts\\nor inputs. Metrics like readability can provide valuable insights. LLM evaluators should be employed to\\ndetect potential toxicity in responses. Additionally, metrics such as embedding distances from reference\\nprompts prove insightful, ensuring adaptability to varying user interactions over time.\\nIntroducing a new evaluation category involves identifying adversarial attempts or malicious prompt\\ninjections, often overlooked in initial evaluations. Comparison against reference sets of known adversarial\\nprompts helps identify and flag malicious activities. Evaluative LLMs play a crucial role in classifying\\nprompts as benign or malicious.\\n9.2.3\\nResponse Monitoring\\nMonitoring responses involves several critical checks to ensure alignment with expected outcomes. Pa-\\nrameters such as relevance, coherence (hallucination), topical alignment, sentiment, and their evolution\\nover time are essential. Metrics related to toxicity and harmful output require frequent monitoring due\\nto their critical impact. Prompt leakage represents an adversarial tactic wherein sensitive prompt in-\\nformation is illicitly extracted from the application’s stored data. Monitoring responses and comparing\\nthem against the database of prompt instructions can help detect such breaches. Embedding distance\\nmetrics are particularly effective in this regard. Regular testing against evaluation datasets provides\\nbenchmarks for accuracy and highlights any performance drift over time. Tools capable of managing\\nembeddings allow exportation of underperforming output datasets for targeted improvements.\\n9.2.4\\nAlerting Mechanisms and Thresholds\\nEffective monitoring necessitates well-calibrated alerting thresholds to avoid excessive false alarms. Im-\\nplementing multivariate drift detection and alerting mechanisms can enhance accuracy. Consideration\\nof false alarm rates and best practices for setting thresholds is paramount for effective monitoring sys-\\ntem design. Alerting features should include integration with communication tools such as Slack and\\nPagerDuty. Some systems offer automated response blocking in case of alerts triggered by problematic\\nprompts. Similar mechanisms can be employed to screen responses for personal identifiable information\\n(PII), toxicity, and other quality metrics before delivery to users. Custom metrics tailored to specific\\napplication nuances or innovative insights from data scientists can significantly enhance monitoring ef-\\nficacy. Flexibility to incorporate such metrics is essential to adapt to evolving monitoring needs and\\nadvancements in the field.\\n73\\n9.2.5\\nMonitoring User Interface (UI)\\nThe monitoring system’s UI is pivotal, typically featuring time-series graphs of monitored metrics. Dif-\\nferentiated UIs facilitate in-depth analysis of alert trends, aiding root cause analysis.\\nAdvanced UI\\ncapabilities may include visualisations of embedding spaces through clustering and projections, provid-\\ning insights into data patterns and relationships. Mature monitoring systems categorise data by users,\\nprojects, and teams, ensuring role-based access control (RBAC) to protect sensitive information. Op-\\ntimising alert analysis within the UI interface remains an area where improvements can significantly\\nreduce false alarm rates and enhance operational efficiency.\\n9.3\\nUpdating LLM Knowledge\\nTo improve the knowledge base of an LLM, continued pretraining is used to help LLM evolve with the\\nlatest knowledge and information. The world and language are constantly evolving. New information\\nemerges, trends shift, and cultural references change. LLMs trained on static data can become outdated,\\nleading to:\\n• Factual Errors: Outdated information can cause LLMs to provide inaccurate responses.\\n• Irrelevance: Models might miss the context of current events or use outdated references.\\n• Bias Perpetuation: Biases present in training data can become entrenched if not addressed\\nthrough updates.\\n9.3.1\\nRetraining Methods\\n• Periodic Retraining: This involves refreshing the model’s knowledge base at regular intervals\\n(weekly, monthly, yearly) with new data. This is a straightforward method but requires a steady\\nstream of high-quality, unbiased data.\\n• Trigger-Based Retraining: This approach monitors the LLM’s performance. When metrics like\\naccuracy or relevance fall below a certain threshold, a retraining process is triggered. This method\\nis more dynamic but requires robust monitoring systems and clear performance benchmarks.\\n9.3.2\\nAdditional Methods\\n• Fine-Tuning: LLMs can be fine-tuned for specific tasks by training them on smaller, domain-\\nspecific datasets. This allows for specialisation without complete retraining.\\n• Active Learning: This approach involves selectively querying the LLM to identify areas where\\nit lacks knowledge. The retrieved information is then used to update the model.\\n9.3.3\\nKey Considerations\\n• Data Quality and Bias: New training data must be carefully curated to ensure quality and\\nmitigate bias. Techniques like human annotation and fairness checks are crucial.\\n• Computational Cost: Retraining LLMs can be computationally expensive, requiring significant\\nresources. Optimisations like transfer learning (using pre-trained models as a starting point) can\\nhelp reduce costs.\\n• Downtime: Retraining often takes time, leading to LLM downtime. Strategies like rolling updates\\nor deploying multiple models can minimise service disruptions.\\n• Version Control: Tracking different versions of the LLM and their training data is essential for\\nrollbacks in case of performance issues.\\n74\\n9.4\\nThe Future of LLM Updates\\nResearch is ongoing to develop more efficient and effective LLM update strategies. One promising area\\nis continuous learning, where LLMs can continuously learn and adapt from new data streams without\\nretraining from scratch. Continuous learning aims to reduce the need for frequent full-scale retraining by\\nenabling models to update incrementally with new information. This approach can significantly enhance\\nthe model’s ability to remain current with evolving knowledge and language use, improving its long-term\\nperformance and relevance.\\nInnovations in transfer learning and meta-learning are also contributing to advancements in LLM updates.\\nThese techniques allow models to leverage pre-existing knowledge and adapt quickly to new tasks or\\ndomains with minimal additional training.\\nBy integrating these advanced learning methods, future\\nLLMs can become more adaptable and efficient in processing and understanding new information.\\nFurthermore, ongoing improvements in hardware and computational resources will support more frequent\\nand efficient updates. As processing power increases and becomes more accessible, the computational\\nburden of updating large models will decrease, enabling more regular and comprehensive updates.\\nCollaboration between academia and industry is vital in driving these advancements. By sharing research\\nfindings and best practices, the field can collectively move towards more robust and efficient LLM update\\nmethodologies, ensuring that models remain accurate, relevant, and valuable over time.\\n75\\nChapter 10\\nIndustrial Fine-Tuning Platforms\\nand Frameworks for LLMs\\nThe evolution of fine-tuning techniques has been propelled by leading tech companies and platforms that\\nhave introduced innovative frameworks and services. Companies like HuggingFace, Amazon Web Services\\n(AWS), Microsoft Azure, and OpenAI have developed tools and platforms that simplify and democratise\\nthe fine-tuning process. These advancements have not only lowered the barrier to entry for leveraging\\nstate-of-the-art AI models but have also enabled a wide range of applications across various industries,\\nfrom healthcare and finance to customer service and content creation. Each of these platforms offers\\nunique capabilities that cater to different needs, whether it be through automated fine-tuning workflows,\\nscalable cloud-based training environments, or accessible API interfaces for deploying custom models.\\nHuggingFace, for example, has made significant strides with its Transformers library1 and tools like Au-\\ntotrain2 and SetFit, which allow users to fine-tune models with minimal coding and data. Their platform\\nprovides a robust infrastructure that supports both the research community and industry practitioners,\\nfacilitating the rapid development and deployment of custom AI solutions. Similarly, AWS’s SageMaker3\\nand SetFit4 provides an extensive suite of services that cover the entire machine learning lifecycle, from\\ndata preparation and training to model deployment and optimisation, making it a comprehensive solu-\\ntion for enterprise-level applications.\\nOn the other hand, Microsoft Azure integrates its fine-tuning capabilities with enterprise-grade tools\\nand services, offering solutions like Azure Machine Learning and the Azure OpenAI Service that cater to\\nlarge organisations looking to incorporate advanced AI into their operations. Azure’s focus on MLOps\\nand seamless integration with other Azure services ensures that fine-tuned models can be efficiently de-\\nployed and maintained in production environments. Meanwhile, OpenAI has pioneered the concept of\\n”fine-tuning as a service” allowing businesses to leverage their powerful models like GPT-4 through a\\nuser-friendly API 5, enabling custom model adaptations without the need for in-house AI expertise or\\ninfrastructure.\\nThe collective efforts of these tech companies have not only enhanced the efficiency and scalability of\\nfine-tuning but also democratised access to sophisticated AI tools. By reducing the technical barriers\\nand providing comprehensive, user-friendly platforms, these innovations have enabled a wider range of\\nindustries to deploy advanced AI models tailored to their specific needs. Tables 10.1 and 10.2 offer a\\nquick comparison of LLM fine-tuning tools and frameworks from different providers.\\n1https://huggingface.co/docs/transformers/en/index/\\n2https://huggingface.co/autotrain\\n3https://huggingface.co/autotrain\\n4https://aws.amazon.com/sagemaker/\\n5https://platform.openai.com/docs/guides/fine-tuning/fine-tuning-integrations\\n76\\nParameter\\nNVIDIA\\nNeMo\\nHugging\\nFace\\nAutoTrain\\nAPI\\nAmazon\\nBedrock\\nAWS\\nSage-\\nMaker\\nJump-\\nStart\\nHugging\\nFace\\nTrainer API\\nPrimary Use\\nCase\\nCustom\\nfine-\\ntuning of LLMs\\nwith\\nadvanced\\nNVIDIA GPUs.\\nFine-tuning\\nand deployment\\nof\\nLLMs\\nwith\\nminimal code.\\nFine-tuning and\\ndeploying LLMs\\non AWS infras-\\ntructure.\\nSimplified\\nfine-\\ntuning and de-\\nployment within\\nthe AWS ecosys-\\ntem.\\nManual\\nfine-\\ntuning of LLMs\\nwith\\ndetailed\\ncontrol\\nover\\ntraining\\npro-\\ncesses.\\nModel Support\\nSupports a vari-\\nety of large, pre-\\ntrained\\nmodels,\\nincluding Mega-\\ntron series.\\nSupports a wide\\nrange\\nof\\npre-\\ntrained\\nmodels\\nfrom\\nthe\\nHug-\\nging Face model\\nhub.\\nSupports\\nvari-\\nous\\nLLMs\\nlike\\nAmazon\\nTitan\\nand\\nthird-party\\nmodels.\\nPre-trained\\nmodels\\nfrom\\nAWS and part-\\nners; integration\\nwith\\ncustom\\nmodels.\\nSupports a vast\\narray of models\\nfrom\\nthe\\nHug-\\nging Face model\\nhub.\\nData Handling\\nUsers\\nprovide\\ntask-specific\\ndata\\nfor\\nfine-\\ntuning,\\npro-\\ncessed\\nusing\\nNVIDIA’s\\nin-\\nfrastructure.\\nUploads\\ndatasets\\nvia\\na\\nsimple\\ninter-\\nface; AutoTrain\\nhandles\\npre-\\nprocessing\\nand\\nmodel training.\\nData is uploaded\\nand\\nmanaged\\nwithin the AWS\\nenvironment;\\nintegrates\\nwith\\nAWS data ser-\\nvices.\\nUploads\\nand\\nprocesses\\ndata\\nwithin\\nAWS;\\nsupports various\\ndata formats.\\nUsers\\nmanually\\npreprocess data\\nand\\nmanage\\ntraining steps.\\nCustomisation\\nLevel\\nHigh;\\nextensive\\ncontrol\\nover\\nfine-tuning pro-\\ncess and model\\nparameters.\\nModerate; auto-\\nmated\\nprocess\\nwith\\nsome\\ncustomisation\\noptions.\\nHigh;\\ndetailed\\nconfiguration\\nand\\nintegration\\nwith AWS ser-\\nvices.\\nModerate;\\npre-configured\\nsettings\\nwith\\nsome customisa-\\ntion available.\\nVery\\nHigh;\\ndetailed\\ncon-\\ntrol\\nover\\nevery\\naspect\\nof\\nfine-\\ntuning.\\nScalability\\nHigh;\\nleverages\\nNVIDIA’s GPU\\ncapabilities\\nfor\\nefficient scaling.\\nHigh;\\nscalable\\nvia\\nHugging\\nFace’s\\ncloud\\ninfrastructure.\\nVery\\nHigh;\\nscalable\\nacross\\nAWS’s extensive\\ncloud infrastruc-\\nture.\\nHigh;\\nscalable\\nwithin the AWS\\ncloud\\necosys-\\ntem.\\nHigh; scalability\\ndepends on the\\ninfrastructure\\nused (e.g., local\\nvs. cloud).\\nDeployment\\nOptions\\nOn-premises\\nor\\ncloud\\nde-\\nployment\\nvia\\nNVIDIA infras-\\ntructure.\\nDeployed\\nvia\\nHugging\\nFace’s\\ncloud or can be\\nexported for lo-\\ncal deployment.\\nIntegrated\\ninto\\nAWS\\nservices,\\neasily\\ndeploy-\\nable\\nacross\\nAWS’s\\nglobal\\ninfrastructure.\\nAWS\\ncloud\\ndeployment;\\nintegrates\\nwith\\nother AWS ser-\\nvices.\\nDeployable\\nlo-\\ncally,\\nin cloud,\\nor\\nexported\\nto\\nother platforms.\\nIntegration with\\nEcosystem\\nDeep integration\\nwith\\nNVIDIA\\ntools\\n(e.g.,\\nTensorRT)\\nand\\nGPU-based\\nworkflows.\\nIntegrates\\nwell\\nwith\\nthe\\nHugging\\nFace\\necosystem\\nand\\nother ML tools.\\nSeamless\\ninte-\\ngration\\nwith\\nAWS\\nser-\\nvices\\n(e.g.,\\nS3,\\nLambda,\\nSage-\\nMaker).\\nStrong\\nintegra-\\ntion with AWS\\nservices;\\neasy\\nto connect with\\ndata\\npipelines\\nand analytics.\\nIntegrates\\nwith\\nHugging\\nFace\\necosystem\\nand\\nother\\nPython-\\nbased ML tools.\\nData Privacy\\nUsers\\nmust\\nensure\\ndata\\nprivacy\\ncompli-\\nance;\\nNVIDIA\\nhandles\\ndata\\nduring\\nprocess-\\ning.\\nData\\nhandled\\nwithin\\nHugging\\nFace’s\\nenviron-\\nment;\\nprivacy\\ndepends\\non\\ndata\\nhandling\\npractices.\\nStrong\\nfocus\\non data privacy\\nwithin\\nAWS\\nenvironment;\\ncompliant\\nwith\\nvarious\\nstan-\\ndards.\\nStrong\\nAWS\\nprivacy\\nand\\nsecurity\\nmea-\\nsures; compliant\\nwith\\nindustry\\nstandards.\\nUser-managed;\\ndepends\\non\\nwhere the mod-\\nels and data are\\nhosted.\\nTarget Users\\nEnterprises and\\ndevelopers need-\\ning\\nadvanced\\ncustomisation\\nand\\nperfor-\\nmance in LLM\\nfine-tuning.\\nDevelopers\\nand\\nbusinesses look-\\ning\\nfor\\neasy,\\nautomated LLM\\nfine-tuning solu-\\ntions.\\nBusinesses\\nand\\ndevelopers inte-\\ngrated\\ninto\\nor\\nseeking to lever-\\nage AWS cloud\\nservices.\\nEnterprises and\\ndevelopers seek-\\ning\\nstreamlined\\nAI/ML solutions\\nwithin AWS.\\nResearchers,\\ndevelopers,\\nand\\nML\\nengineers\\nneeding detailed\\ncontrol\\nover\\ntraining.\\nLimitations\\nHigh\\nresource\\ndemand\\nand\\npotential\\ncosts;\\ndependency\\non\\nNVIDIA ecosys-\\ntem.\\nLess\\ncontrol\\nover fine-tuning\\nspecifics; cloud-\\nbased,\\nmay\\nnot suit all on-\\npremises needs.\\nDependency\\non\\nAWS;\\npo-\\ntential\\nvendor\\nlock-in,\\ncost\\nmanagement\\ncomplexity.\\nLimited\\nto\\nAWS\\nservices;\\npre-configured\\noptions\\nmay\\nlimit deep cus-\\ntomisation.\\nRequires\\ntech-\\nnical\\nexpertise;\\nmore\\ncomplex\\nsetup and man-\\nagement.\\nTable 10.1: Detailed Comparison of LLM Fine-Tuning Platforms (Part I). This table provides a compre-\\nhensive comparison of various fine-tuning tools for Large Language Models (LLMs), including NVIDIA\\nNeMo, Hugging Face AutoTrain API, Amazon Bedrock, AWS SageMaker JumpStart, and Hugging Face\\nTrainer API. It covers multiple aspects such as the primary use case, model support, data handling,\\ncustomisation level, scalability, deployment options, integration with the ecosystem, data privacy, target\\nusers, and limitations for each tool.\\n77\\nParameter\\nOpenAI\\nFine-\\nTuning API\\nGoogle Vertex AI\\nStudio\\nMicrosoft\\nAzure\\nAI Studio\\nLangChain\\nPrimary Use\\nCase\\nAPI-based\\nfine-\\ntuning\\nfor\\nOpenAI\\nmodels with custom\\ndatasets.\\nEnd-to-end\\nML\\nmodel\\ndevelopment\\nand\\ndeployment\\nwithin Google Cloud.\\nEnd-to-end AI devel-\\nopment,\\nfine-tuning,\\nand\\ndeployment\\non\\nAzure.\\nBuilding applications\\nusing\\nLLMs\\nwith\\nmodular\\nand\\ncus-\\ntomisable workflows.\\nModel Support\\nLimited\\nto\\nOpenAI\\nmodels\\nlike\\nGPT-3\\nand GPT-4.\\nSupports\\nGoogle’s\\npre-trained\\nmodels\\nand\\nuser-customised\\nmodels.\\nSupports Microsoft’s\\nmodels\\nand\\ncustom\\nmodels\\nfine-tuned\\nwithin Azure.\\nSupports integration\\nwith\\nvarious\\nLLMs\\nand\\nAI\\ntools\\n(e.g.,\\nOpenAI, GPT-4, Co-\\nhere).\\nData Handling\\nUsers upload datasets\\nvia\\nAPI;\\nOpenAI\\nhandles\\npreprocess-\\ning and fine-tuning.\\nData managed within\\nGoogle Cloud;\\nsup-\\nports\\nmultiple\\ndata\\nformats.\\nData\\nintegrated\\nwithin Azure ecosys-\\ntem; supports various\\nformats and sources.\\nData handling is flex-\\nible,\\ndependent\\non\\nthe specific LLM and\\nintegration used.\\nCustomisation\\nLevel\\nModerate; focuses on\\nease of use with lim-\\nited deep customisa-\\ntion.\\nHigh;\\noffers custom\\nmodel\\ntraining\\nand\\ndeployment with de-\\ntailed configuration.\\nHigh; extensive cus-\\ntomisation\\noptions\\nthrough\\nAzure’s\\nAI\\ntools.\\nVery High; allows de-\\ntailed\\ncustomisation\\nof workflows, models,\\nand data processing.\\nScalability\\nHigh;\\nscalable\\nthrough\\nOpenAI’s\\ncloud infrastructure.\\nVery High; leverages\\nGoogle\\nCloud’s\\nin-\\nfrastructure for scal-\\ning.\\nVery High;\\nscalable\\nacross Azure’s global\\ninfrastructure.\\nHigh; scalability de-\\npends on the specific\\ninfrastructure\\nand\\nmodels used.\\nDeployment\\nOptions\\nDeployed via API, in-\\ntegrated into applica-\\ntions using OpenAI’s\\ncloud.\\nDeployed\\nwithin\\nGoogle\\nCloud;\\nin-\\ntegrates\\nwith\\nother\\nGCP services.\\nDeployed\\nwithin\\nAzure;\\nintegrates\\nwith Azure’s suite of\\nservices.\\nDeployed\\nwithin\\ncustom\\ninfrastruc-\\nture; integrates with\\nvarious\\ncloud\\nand\\non-premises services.\\nIntegration with\\nEcosystem\\nLimited\\nto\\nOpenAI\\necosystem; integrates\\nwell\\nwith\\napps\\nvia\\nAPI.\\nSeamless\\nintegration\\nwith\\nGoogle\\nCloud\\nservices\\n(e.g.,\\nBig-\\nQuery, AutoML).\\nDeep integration with\\nAzure’s services (e.g.,\\nData Factory, Power\\nBI).\\nFlexible\\nintegration\\nwith multiple tools,\\nAPIs,\\nand\\ndata\\nsources.\\nData Privacy\\nManaged by OpenAI;\\nusers\\nmust\\nconsider\\ndata transfer and pri-\\nvacy implications.\\nStrong\\nprivacy\\nand\\nsecurity\\nmeasures\\nwithin Google Cloud\\nenvironment.\\nStrong\\nprivacy\\nand\\nsecurity\\nmeasures\\nwithin\\nAzure\\nenvi-\\nronment.\\nDependent on the in-\\ntegrations and infras-\\ntructure used;\\nusers\\nmanage privacy.\\nTarget Users\\nDevelopers\\nand\\nen-\\nterprises\\nlooking\\nfor\\nstraightforward,\\nAPI-based\\nLLM\\nfine-tuning.\\nDevelopers and busi-\\nnesses integrated into\\nGoogle Cloud or seek-\\ning to leverage GCP.\\nEnterprises\\nand\\nde-\\nvelopers\\nintegrated\\ninto Azure or seeking\\nto\\nleverage\\nAzure’s\\nAI tools.\\nDevelopers\\nneeding\\nto\\nbuild\\ncomplex,\\nmodular\\nLLM-based\\napplications\\nwith\\ncustom workflows.\\nLimitations\\nLimited\\ncustomisa-\\ntion; dependency on\\nOpenAI’s infrastruc-\\nture; potential cost.\\nLimited\\nto\\nGoogle\\nCloud ecosystem; po-\\ntential cost and ven-\\ndor lock-in.\\nLimited\\nto\\nAzure\\necosystem;\\npotential\\ncost\\nand\\nvendor\\nlock-in.\\nComplexity in chain-\\ning multiple models\\nand data sources; re-\\nquires more setup.\\nTable 10.2: Detailed Comparison of LLM Fine-Tuning Platforms (Part II). This table continues the\\ncomparison of LLM fine-tuning tools, focusing on OpenAI Fine-Tuning API, Google Vertex AI Studio,\\nMicrosoft Azure AI Studio, and LangChain.\\nIt evaluates the tools based on the primary use case,\\nmodel support, data handling, customisation level, scalability, deployment options, integration with the\\necosystem, data privacy, target users, and limitations, offering a complete view of their capabilities and\\nconstraints.\\n10.1\\nAutotrain\\nAutotrain is HuggingFace’s innovative platform that automates the fine-tuning of large language models,\\nmaking it accessible even to those with limited machine learning expertise. The complexity and resource\\ndemands of fine-tuning LLMs can be daunting, but Autotrain simplifies the process by handling the most\\nchallenging aspects, such as data preparation, model configuration, and hyperparameter optimisation.\\nThis automation is particularly valuable for small teams or individual developers who need to deploy\\ncustom LLMs quickly and efficiently.\\n10.1.1\\nSteps Involved in Fine-Tuning Using Autotrain\\nFollowing are the steps involved in fine-tuning LLMs using Autotrain. Figure 10.1 represents the visual\\nworkflow.\\n• Dataset Upload and Model Selection:\\n78\\nFigure 10.1: Overview of the Autotrain Workflow. This diagram illustrates the step-by-step process\\nwithin the Autotrain system, beginning with the upload of datasets and model selection by users. The\\nworkflow then moves to data preparation and model configuration, followed by automated hyperpa-\\nrameter tuning to optimise model performance. The fine-tuning phase adjusts the model based on the\\nprovided datasets, culminating in the deployment of the fully fine-tuned model for practical use.\\n– Users begin by uploading their datasets to the Autotrain platform.\\n– They then select a pre-trained model from the extensive HuggingFace Model Hub.\\n• Data Preparation:\\n– Autotrain automatically processes the uploaded data, including tasks like tokenization to\\nconvert text into a format the LLM can understand.\\n• Model Configuration:\\n– The platform configures the model for fine-tuning, setting up the training environment and\\nnecessary parameters.\\n• Automated Hyperparameter Tuning:\\n– Autotrain explores various hyperparameter configurations (such as learning rate, batch size,\\nand sequence length) and selects the best-performing ones.\\n• Fine-Tuning:\\n– The model is fine-tuned on the prepared data with the optimised hyperparameters.\\n• Deployment:\\n– Once fine-tuning is complete, the model is ready for deployment in various NLP applications,\\nsuch as text generation, completion, and language translation.\\n79\\n10.1.2\\nBest Practices of Using Autotrain\\n• Data Quality: Ensure high-quality, well-labelled data for better model performance.\\n• Model Selection: Choose pre-trained models that are well-suited to your specific task to minimize\\nfine-tuning effort.\\n• Hyperparameter Optimisation: Leverage Autotrain’s automated hyperparameter tuning to\\nachieve optimal performance without manual intervention.\\n10.1.3\\nChallenges of Using Autotrain\\n• Data Privacy: Ensuring the privacy and security of sensitive data during the fine-tuning process.\\n• Resource Constraints: Managing computational resources effectively, especially in environments\\nwith limited access to powerful hardware.\\n• Model Overfitting: Avoiding overfitting by ensuring diverse and representative training data\\nand using appropriate regularization techniques.\\n10.1.4\\nWhen to Use Autotrain\\n1. Lack of Deep Technical Expertise: Ideal for individuals or small teams without extensive\\nmachine learning or LLM background who need to fine-tune models quickly and effectively.\\n2. Quick Prototyping and Deployment: Suitable for rapid development cycles where time is\\ncritical, such as proof-of-concept projects or MVPs.\\n3. Resource-Constrained Environments: Useful for scenarios with limited computational re-\\nsources or where a quick turnaround is necessary.\\nIn summary, Autotrain is an excellent tool for quick, user-friendly fine-tuning of LLMs for standard NLP\\ntasks, especially in environments with limited resources or expertise. However, it may not be suitable\\nfor highly specialised applications or those requiring significant customisation and scalability.\\n10.1.5\\nTutorials\\n1. How To Create HuggingFace Custom AI Models Using AutoTrain\\n2. Finetune models with HuggingFace AutoTrain\\n10.2\\nTransformers Library and Trainer API\\nThe Transformers Library by HuggingFace stands out as a pivotal tool for fine-tuning large language\\nmodels (LLMs) such as BERT, GPT-3, and GPT-4. This comprehensive library offers a wide array of\\npre-trained models tailored for various LLM tasks, making it easier for users to adapt these models to\\nspecific needs with minimal effort. Whether you’re fine-tuning for tasks like sentiment analysis, text\\nclassification, or generating customer support responses, the library simplifies the process by allowing\\nseamless model selection from the HuggingFace Model Hub and straightforward customisation through\\nits high-level APIs.\\nCentral to the fine-tuning process within the Transformers Library is the Trainer API. This API includes\\nthe Trainer class, which automates and manages the complexities of fine-tuning LLMs. After completing\\ndata preprocessing, the Trainer class streamlines the setup for model training, including data handling,\\noptimisation, and evaluation. Users only need to configure a few parameters, such as learning rate and\\nbatch size, and the API takes care of the rest. However, it’s crucial to note that running Trainer.train()\\ncan be resource-intensive and slow on a CPU. For efficient training, a GPU or TPU is recommended.\\nPlatforms like Google Colab provide free access to these resources, making it feasible for users without\\nhigh-end hardware to fine-tune models effectively.\\n80\\nThe Trainer API also supports advanced features like distributed training and mixed precision, which\\nare essential for handling the large-scale computations required by modern LLMs. Distributed training\\nallows the fine-tuning process to be scaled across multiple GPUs or nodes, significantly reducing training\\ntime. Mixed precision training, on the other hand, optimises memory usage and computation speed by\\nusing lower precision arithmetic without compromising model performance. HuggingFace’s dedication to\\naccessibility is evident in the extensive documentation and community support they offer, enabling users\\nof all expertise levels to fine-tune LLMs. This democratisation of advanced NLP technology empowers\\ndevelopers and researchers to deploy sophisticated, fine-tuned models for a wide range of applications,\\nfrom specialised language understanding to large-scale data processing.\\n10.2.1\\nLimitations of the Transformers Library and Trainer API\\n• Limited Customisation for Advanced Users: While the Trainer API simplifies many aspects\\nof training, it might not offer the deep customisation that advanced users or researchers might need\\nfor novel or highly specialised applications.\\n• Learning Curve: Despite the simplified API, there is still a learning curve associated with un-\\nderstanding and effectively using the Transformers Library and Trainer API, particularly for those\\nnew to NLP and LLM.\\n• Integration Limitations: The seamless integration and ease of use are often tied to the Hug-\\ngingFace ecosystem, which might not be compatible with all workflows or platforms outside their\\nenvironment.\\nIn summary, the Transformers Library and Trainer API provide robust, scalable solutions for fine-tuning\\nLLMs across a range of applications, offering ease of use and efficient training capabilities. However, users\\nmust be mindful of the resource requirements and potential limitations in customisation and complexity\\nmanagement.\\n10.3\\nOptimum: Enhancing LLM Deployment Efficiency\\nOptimum6 is HuggingFace’s tool designed to optimise the deployment of large language models (LLMs)\\nby enhancing their efficiency across various hardware platforms. As LLMs grow in size and complexity,\\ndeploying them in a cost-effective and performant manner becomes increasingly challenging. Optimum\\naddresses these challenges by applying a range of hardware-specific optimisations, such as quantisation,\\npruning, and model distillation, which reduce the model’s size and improve inference speed without\\nsignificantly affecting accuracy. The following are the key techniques supported by Optimum:\\n• Quantisation: Quantisation is one of the key techniques supported by Optimum. This process in-\\nvolves converting the model’s weights from high-precision floating-point numbers to lower-precision\\nformats, such as int8 or float16. This reduction in precision decreases the model’s memory foot-\\nprint and computational requirements, enabling faster execution and lower power consumption,\\nespecially on edge devices and mobile platforms. Optimum automates the quantisation process,\\nmaking it accessible to users who may not have expertise in low-level hardware optimisation.\\n• Pruning: Pruning is another critical optimisation strategy offered by Optimum. It involves iden-\\ntifying and removing less significant weights from the LLM, reducing its overall complexity and\\nsize. This leads to faster inference times and lower storage needs, which are particularly beneficial\\nfor deploying models in environments with limited computational resources. Optimum’s pruning\\nalgorithms carefully eliminate these redundant weights while maintaining the model’s performance,\\nensuring that it continues to deliver high-quality results even after optimisation.\\n• Model Distillation: In addition to these techniques, Optimum supports model distillation, a\\nprocess where a smaller, more efficient model is trained to replicate the behaviour of a larger, more\\ncomplex model. This distilled model retains much of the knowledge and capabilities of the original\\nwhile being significantly lighter and faster. Optimum provides tools to facilitate the distillation\\nprocess, allowing users to create compact LLMs that are well-suited for real-time applications. By\\noffering a comprehensive suite of optimisation tools, Optimum ensures that HuggingFace’s LLMs\\ncan be deployed effectively across a wide range of environments, from powerful cloud servers to\\nresource-constrained edge devices.\\n6https://huggingface.co/docs/optimum/en/index\\n81\\n10.3.1\\nBest Practices of Using Optimum\\n• Understand Hardware Requirements: Assess the target deployment environment (e.g., edge\\ndevices, cloud servers) to optimise model configuration accordingly.\\n• Iterative Optimisation: Experiment with different optimisation techniques (quantisation levels,\\npruning thresholds) to find the optimal balance between model size, speed, and accuracy.\\n• Validation and Testing: Validate optimised models thoroughly to ensure they meet performance\\nand accuracy requirements across different use cases.\\n• Documentation and Support: Refer to HuggingFace’s resources for detailed guidance on using\\nOptimum’s tools effectively, and leverage community support for troubleshooting and best practices\\nsharing.\\n• Continuous Monitoring: Monitor deployed models post-optimisation to detect any performance\\ndegradation and adjust optimisation strategies as needed to maintain optimal performance over\\ntime.\\n10.3.2\\nTutorials\\n1. An Introduction to Using Transformers and Hugging Face\\n10.4\\nAmazon SageMaker JumpStart\\nAmazon SageMaker JumpStart is a feature within the SageMaker ecosystem designed to simplify and\\nexpedite the fine-tuning of large language models (LLMs). It provides users with a rich library of pre-\\nbuilt models and solutions that can be quickly customised for various use cases. This tool is particularly\\nvaluable for organisations looking to deploy NLP solutions efficiently without deep expertise in machine\\nlearning or the extensive computational resources typically required for training LLMs from scratch. The\\narchitecture depicted in Figure 10.2 outlines a comprehensive pipeline for the fine-tuning and deployment\\nof large language models (LLMs) Utilising AWS services.\\n10.4.1\\nSteps Involved in Using JumpStart\\n• Data Preparation and Preprocessing:\\n– Data Storage: Begin by securely storing raw datasets in Amazon S3, AWS’s scalable object\\nstorage service.\\n– Preprocessing: Utilise the EMR Serverless framework with Apache Spark for efficient data\\npreprocessing. This step refines and prepares the raw data for subsequent model training and\\nevaluation.\\n– Data Refinement: Store the processed dataset back into Amazon S3 after preprocessing,\\nensuring accessibility and readiness for the next stages.\\n• Model Fine-Tuning with SageMaker JumpStart:\\n– Model Selection: Choose from a variety of pre-built models and solutions available through\\nSageMaker JumpStart’s extensive library, tailored for tasks such as sentiment analysis, text\\ngeneration, or customer support automation.\\n– Fine-Tuning Execution: Utilise Amazon SageMaker’s capabilities, integrated with Sage-\\nMaker JumpStart, to fine-tune the selected model. This involves adjusting parameters and\\nconfigurations to optimise the model’s performance for specific use cases.\\n– Workflow Simplification: Leverage pre-built algorithms and model templates provided by\\nSageMaker JumpStart to streamline the fine-tuning workflow, reducing the time and effort\\nrequired for deployment.\\n• Model Deployment and Hosting:\\n82\\nFigure 10.2: A step-by-step workflow illustrating the Amazon SageMaker JumpStart process, starting\\nfrom data preprocessing using EMR Serverless Spark to the fine-tuning of LLMs, and ending with model\\ndeployment on Amazon SageMaker Endpoints. (adapted from [85])\\n– Deployment Setup: Deploy the fine-tuned model using Amazon SageMaker’s endpoint\\ndeployment capabilities. This setup ensures that the model is hosted in a scalable environment\\ncapable of handling real-time predictions efficiently.\\n– Scalability: Benefit from AWS’s infrastructure scalability, allowing seamless scaling of re-\\nsources to accommodate varying workloads and operational demands.\\n– Efficiency and Accessibility: Ensure that the deployed model is accessible via SageMaker\\nendpoints, enabling efficient integration into production applications for real-time inference\\ntasks.\\n10.4.2\\nBest Practices for Using JumpStart\\n• Robust Data Management: Maintain secure and organised data storage practices in Amazon\\nS3, facilitating efficient data access and management throughout the pipeline.\\n• Cost-Effective Processing: Utilise serverless computing frameworks like EMR Serverless with\\nApache Spark for cost-effective and scalable data preprocessing.\\n• Optimised Fine-Tuning: Capitalise on SageMaker JumpStart’s pre-built models and algorithms\\nto expedite and optimise the fine-tuning process, ensuring optimal model performance without\\n83\\nextensive manual configuration.\\n• Continuous Monitoring and Optimisation: Implement robust monitoring mechanisms post-\\ndeployment to track model performance metrics. This allows for timely optimisations and adjust-\\nments to maintain accuracy and efficiency over time.\\n• Integration with AWS Services: Leverage AWS’s comprehensive suite of services and inte-\\ngration capabilities to create end-to-end pipelines that ensure reliable and scalable deployment of\\nlarge-scale language models across diverse operational environments.\\n10.4.3\\nLimitations of Using JumpStart\\n• Limited Customisation: While JumpStart simplifies the process for common use cases, it may\\noffer limited flexibility for highly specialised or complex applications that require significant cus-\\ntomisation beyond the provided templates and workflows.\\n• Dependency on AWS Ecosystem: JumpStart is tightly integrated with AWS services, which\\nmay pose challenges for users who prefer or need to operate in multi-cloud environments or those\\nwith existing infrastructure outside of AWS.\\n• Resource Costs: Utilising SageMaker’s scalable resources for fine-tuning LLMs, especially large\\nmodels, can incur substantial costs, which might be a barrier for smaller organisations or those\\nwith limited budgets.\\n10.4.4\\nTutorials\\n1. Fine-Tuning LLaMA 2 with Amazon SageMaker JumpStart\\n2. LLM Agents Using AWS SageMaker JumpStart Foundation Models\\n10.5\\nAmazon Bedrock\\nAmazon Bedrock7 is a fully managed service designed to simplify access to high-performing foundation\\nmodels (FMs) from top AI innovators like AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability\\nAI, and Amazon. It provides a unified API that integrates these models and offers extensive capabilities\\nfor developing secure, private, and responsible generative AI applications. With Amazon Bedrock, users\\ncan effortlessly experiment with and assess leading FMs tailored to their specific needs. The service sup-\\nports private customisation of models through fine-tuning and Retrieval Augmented Generation (RAG),\\nenabling the creation of intelligent agents that leverage enterprise data and systems. Amazon Bedrock’s\\nserverless architecture allows for quick deployment, seamless integration, and secure customisation of\\nFMs without the burden of infrastructure management, Utilising AWS tools to deploy these models into\\napplications efficiently and securely.\\n10.5.1\\nSteps Involved in Using Amazon Bedrock\\nAmazon Bedrock offers a streamlined workflow for deploying and fine-tuning LLMs, making it an ideal\\nchoice for businesses looking to quickly integrate advanced AI capabilities into their operations. Here’s\\na high-level overview of how Bedrock operates:\\n• Model Selection: Users start by choosing from a curated selection of foundation models available\\nthrough Bedrock. These include models from AWS (like Amazon Titan) and third-party providers\\n(such as Anthropic Claude and Stability AI).\\n• Fine-Tuning:\\n– Once a model is selected, users can fine-tune it to better fit their specific needs. This involves\\nfeeding the model with domain-specific data or task-specific instructions to tailor its outputs.\\n7https://aws.amazon.com/bedrock/\\n84\\n– The fine-tuning process is handled via simple API calls, eliminating the need for extensive\\nsetup or detailed configuration. Users provide their custom data, and Bedrock manages the\\ntraining process in the background.\\n• Deployment:\\n– After fine-tuning, Bedrock takes care of deploying the model in a scalable and efficient manner.\\nThis means that users can quickly integrate the fine-tuned model into their applications or\\nservices.\\n– Bedrock ensures that the model scales according to demand and handles performance optimi-\\nsation, providing a seamless user experience.\\n• Integration and Monitoring:\\n– Bedrock integrates smoothly with other AWS services, allowing users to embed AI capabilities\\ndirectly into their existing AWS ecosystem.\\n– Users can monitor and manage the performance of their deployed models through AWS’s\\ncomprehensive monitoring tools, ensuring that the models continue to perform optimally.\\n10.5.2\\nLimitations of Using Amazon Bedrock\\nWhile Amazon Bedrock offers a robust suite of tools and services for addressing certain AI challenges,\\nit is not a comprehensive solution for all AI needs. One key limitation is that it does not eliminate the\\nrequirement for human expertise. Organisations still need skilled professionals who understand the in-\\ntricacies of AI technology to effectively develop, fine-tune, and optimise the models provided by Bedrock.\\nAdditionally, Amazon Bedrock is not designed to function as a standalone service. It relies on integration\\nwith other AWS services, such as Amazon S3 for data storage, AWS Lambda for serverless computing,\\nand AWS SageMaker for machine learning model development. Therefore, businesses leveraging Amazon\\nBedrock will also need to use these complementary AWS services to fully realise its potential. This\\ninterconnectedness means that while Amazon Bedrock enhances the AI capabilities within an AWS\\necosystem, it may present a steep learning curve and require significant infrastructure management for\\nthose new to AWS.\\n10.5.3\\nTutorials\\n1. Finetuning LLMs on Amazon Bedrock\\n2. Amazon Bedrock for Generative AI\\n10.6\\nOpenAI’s Fine-Tuning API\\nOpenAI’s Fine-Tuning API is a comprehensive platform that facilitates the customisation of OpenAI’s\\npre-trained LLMs to cater to specific tasks and domains. This service is designed to be user-friendly,\\nenabling a broad range of users, from businesses to individual developers, to harness the power of\\nadvanced AI without the complexities typically associated with model training and deployment.\\n10.6.1\\nSteps Involved in Using OpenAI’s Fine-Tuning API\\n• Model Selection:\\n– Choosing a Pre-Trained Model: Users begin by selecting a base model from OpenAI’s\\nextensive lineup. This includes powerful models like GPT-4, which offer a robust starting\\npoint for a wide range of language processing tasks.\\n– Customisable Base: These models come pre-trained with vast amounts of data, providing\\na solid foundation that can be further refined to suit specific requirements.\\n• Data Preparation and Upload:\\n85\\n– Curating Relevant Data: Users need to gather and prepare a dataset that reflects the\\nspecific task or domain they wish to fine-tune the model for. This data is crucial for teaching\\nthe model to perform the desired function more effectively.\\n– Uploading Data to the API: The Fine-Tuning API facilitates easy data upload. Users\\ncan feed their curated datasets into the API through straightforward commands, making the\\nprocess accessible even to those with limited technical backgrounds.\\n• Initiating Fine-Tuning:\\n– Automated Process: Once the data is uploaded, OpenAI’s infrastructure handles the fine-\\ntuning process. The API adjusts the model’s parameters based on the new data to improve\\nperformance on the specified tasks.\\n• Deploying the Fine-Tuned Model:\\n– API Integration: The fine-tuned model can be accessed and deployed via OpenAI’s API.\\nThis allows for seamless integration into various applications, such as chatbots, automated\\ncontent creation tools, or specialised customer service systems.\\n10.6.2\\nLimitations of OpenAI’s Fine-Tuning API\\n• Pricing Models: Fine-tuning and using OpenAI’s models through the API can be costly, espe-\\ncially for large-scale deployments or continuous usage. This can be a significant consideration for\\nsmaller organisations or budget-constrained projects.\\n• Data Privacy and Security: Users must upload their data to OpenAI’s servers for the fine-\\ntuning process. This raises potential concerns about data privacy and the security of sensitive or\\nproprietary information.\\n• Dependency on OpenAI Infrastructure: The reliance on OpenAI’s infrastructure for model\\nhosting and API access can lead to vendor lock-in, limiting flexibility and control over the deploy-\\nment environment.\\n• Limited Control Over Training Process: The fine-tuning process is largely automated and\\nmanaged by OpenAI, offering limited visibility and control over the specific adjustments made to\\nthe model.\\n10.6.3\\nTutorials\\n1. Fine-Tuning GPT-3 Using the OpenAI API\\n10.7\\nNVIDIA NeMo Customizer\\nNVIDIA NeMo Customiser8 is part of the NeMo framework, a suite of tools and models designed by\\nNVIDIA to facilitate the development and fine-tuning of LLM models. The Customiser focuses specifi-\\ncally on making it easier to fine-tune large language models (LLMs) for specialised tasks and domains.\\nLike other fine-tuning tools, NeMo Customiser is geared toward users who want to adapt pre-trained\\nmodels for specific applications, such as conversational AI, translation, or domain-specific text gener-\\nation. It delivers enterprise-ready models by offering accurate data curation, extensive customisation\\noptions, retrieval-augmented generation (RAG), and improved performance features. The platform sup-\\nports training and deploying generative AI models across diverse environments, including cloud, data\\ncenter, and edge locations. It provides a comprehensive package with support, security, and reliable APIs\\nas part of the NVIDIA AI Enterprise.\\n8https://developer.nvidia.com/blog/fine-tune-and-align-llms-easily-with-nvidia-nemo-customizer/\\n86\\n10.7.1\\nKey Features of NVIDIA NeMo\\nNVIDIA NeMo is designed to enhance AI projects with several standout features.[86]\\n• State-of-the-Art Training Techniques NeMo employs GPU-accelerated tools like NeMo Cu-\\nrator for preparing large-scale, high-quality datasets. These tools facilitate efficient pretraining of\\ngenerative AI models by leveraging thousands of compute cores, which significantly reduces training\\ntime and enhances the accuracy of large language models (LLMs).\\n• Advanced Customisation for LLMs The NeMo Customiser microservice allows for precise fine-\\ntuning and alignment of LLMs for specific domains. It uses model parallelism to speed up training\\nand supports scaling across multiple GPUs and nodes, enabling the fine-tuning of larger models.\\n• Optimised AI Inference with NVIDIA Triton NeMo includes NVIDIA Triton Inference Server\\nto streamline AI inference at scale. This integration accelerates generative AI inference, ensuring\\nconfident deployment of AI applications both on-premises and in the cloud.\\n• User-Friendly Tools for Generative AI NeMo features a modular, reusable architecture that\\nsimplifies the development of conversational AI models. It supports comprehensive workflows from\\ndata processing to deployment and includes pre-trained models for automatic speech recognition\\n(ASR), natural language processing (NLP), and text-to-speech (TTS), which can be fine-tuned or\\nused as-is.\\n• Best-in-Class Pretrained Models NeMo Collections offer a variety of pre-trained models and\\ntraining scripts, facilitating rapid application development or fine-tuning for specific tasks. Cur-\\nrently, NeMo supports models like Llama 2, Stable Diffusion, and NVIDIA’s Nemotron-3 8B family.\\n• Optimised Retrieval-Augmented Generation NeMo Retriever delivers high-performance, low-\\nlatency information retrieval, enhancing generative AI applications with enterprise-grade retrieval-\\naugmented generation (RAG) capabilities. This feature supports real-time business insights and\\ndata Utilisation.\\n10.7.2\\nComponents of NVIDIA NeMo\\n• NeMo Core Provides essential elements like the Neural Module Factory for training and inference,\\nstreamlining the development of conversational AI models.\\n• NeMo Collections Offers specialised modules and models for ASR, NLP, and TTS, including\\npre-trained models and training scripts, making the platform versatile.\\n• Neural Modules Serve as the building blocks of NeMo, defining trainable components such as\\nencoders and decoders, which can be connected to create comprehensive models.\\n• Application Scripts Simplify the deployment of conversational AI models with ready-to-use\\nscripts, enabling quick training or fine-tuning on specific datasets for various AI applications.\\n10.7.3\\nCustomising Large Language Models (LLMs)\\nWhile general-purpose LLMs, enhanced with prompt engineering or light fine-tuning, have enabled organ-\\nisations to achieve successful proof-of-concept projects, transitioning to production presents additional\\nchallenges.\\nFigure 10.3 illustrates NVIDIA’s detailed LLM customisation lifecycle, offering valuable\\nguidance for organisations that are preparing to deploy customised models in a production environment\\n[87].\\n1. Model Selection or Development\\nNVIDIA provides a range of pre-trained models, from 8B to 43B parameters, and supports the\\nintegration of other open-source models of any size. Alternatively, users can develop their own\\nmodels, starting with data curation, which includes selecting, labeling, cleansing, validating, and\\nintegrating data. This process, better termed data engineering, involves additional analysis, de-\\nsigning storage, evaluating model training results, and incorporating reinforcement learning with\\nhuman feedback (RLHF). While building a custom foundation model is often costly, complex, and\\ntime-consuming, most enterprises opt to start with a pre-trained model and focus on customisation.\\n87\\nFigure 10.3: Nvidia NeMo Framework for Customising and Deploying LLMs. The Nvidia NeMo frame-\\nwork is designed for end-to-end customisation and deployment of large language models (LLMs). This\\ndiagram illustrates the process from data curation and distributed training of foundation models, through\\nmodel customisation, to accelerated inference with guardrails. The platform enables AI developers to\\nintegrate in-domain, secure, and cited responses into enterprise applications, ensuring that LLMs are\\neffectively tailored for specific tasks and industries. The NeMo framework, supported by Nvidia AI En-\\nterprise, also offers robust support for various pre-trained foundation models like OpenAI’s GPT family,\\nensuring scalability and reliability in AI deployments. (adapted from [87])\\n2. Model Customisation\\nModel customisation involves optimising performance with task-specific datasets and adjusting\\nmodel weights. NeMo offers recipes for customisation, and enterprises can choose models already\\ntailored to specific tasks and then fine-tune them with proprietary data.\\n3. Inference\\nInference refers to running models based on user queries. This phase involves considering hardware,\\narchitecture, and performance factors that significantly impact usability and cost in production.\\n4. Guardrails\\nNVIDIA employs guardrails as intermediary services between models and applications.\\nThese\\nservices review incoming prompts for policy compliance, execute arbitration or orchestration steps,\\nand ensure model responses adhere to policies. Guardrails help maintain relevance, accuracy, safety,\\nprivacy, and security.\\n5. Applications\\nNVIDIA’s framework presents enterprise applications as LLM-ready, though this is not always\\nthe case.\\nExisting applications may be connected to LLMs to enable new features.\\nHowever,\\ncreating assistants for knowledge access or task execution often involves designing new applications\\nspecifically for natural language interfaces.\\n10.7.4\\nTutorials\\n1. Introduction to NVIDIA NeMo — Tutorial and Example\\n2. How to fine-tune a Riva NMT Bilingual model with Nvidia NeMo\\n88\\nChapter 11\\nMultimodal LLMs and their\\nFine-tuning\\nA multimodal model is a machine learning model that can process information from various modalities,\\nsuch as images, videos, and text. For instance, Google’s multimodal model, Gemini[88], can analyse a\\nphoto of a plate of cookies and produce a written recipe in response, and it can perform the reverse as well.\\nThe difference between Generative AI and Multimodal AI is that generative AI refers to the use of\\nmachine learning models to create new content, such as text, images, music, audio, and videos, typically\\nfrom a single type of input. Multimodal AI extends these generative capabilities by processing informa-\\ntion from multiple modalities, including images, videos, and text. This enables the AI to understand\\nand interpret different sensory modes, allowing users to input various types of data and receive a diverse\\nrange of content types in return.\\nFigure 11.1: Timeline of Multimodal Model Developments — This figure illustrates the progression\\nof significant multimodal models, highlighting key releases from major tech companies and research\\ninstitutions from December 2023 to March 2024. The timeline showcases models like Google’s TinyGPT-\\nV and Gemini Nano, along with other innovations such as MoE-LLAVA, DeepSeek-VL, and LLAVA-\\nGemma, indicating the rapid advancement in multimodal AI technologies (adapted from [89]).\\n89\\n11.1\\nVision Language Model (VLMs)\\nVision language models encompass multimodal models capable of learning from both images and text\\ninputs. They belong to the category of generative models that utilise image and text data to produce\\ntextual outputs. These models, especially at larger scales, demonstrate strong zero-shot capabilities,\\nexhibit robust generalisation across various tasks, and effectively handle diverse types of visual data such\\nas documents and web pages. Typical applications include conversational interactions involving images,\\nimage interpretation based on textual instructions, answering questions related to visual content, under-\\nstanding documents, generating captions for images, and more. Certain advanced vision language models\\ncan also understand spatial attributes within images. They can generate bounding boxes or segmentation\\nmasks upon request to identify or isolate specific subjects, localise entities within images, or respond to\\nqueries regarding their relative or absolute positions. The landscape of large vision language models is\\ncharacterised by considerable diversity in training data, image encoding techniques, and consequently,\\ntheir functional capabilities.\\n11.1.1\\nArchitecture\\nVision-language models adeptly integrate both visual and textual information, leveraging three funda-\\nmental components:\\n• Image Encoder: This component translates visual data (images) into a format that the model\\ncan process.\\n• Text Encoder: Similar to the image encoder, this component converts textual data (words and\\nsentences) into a format the model can understand.\\n• Fusion Strategy: This component combines the information from both the image and text en-\\ncoders, merging the two data types into a unified representation.\\nThese elements work collaboratively, with the model’s learning process (loss functions) specifically tai-\\nlored to the architecture and learning strategy employed. Although the concept of vision-language mod-\\nels is not new, their construction has evolved significantly. Early models used manually crafted image\\ndescriptions and pre-trained word vectors. Modern models, however, utilise transformers—an advanced\\nneural network architecture—for both image and text encoding. These encoders can learn features either\\nindependently or jointly.\\nA crucial aspect of these models is pre-training. Before being applied to specific tasks, the models are\\ntrained on extensive datasets using carefully selected objectives. This pre-training equips them with the\\nfoundational knowledge required to excel in various downstream applications. Following is one of the\\nexample architectures of VLMs.\\n11.1.2\\nContrastive Learning\\nContrastive learning is a technique that focuses on understanding the differences between data points. It\\ncomputes a similarity score between instances and aims to minimise contrastive loss, making it particu-\\nlarly useful in semi-supervised learning where a limited number of labelled samples guide the optimisation\\nprocess to classify unseen data points.\\nHow it works\\nFor instance, to recognise a cat, contrastive learning compares a cat image with a similar cat image and\\na dog image. The model learns to distinguish between a cat and a dog by identifying features such as\\nfacial structure, body size, and fur. By determining which image is closer to the ”anchor” image, the\\nmodel predicts its class.\\nCLIP is a model that utilises contrastive learning to compute similarity between text and image embed-\\ndings through textual and visual encoders. It follows a three-step process for zero-shot predictions:\\n• Pre-training: Trains a text and image encoder to learn image-text pairs.\\n• Caption Conversion: Converts training dataset classes into captions.\\n• Zero-Shot Prediction: Estimates the best caption for a given input image based on learned\\nsimilarities.\\n90\\nFigure 11.2: Workflow of Contrastive Pre-Training for Multimodal Models. This figure illustrates the\\nprocess of contrastive pre-training where text and image encoders are trained to align representations\\nfrom both modalities. Step 1 involves contrastive pre-training by pairing text and image data, while\\nStep 2 showcases the creation of a dataset classifier using label text encoded by the text encoder. Step\\n3 demonstrates the model’s application for zero-shot prediction by leveraging the pre-trained text and\\nimage encoders. This method enables the model to generalise across various tasks without requiring\\ntask-specific fine-tuning (adopted from [90]).\\n11.2\\nFine-tuning of multimodal models\\nFor fine-tuning a Multimodal Large Language Model (MLLM), PEFT techniques such as LoRA and\\nQLoRA can be utilised. The process of fine-tuning for multimodal applications is analogous to that for\\nlarge language models, with the primary difference being the nature of the input data. In addition to\\nLoRA, which employs matrix factorisation techniques to reduce the number of parameters, other tools\\nsuch as LLM-Adapters and (IA)³[91] can be effectively used. LLM-Adapters integrate various adapter\\nmodules into the pre-trained model’s architecture, enabling parameter-efficient fine-tuning for diverse\\ntasks by updating only the adapter parameters while keeping the base model parameters fixed. (IA)³,\\nor Infused Adapters by Inhibiting and Amplifying Inner Activations, enhances performance by learn-\\ning vectors to weight model parameters through activation multiplications, supporting robust few-shot\\nperformance and task mixing without manual adjustments. Moreover, dynamic adaptation techniques\\nlike DyLoRA[92] allow for the training of low-rank adaptation blocks across different ranks, optimising\\nthe learning process by sorting the representations during training. LoRA-FA[93], a variant of LoRA,\\noptimises the fine-tuning process by freezing the first low-rank matrix after initialisation and using it as a\\nrandom projection while training the other, thereby reducing the number of parameters by half without\\ncompromising performance.\\nThe Efficient Attention Skipping (EAS)[94] module introduces a novel parameter and computation-\\nefficient tuning method for MLLMs, aiming to maintain high performance while reducing parameter and\\ncomputation costs for downstream tasks. However, MemVP[95] critiques this approach, noting that it\\nstill increases the input length of language models. To address this, MemVP integrates visual prompts\\nwith the weights of Feed Forward Networks, thereby injecting visual knowledge to decrease training time\\nand inference latency, ultimately outperforming previous PEFT methods.\\n11.2.1\\nFull-parameter Fine-Tuning\\nMethods such as those introduced by LOMO[96] and MeZO[97] provide alternative solutions by focusing\\non memory efficiency.\\nLOMO utilises a low-memory optimisation technique derived from Stochastic\\nGradient Descent (SGD), reducing memory consumption typically associated with the ADAM optimiser.\\nMeZO, on the other hand, offers a memory-efficient optimiser that requires only two forward passes\\nto compute gradients, enabling comprehensive fine-tuning of large models with a memory footprint\\nequivalent to inference [89].\\n91\\n11.2.2\\nCase study of fine-tuning MLLMs for Medical domain\\nThe following section provides a case study on fine-tuning MLLMs for the Visual Question Answering\\n(VQA) task. In this example, we present a PEFT for fine-tuning MLLM specifically designed for Med-\\nVQA applications. To ensure accurate performance measurement, human evaluations were conducted,\\ndemonstrating that the model achieves an overall accuracy of 81.9% and surpasses the GPT-4v model\\nby a substantial margin of 26% in absolute accuracy on closed-ended questions.\\nThe model consists of three components: the vision encoder, a pre-trained Large Language Model (LLM)\\nfor handling multimodal inputs and generating responses, and a single linear layer for projecting embed-\\ndings from the visual encoding space to the LLM space, as shown in figure 11.3.\\nThe Vision Transformer (ViT) type backbone, EVA, encodes image tokens into visual embeddings,\\nwith model weights remaining frozen during the fine-tuning process. The technique from MiniGPT-v2\\nis utilised, grouping four consecutive tokens into one visual embedding to efficiently reduce resource\\nconsumption by concatenating on the embedding dimension.\\nThese grouped visual tokens are then processed through the projection layer, resulting in embeddings\\n(length 4096) in the LLM space. A multimodal prompt template integrates both visual and question\\ninformation, which is input into the pre-trained LLM, LLaMA2-chat(7B), for answer generation. The\\nlow-rank adaptation (LoRA) technique is applied for efficient fine-tuning, keeping the rest of the LLM\\nfrozen during downstream fine-tuning. A beam search with a width of 1 is utilised.\\nFigure 11.3: Overview of Med VQA architecture integrating LoRA and a pre-trained LLM with a Vision\\nEncoder for medical visual question answering tasks. The architecture includes stages for processing\\nimages and generating contextually relevant responses, demonstrating the integration of vision and lan-\\nguage models in a medical setting (adopted from [98]).\\nThe multimodal prompt includes input images, questions, and a specific token for VQA tasks, following\\nthe MiniGPT-v2 template. In Figure 11.3, the image features derived from linear projection are labelled\\nas ImageFeature, with the corresponding questions serving as text instructions. The special token [VQA]\\nis used as the task identifier, forming the complete multimodal instructional template:\\n92\\n[INST]<img><ImageFeature></img>[VQA] Instruction [/INST].\\nModel Training\\nWeights from MiniGPT-v2, pre-trained on general domain datasets, are further fine-tuned using multi-\\nmodal medical datasets in two stages. The LoRA technique is employed for efficient fine-tuning, updating\\nonly a small portion of the entire model, as detailed below:\\n• Fine-tuning with image captioning: During this stage, the model is fine-tuned using the ROCO\\nmedical image-caption dataset, which contains medical image-caption pairs of varying lengths. The\\nprompt template used is <Img><ImageHere></Img>[caption] <instruction>, with the instruc-\\ntion prompt randomly selected from a pool of four candidates, such as “Briefly describe this image.”\\nDuring training, only the linear projection layer and the LoRA layer in the LLM are fine-tuned,\\nwhile other parts of the model remain frozen.\\n• Fine-tuning on VQA: In the second stage, the model is fine-tuned on the Med-VQA dataset,\\nVQA-RAD, which contains triplets of images, questions, and answers. Following the instruction\\ntemplate proposed in MiniGPT-v2, the template used is: “[INST] <img><ImageFeature></img>[VQA]\\nInstruction [/INST]”, where the instruction prompt is: “Based on the image, respond to this\\nquestion with a short answer: question,” with question signifying the question corresponding to\\nthe given medical image. The motivation for generating short answers is to validate against the\\nexisting labelled data in VQA-RAD, where the answers are typically short in both open-ended and\\nclosed-ended QA pairs. Similar to the first stage, the vision encoder and the LLM remain frozen\\nwhile only the linear projection and LoRA layers in the LLM are updated.\\n11.3\\nApplications of Multimodal models\\n1. Gesture Recognition - These models interpret and recognise human gestures, which is crucial\\nfor sign language translation. Multimodal models facilitate inclusive communication by processing\\ngestures and converting them into text or speech.\\n2. Video Summarisation - Multimodal models can summarise lengthy videos by extracting key vi-\\nsual and audio elements. This capability streamlines content consumption, enables efficient content\\nbrowsing, and enhances video content management platforms.\\n3. DALL-E is a notable example of multimodal AI that generates images from textual descriptions.\\nThis technology expands creative possibilities in content creation and visual storytelling, with\\napplications in art, design, advertising, and more.\\n4. Educational Tools - Multimodal models enhance learning experiences by providing interactive\\neducational content that responds to both visual and verbal cues from students. They are integral\\nto adaptive learning platforms that adjust content and difficulty based on student performance and\\nfeedback.\\n5. Virtual Assistants - Multimodal models power virtual assistants by understanding and respond-\\ning to voice commands while processing visual data for comprehensive user interaction. They are\\nessential for smart home automation, voice-controlled devices, and digital personal assistants.\\n11.4\\nAudio or Speech LLMs Or Large Audio Models\\nAudio or speech LLMs are models designed to understand and generate human language based on audio\\ninputs. They have applications in speech recognition, text-to-speech conversion, and natural language\\nunderstanding tasks. These models are typically pre-trained on large datasets to learn generic language\\npatterns, which are then fine-tuned on specific tasks or domains to enhance performance.\\nAudio and Speech Large Language Models (LLMs) represent a significant advancement in the integration\\nof language processing with audio signals. These models leverage a robust Large Language Model as a\\nfoundational backbone, which is enhanced to handle multimodal data through the inclusion of custom\\naudio tokens. This transformation allows the models to learn and operate within a shared multimodal\\nspace, where both text and audio signals can be effectively processed.\\n93\\nUnlike text, which is inherently discrete, audio signals are continuous and need to be discretized into\\nmanageable audio tokens. Techniques like HuBERT[99] and wav2vec[100] are employed for this purpose,\\nconverting audio into a tokenized format that the LLM can process alongside text. The model, typically\\nautoregressive and decoder-based, is pre-trained using a combination of self-supervised tasks, such as\\npredicting masked tokens in interleaved text and audio, and supervised fine-tuning for specific tasks like\\ntranscription or sentiment analysis. This capability to handle and generate audio and text simultane-\\nously allows for a wide range of applications, from audio question answering to speech-based sentiment\\ndetection, making Audio and Speech LLMs a versatile tool in multimodal AI. The figure 11.4 illustrates\\nan example of a multimodal Audio LM architecture. In this setup, a prompt provides instructions in\\nboth text and audio formats. The audio is tokenized using an audio tokenizer. The multimodal model\\nthen combines these text and audio tokens and generates spoken speech through a vocoder (also known\\nas a voice decoder).\\nFigure 11.4: Multimodal Audio-Text Language Model architecture that integrates text and audio in-\\nputs for advanced multimodal processing.\\nThe architecture utilises text tokenizers and audio en-\\ncoders/tokenizers to convert inputs into tokens, which are then processed by the audio-text LM. This\\nmodel supports both discrete and continuous speech processing and enables tasks such as sentiment anal-\\nysis and response generation in natural language. The audio tokens are further refined using a vocoder,\\nwhile text tokens are detokenized to produce coherent text outputs (adapted from [101]).\\n94\\nAudio and speech LLMs like AudioPaLM[102], AudioLM[103], and various adaptations of models like\\nWhisper and LLaMA, integrate capabilities for understanding and generating audio data, including\\nspeech-to-text (STT), text-to-speech (TTS), and speech-to-speech (STS) translation.\\nThese models\\nhave shown that LLMs, initially designed for text, can be effectively adapted for audio tasks through\\nsophisticated tokenization and fine-tuning techniques.\\n11.4.1\\nTokenization and Preprocessing\\nA key aspect of adapting LLMs for audio is the tokenization of audio data into discrete representations\\nthat the model can process. For instance, AudioLM and AudioPaLM utilise a combination of acoustic\\nand semantic tokens. Acoustic tokens capture the high-quality audio synthesis aspect, while semantic\\ntokens help maintain long-term structural coherence in the generated audio. This dual-token approach\\nallows the models to handle both the intricacies of audio waveforms and the semantic content of speech.\\n11.4.2\\nFine-Tuning Techniques\\nFine-tuning audio and speech LLMs typically involve several key strategies:\\n• Full Parameter Fine-Tuning: This involves updating all the model’s parameters during fine-\\ntuning. For instance, LauraGPT and SpeechGPT fine-tune all parameters to adapt pre-trained\\ntext LLMs to various audio tasks, although this can be computationally expensive.\\n• Layer-Specific Fine-Tuning: Techniques like LoRA (Low-Rank Adaptation) update only spe-\\ncific layers or modules of the model. This method significantly reduces computational requirements\\nwhile still allowing effective adaptation. Models like Qwen-Audio leverage LoRA to fine-tune pre-\\ntrained components for enhanced performance on speech recognition tasks.\\n• Component-Based Fine-Tuning: Recent models, such as those integrating the Whisper en-\\ncoder, freeze certain parts of the model (like the speech encoder) and only fine-tune a linear\\nprojector or specific adapters to align the speech and text modalities. This approach simplifies the\\ntraining process and enhances efficiency[104].\\n• Multi-Stage Fine-Tuning: Models like AudioPaLM perform multi-stage fine-tuning, starting\\nwith a text-based pre-training phase, followed by fine-tuning on a mixture of tasks that include\\nboth text and audio data. This staged approach leverages the strengths of pre-trained text models\\nwhile adapting them for multimodal tasks.\\n11.4.3\\nFine-Tuning Whisper for Automatic Speech Recognition (ASR)\\nWhisper1 is an advanced Automatic Speech Recognition (ASR) model developed by OpenAI, designed\\nto convert spoken language into text. Built upon the powerful Transformer architecture, Whisper excels\\nat capturing and transcribing diverse speech patterns across various languages and accents.\\nUnlike\\ntraditional ASR models that require extensive labelled data, Whisper leverages a vast dataset and self-\\nsupervised learning, enabling it to perform robustly in noisy environments and handle a wide range of\\nspeech variations. Its versatility and high accuracy make it an ideal choice for applications such as voice\\nassistants, transcription services, and multilingual speech recognition systems.\\nWhy Fine-Tune Whisper?\\nFine-tuning Whisper for specific ASR tasks can significantly enhance its performance in specialised\\ndomains. Although Whisper is pre-trained on a large and diverse dataset, it might not fully capture\\nthe nuances of specific vocabularies or accents present in niche applications. Fine-tuning allows Whisper\\nto adapt to particular audio characteristics and terminologies, leading to more accurate and reliable\\ntranscriptions. This process is especially beneficial in industries with domain-specific jargon, like medical,\\nlegal, or technical fields, where the generic model might struggle with specialised vocabulary.\\n1https://openai.com/index/whisper/\\n95\\nSteps to Fine-Tune Whisper\\n• Data Collection and Preparation: Gather a sizable dataset that matches the target domain or\\ntask. Ensure the dataset includes diverse examples with clear transcriptions. Clean and preprocess\\nthe audio files and transcripts, ensuring they are in a consistent format and aligned correctly. Tools\\nlike FFmpeg2 can help standardise audio formats and sample rates.\\n• Data Augmentation: To improve robustness, augment the dataset with variations such as dif-\\nferent noise levels, accents, or speeds. Techniques like adding background noise, altering pitch, or\\nchanging the tempo can help the model generalise better to real-world conditions.\\n• Preprocessing: Convert the audio files into a format suitable for Whisper, typically into mel\\nspectrograms or another time-frequency representation. This transformation is crucial as Whisper\\nrelies on such representations to learn and transcribe speech effectively.\\n• Model Configuration: Initialise the Whisper model with pre-trained weights. Configure the\\nmodel to accommodate the target language or domain-specific adjustments. This includes setting\\nappropriate hyperparameters, like learning rate and batch size, tailored to the dataset’s size and\\ncomplexity.\\n• Training: Fine-tune the Whisper model on the prepared dataset using a framework like PyTorch\\nor TensorFlow. Ensure to monitor the model’s performance on a validation set to avoid overfitting.\\nTechniques like gradient clipping, learning rate scheduling, and early stopping can help maintain\\ntraining stability and efficiency.\\n• Evaluation and Testing: After training, evaluate the model’s performance on a separate test\\nset to assess its accuracy and generalisability. Metrics like Word Error Rate (WER) or Character\\nError Rate (CER) provide insights into how well the model transcribes audio compared to ground\\ntruth transcriptions.\\n11.4.4\\nCase Studies and Applications\\n1. Medical Transcription: Fine-tuning speech LLMs on medical data has led to significant im-\\nprovements in transcribing doctor-patient interactions. Models like Whisper have been fine-tuned\\non medical terminologies, resulting in more accurate and reliable transcriptions.\\n2. Legal Document Processing: Legal firms have employed fine-tuned audio LLMs to transcribe\\ncourt proceedings and legal discussions.\\nDomain-specific fine-tuning has enhanced the models’\\nability to recognise and accurately transcribe legal jargon.\\n3. Customer Service Automation: Companies are using fine-tuned speech models to automate\\ncustomer service interactions. These models are trained on customer support data to understand\\nand respond to queries more effectively, providing a more seamless user experience.\\n2https://ffmpeg.org/ffmpeg.html\\n96\\nChapter 12\\nOpen Challenges and Research\\nDirections\\n12.1\\nScalability Issues\\nThe fine-tuning of Large Language Models (LLMs) such as GPT-4, PaLM1 , and T52 has become a critical\\narea of research, presenting several significant challenges and opening up new avenues for exploration,\\nparticularly in scaling these processes efficiently. This discussion focuses on the two main aspects: the\\nchallenges in scaling fine-tuning processes and potential research directions for scalable solutions.\\n12.1.1\\nChallenges in Scaling Fine-Tuning Processes\\n1. Computational Resources: Large-scale models such as GPT-3 and PaLM require enormous\\ncomputational resources for fine-tuning. For instance, fine-tuning a 175-billion parameter model\\nlike GPT-3 necessitates high-performance GPUs or TPUs capable of handling vast amounts of data\\nand complex operations. The sheer volume of parameters translates to extensive computational\\ndemands. Even a relatively smaller model, such as BERT-large with 340 million parameters, can\\nbe computationally intensive to fine-tune.\\n2. Memory Requirements: The memory footprint for fine-tuning LLMs is staggering. Each pa-\\nrameter in the model requires storage, and during training, additional memory is needed to store\\nintermediate computations, gradients, and optimiser states. For example, loading a 7 billion pa-\\nrameter model (e.g., LLaMA 2) in FP32 (4 bytes per parameter) requires approximately 28 GB\\nof GPU memory, while fine-tuning demands around 112 GB of GPU memory[105]. This memory\\ndemand is beyond the capability of most consumer-grade hardware, making fine-tuning accessible\\nprimarily to well-funded organisations or research institutions.\\n3. Data Volume: LLMs typically require vast amounts of training data to achieve state-of-the-art\\nperformance during fine-tuning. This data needs to be loaded, preprocessed, and fed into the model\\nat high speeds to maintain efficient training. Managing large datasets can become a bottleneck,\\nespecially if the data is stored in a distributed fashion across multiple systems or if it needs to be\\nfetched from remote storage.\\n4. Throughput and Bottlenecks: High throughput is essential to keep GPUs or TPUs fully\\nutilised. However, data pipelines can become bottlenecks if not properly optimised. For exam-\\nple, shuffling large datasets or loading them into memory quickly enough to keep up with the\\ntraining process can be challenging. Techniques like data packing, where multiple small examples\\nare combined into larger batches, help improve throughput but add complexity to data handling\\nroutines.[106]\\n5. Efficient Use of Resources: The financial and environmental costs of fine-tuning large models\\nare significant. Large-scale fine-tuning involves not just the direct cost of computational resources\\nbut also the indirect costs associated with energy consumption and infrastructure maintenance.\\n1https://ai.google/discover/palm2/\\n2https://huggingface.co/docs/transformers/en/model_doc/t5\\n97\\nTechniques such as mixed-precision training and gradient checkpointing can reduce these costs by\\noptimising memory and computational efficiency.\\nThe challenges in scaling the fine-tuning processes of LLMs are multifaceted and complex, involving sig-\\nnificant computational, memory, and data handling constraints. Innovations in PEFT, data throughput\\noptimisation, and resource-efficient training methods are critical for overcoming these challenges. As\\nLLMs continue to grow in size and capability, addressing these challenges will be essential for making\\nadvanced AI accessible and practical for a wider range of applications.\\n12.1.2\\nResearch Directions for Scalable Solutions\\nAdvanced PEFT Techniques and Sparse Fine-Tuning\\nRecent advancements in PEFT techniques, like LoRA and its variant, Quantised LoRA, are revolu-\\ntionising the scalability of LLMs. LoRA reduces the computational burden by updating only a low-rank\\napproximation of the parameters, significantly lowering memory and processing requirements. Quantised\\nLoRA further optimises resource usage by applying quantisation to these low-rank matrices, maintaining\\nhigh model performance while minimising the need for extensive hardware. This has enabled efficient\\nfine-tuning of massive models, such as in Meta’s LLaMA project, where adapting a smaller set of influ-\\nential parameters allowed the models to perform robustly across various tasks with less computational\\nstrain.\\nSparse fine-tuning techniques, such as SpIEL [107] complement these efforts by selectively updating\\nonly the most impactful parameters. SpIEL fine-tunes models by only changing a small portion of the\\nparameters, which it tracks with an index. The process includes updating the parameters, removing the\\nleast important ones, and adding new ones based on their gradients or estimated momentum using an\\nefficient optimiser.\\nData Efficient Fine-Tuning (DEFT)\\nTo address the scalability challenges, recently the concept of DEFT has emerged. This novel approach\\nintroduces data pruning as a mechanism to optimise the fine-tuning process by focusing on the most\\ncritical data samples.\\nDEFT aims to enhance the efficiency and effectiveness of fine-tuning LLMs by selectively pruning the\\ntraining data to identify the most influential and representative samples. This method leverages few-shot\\nlearning principles, enabling LLMs to adapt to new data with minimal samples while maintaining or even\\nexceeding performance levels achieved with full datasets [108].\\nKey Components of DEFT\\nHigh Accuracy Through Influence Score: DEFT introduces the concept of an influence score to\\nevaluate and rank the importance of each data sample in the context of LLM fine-tuning. The influence\\nscore estimates how removing a specific sample would impact the overall performance of the model. This\\napproach allows for the selection of a small subset of data that is highly representative and influential,\\nthereby enabling the model to maintain high accuracy with significantly fewer samples.\\nHigh Efficiency Through Effort Score and Surrogate Models: To address the cost and complexity\\nof evaluating large datasets, DEFT employs a surrogate model—a smaller, computationally less intensive\\nmodel—to approximate the influence scores. This surrogate model helps estimate the impact of each\\nsample without the heavy computational burden associated with directly using the LLM. Additionally,\\nDEFT introduces an effort score to identify and prioritise more challenging samples that may require\\nspecial attention from the LLM. This dual-score system ensures that the fine-tuning process remains\\nboth efficient and effective.\\nPractical Implications and Use Cases\\n• Few-Shot Fine-Tuning for Rapid Adaptation: DEFT is particularly beneficial for applica-\\ntions where models need to quickly adapt to new data with minimal samples. In scenarios such as\\n98\\npersonalised recommendations or adapting to sudden changes in user behaviour, DEFT allows for\\nrapid fine-tuning, maintaining high performance with a fraction of the data typically required.\\n• Reducing Computational Costs in Large-Scale Deployments: By focusing on the most\\ninfluential data samples and using surrogate models, DEFT significantly reduces the computational\\nresources needed for fine-tuning. This makes it feasible to maintain high-performing LLMs even in\\nlarge-scale deployments where data volumes are substantial.\\nFuture Directions\\nThe DEFT introduces a data pruning task for fine-tuning large language models (LLMs), setting the\\nstage for new research into efficient LLM-based recommendation systems and presenting numerous op-\\nportunities for future exploration. Key areas for further investigation include:\\n• Applying the proposed DEALRec[109] approach to a broader range of LLM-based recommender\\nmodels across diverse cross-domain datasets, thereby enhancing fine-tuning performance within\\nresource constraints.\\n• Addressing the limited context window of LLMs by selectively focusing on the most informative\\nitems in user interaction sequences for fine-tuning purposes.\\n12.1.3\\nHardware and Algorithm Co-Design\\nCo-designing hardware and algorithms tailored for LLMs can lead to significant improvements in the\\nefficiency of fine-tuning processes. Custom hardware accelerators optimised for specific tasks or types of\\ncomputation can drastically reduce the energy and time required for model training and fine-tuning.\\n• Custom Accelerators: Developing hardware accelerators specifically for the sparse and low-\\nprecision computations often used in LLM fine-tuning can enhance performance. These accelerators\\nare designed to efficiently handle the unique requirements of LLMs, such as the high memory\\nbandwidth and extensive matrix multiplications involved in transformer architectures.\\n• Algorithmic Optimisation: Combining hardware innovations with algorithmic optimisation\\ntechniques, such as those that minimise data movement or leverage hardware-specific features\\n(e.g., tensor cores for mixed-precision calculations), can further enhance the efficiency of fine-tuning\\nprocesses.\\n• Example: NVIDIA’s TensorRT3 is an example of hardware and algorithm co-design in action.\\nIt optimises deep learning models for inference by leveraging NVIDIA GPUs’ capabilities, signifi-\\ncantly speeding up the process while reducing the resource requirements. TensorRT’s optimisations\\ninclude support for mixed-precision and sparse tensor operations, making it highly suitable for fine-\\ntuning large models.\\nAs the scale of language models continues to grow, addressing the challenges of fine-tuning them efficiently\\nbecomes increasingly critical. Innovations in PEFT, sparse fine-tuning, data handling, and the integration\\nof advanced hardware and algorithmic solutions present promising directions for future research. These\\nscalable solutions are essential not only to make the deployment of LLMs feasible for a broader range of\\napplications but also to push the boundaries of what these models can achieve.\\n12.2\\nEthical Considerations in Fine-Tuning LLMs\\n12.2.1\\nBias and Fairness\\nWhen fine-tuning LLMs, the goal is often to optimise their performance for specific tasks or datasets.\\nHowever, these datasets may inherently carry biases that get transferred to the model during the fine-\\ntuning process. Biases can arise from various sources, including historical data, imbalanced training\\nsamples, and cultural prejudices embedded in language. For instance, an LLM fine-tuned on a dataset\\nprimarily sourced from English-speaking countries might underperform or make biased predictions when\\n3https://docs.nvidia.com/tensorrt/index.html\\n99\\napplied to text from other linguistic or cultural backgrounds. Google AI’s Fairness Indicators tool4 is a\\npractical solution that allows developers to evaluate the fairness of their models by analysing performance\\nmetrics across different demographic groups. This tool can be integrated into the fine-tuning pipeline to\\nmonitor and address bias in real-time.\\nAddressing Bias and Fairness\\n• Diverse and Representative Data: Ensuring that fine-tuning datasets are diverse and repre-\\nsentative of all user demographics can help mitigate bias.\\n• Fairness Constraints: Incorporating fairness constraints, as suggested by the FairBERTa frame-\\nwork5, ensures that fine-tuned models maintain equitable performance across different groups.\\n• Example Application: In healthcare, an LLM fine-tuned to assist in diagnosing conditions might\\ninitially be trained on data from predominantly white patients. Such a model could produce less\\naccurate diagnoses for patients from other racial backgrounds. By using fairness-aware fine-tuning\\ntechniques, healthcare providers can develop models that perform more equitably across diverse\\npatient populations.\\n12.2.2\\nPrivacy Concerns\\nFine-tuning often involves using sensitive or proprietary datasets, which poses significant privacy risks. If\\nnot properly managed, fine-tuned models can inadvertently leak private information from their training\\ndata. This issue is especially critical in domains like healthcare or finance, where data confidentiality is\\nparamount.\\nEnsuring Privacy During Fine-Tuning\\n• Differential Privacy6: Implementing differential privacy techniques during fine-tuning can pre-\\nvent models from leaking sensitive information.\\n• Federated Learning7: Utilising federated learning frameworks allows models to be fine-tuned\\nacross decentralised data sources, which enhances privacy by keeping data localised.\\n• Example Application: In customer service applications, companies might fine-tune LLMs using\\ncustomer interaction data. Employing differential privacy ensures that the model learns from these\\ninteractions without memorising and potentially leaking personal information, thus maintaining\\ncustomer confidentiality.\\n12.2.3\\nSecurity Risks\\n• Security Vulnerabilities in Fine-Tuned Models: Fine-tuned LLMs are susceptible to secu-\\nrity vulnerabilities, particularly from adversarial attacks. These attacks involve inputs designed to\\nexploit model weaknesses, causing them to produce erroneous or harmful outputs. Such vulnera-\\nbilities can be more pronounced in fine-tuned models due to their specialised training data, which\\nmay not cover all possible input scenarios.\\n• Recent Research and Industry Practices: Microsoft’s Adversarial ML Threat Matrix pro-\\nvides a comprehensive framework for identifying and mitigating adversarial threats during model\\ndevelopment and fine-tuning. This matrix helps developers understand the potential attack vectors\\nand implement defensive strategies accordingly.\\n• Enhancing Security in Fine-Tuning:\\n– Adversarial Training: Exposing models to adversarial examples during fine-tuning can\\nenhance their robustness against attacks.\\n– Security Audits: Regularly conducting security audits on fine-tuned models can help iden-\\ntify and address potential vulnerabilities.\\n4https://research.google/blog/fairness-indicators-scalable-infrastructure-for-fair-ml-systems/\\n5https://huggingface.co/facebook/FairBERTa\\n6https://privacytools.seas.harvard.edu/differential-privacy\\n7https://research.ibm.com/blog/what-is-federated-learning\\n100\\n12.3\\nAccountability and Transparency\\n12.3.1\\nThe Need for Accountability and Transparency\\nFine-tuning can significantly alter an LLM’s behaviour, making it crucial to document and understand\\nthe changes and their impacts.\\nThis transparency is essential for stakeholders to trust the model’s\\noutputs and for developers to be accountable for its performance and ethical implications.\\n12.3.2\\nRecent Research and Industry Practices\\nMeta’s Responsible AI framework8 underscores the importance of documenting the fine-tuning process\\nand its effects on model behaviour. This includes maintaining detailed records of the data used, the\\nchanges made during fine-tuning, and the evaluation metrics applied.\\n12.3.3\\nPromoting Accountability and Transparency\\n• Comprehensive Documentation: Creating detailed documentation of the fine-tuning process\\nand its impact on model performance and behaviour.\\n• Transparent Reporting: Utilising frameworks like Model Cards9 to report on the ethical and\\noperational characteristics of fine-tuned models.\\n• Example Application: In content moderation systems, LLMs fine-tuned to identify and filter\\nharmful content need clear documentation and reporting. This ensures that platform users and\\nregulators understand how the model operates and can trust its moderation decisions.\\n12.3.4\\nProposed frameworks/techniques for Ethical Fine-Tuning\\nFrameworks for Mitigating Bias\\nBias-aware fine-tuning frameworks aim to incorporate fairness into the model training process. Fair-\\nBERTa, introduced by Facebook, is an example of such a framework that integrates fairness constraints\\ndirectly into the model’s objective function during fine-tuning. This approach ensures that the model’s\\nperformance is balanced across different demographic groups.\\nOrganisations can adopt fairness-aware frameworks to develop more equitable AI systems. For instance,\\nsocial media platforms can use these frameworks to fine-tune models that detect and mitigate hate speech\\nwhile ensuring fair treatment across various user demographics.\\nTechniques for Privacy Preservation\\nDifferential privacy and federated learning are key techniques for preserving privacy during fine-tuning.\\nTensorFlow Privacy10, developed by Google, provides built-in support for differential privacy, allowing\\ndevelopers to fine-tune models securely without compromising data confidentiality.\\nLLMs are highly effective but face challenges when applied in sensitive areas where data privacy is cru-\\ncial. To address this, researchers focus on enhancing Small Language Models (SLMs) tailored to specific\\ndomains. Existing methods often use LLMs to generate additional data or transfer knowledge to SLMs,\\nbut these approaches struggle due to differences between LLM-generated data and private client data. In\\nresponse, a new Federated Domain-specific Knowledge Transfer (FDKT)[110] framework is introduced.\\nFDKT leverages LLMs to create synthetic samples that mimic clients’ private data distribution using\\ndifferential privacy. This approach significantly boosts SLMs’ performance by approximately 5% while\\nmaintaining data privacy with a minimal privacy budget, outperforming traditional methods relying\\nsolely on local private data.\\nIn healthcare, federated fine-tuning can allow hospitals to collaboratively train models on patient data\\nwithout transferring sensitive information. This approach ensures data privacy while enabling the de-\\nvelopment of robust, generalisable AI systems.\\n8https://ai.meta.com/responsible-ai/\\n9https://huggingface.co/docs/hub/en/model-cards\\n10https://www.tensorflow.org/responsible_ai/privacy/guide\\n101\\nFrameworks for Enhancing Security\\nAdversarial training and robust security measures[111] are essential for protecting fine-tuned models\\nagainst attacks. The adversarial training approach involves training models with adversarial examples\\nto improve their resilience against malicious inputs. Microsoft Azure’s adversarial training tools provide\\npractical solutions for integrating these techniques into the fine-tuning process, helping developers create\\nmore secure and reliable models.\\nIn cybersecurity, fine-tuned LLMs used for threat detection can benefit from adversarial training to\\nenhance their ability to identify and respond to sophisticated attacks, thereby improving organisational\\nsecurity.\\nFrameworks for Ensuring Transparency\\nTransparency and accountability frameworks, such as Model Cards and AI FactSheets11, provide struc-\\ntured ways to document and report on the fine-tuning process and the resulting model behaviours. These\\nframeworks promote understanding and trust among stakeholders by clearly outlining the model’s capa-\\nbilities, limitations, and ethical considerations.\\nIn government applications, where AI systems might be used for decision-making or public services,\\nmaintaining transparent documentation through frameworks like AI FactSheets ensures that these sys-\\ntems are accountable and their decisions can be audited and trusted by the public.\\nFine-tuning LLMs introduces several ethical challenges, including bias, privacy risks, security vulnera-\\nbilities, and accountability concerns. Addressing these requires a multifaceted approach that integrates\\nfairness-aware frameworks, privacy-preserving techniques, robust security measures, and transparency\\nand accountability mechanisms.\\nBy leveraging recent advancements in these areas, researchers and\\npractitioners can develop and deploy LLMs that are not only powerful but also ethically sound and\\ntrustworthy.\\n12.4\\nIntegration with Emerging Technologies\\nIntegrating LLMs with emerging technologies such as IoT (Internet of Things) and edge computing\\npresents numerous opportunities and challenges, reflecting advancements and insights from recent re-\\nsearch and industry developments.\\n12.4.1\\nOpportunities\\n• Enhanced Decision-Making and Automation: LLMs have the capability to analyse and derive\\ninsights from vast amounts of unstructured data generated by IoT devices. This data can range\\nfrom sensor readings in manufacturing plants to environmental data in smart cities. By processing\\nthis data in real-time, LLMs can optimise decision-making processes and automate tasks that\\ntraditionally required human intervention. For example:\\n– Industrial Applications: Predictive maintenance can be enhanced by LLMs analysing sen-\\nsor data to predict equipment failures before they occur, thereby reducing downtime and\\nmaintenance costs.\\n– Smart Cities: LLMs can analyse traffic patterns and environmental data from IoT sensors\\nto optimise city infrastructure and improve urban planning decisions.\\n• Personalised User Experiences: Integration with edge computing allows LLMs to process\\ndata locally on devices rather than relying solely on cloud-based servers. This enables LLMs to\\ndeliver highly personalised services based on real-time data and user preferences, enhancing user\\nexperiences across various domains:\\n– Healthcare: LLMs can provide personalised healthcare recommendations by analysing data\\nfrom wearable devices and integrating it with medical records securely stored on edge devices.\\n11https://aifs360.res.ibm.com/\\n102\\n• Improved Natural Language Understanding: IoT data integration enriches LLMs’ ability to\\nunderstand context and respond more intelligently to natural language queries. This can signifi-\\ncantly improve user interactions with smart environments:\\n– Smart Homes: LLMs integrated with IoT devices can understand and respond to voice\\ncommands more accurately, adjusting smart home settings based on real-time sensor data\\n(e.g., adjusting lighting and temperature based on occupancy and environmental conditions).\\n12.4.2\\nChallenges\\n• Data Complexity and Integration: Integrating data from diverse IoT devices poses challenges\\nrelated to data quality, interoperability, and scalability.\\nLLMs need to effectively process and\\ninterpret this heterogeneous data to derive meaningful insights:\\n– Data Integration: Ensuring seamless integration of data streams from different IoT plat-\\nforms and devices without compromising data integrity or performance.\\n– Data Preprocessing: Cleaning and preprocessing IoT data to ensure consistency and reli-\\nability before feeding it into LLMs for analysis.\\n• Privacy and Security: Edge computing involves processing sensitive data locally on devices,\\nraising concerns about data privacy and security:\\n– Data Privacy: Implementing robust encryption techniques and access control mechanisms\\nto protect sensitive data processed by LLMs on edge devices.\\n– Secure Communication: Ensuring secure communication channels between IoT devices\\nand LLMs to prevent data breaches or unauthorised access.\\n• Real-Time Processing and Reliability: LLMs deployed in edge computing environments must\\noperate with low latency and high reliability to support real-time applications:\\n– Latency: Optimising algorithms and processing capabilities of LLMs to handle real-time\\ndata streams efficiently without delays.\\n– Reliability: Ensuring the accuracy and consistency of insights generated by LLMs in dynamic\\nand unpredictable IoT environments.\\n12.5\\nFuture Research Areas\\n• Federated Learning and Edge Computing: Exploring federated learning techniques where\\nLLMs can be trained collaboratively across edge devices without centralised data aggregation.\\nThis approach addresses privacy concerns and reduces communication overhead.\\n• Real-Time Decision Support Systems: Developing LLM-based systems capable of real-time\\ndecision-making by integrating with edge computing infrastructure. This includes optimising algo-\\nrithms for low-latency processing and ensuring reliability under dynamic environmental conditions.\\n• Ethical and Regulatory Implications: Investigating the ethical implications of integrating\\nLLMs with IoT and edge computing, particularly regarding data ownership, transparency, and\\nfairness. This area requires frameworks for ethical AI deployment and governance.\\n103\\nGlossary\\nLLM Large Language Model – A type of AI model, typically with billions of parameters, trained on vast\\namounts of text data to understand and generate human-like text. They are primarily designed\\nfor tasks in natural language processing (NLP).\\nNLP Natural Language Processing – A field of artificial intelligence that focuses on the interaction\\nbetween computers and humans through natural language, including tasks like language generation,\\ntranslation, and sentiment analysis.\\nLoRA Low-Rank Adaptation – A parameter-efficient fine-tuning technique that adjusts only small low-\\nrank matrices to adapt pre-trained models to specific tasks, thus preserving most of the original\\nmodel’s parameters.\\nDoRA Weight-Decomposed Low-Rank Adaptation – A technique that decomposes model weights into\\nmagnitude and direction components, facilitating fine-tuning while maintaining inference efficiency.\\nQLoRA Quantised Low-Rank Adaptation – A variation of LoRA, specifically designed for quantised\\nmodels, allowing for efficient fine-tuning in resource-constrained environments.\\nPPO Proximal Policy Optimisation – A reinforcement learning algorithm that adjusts policies by bal-\\nancing the exploration of new actions and exploitation of known rewards, designed for stability and\\nefficiency in training.\\nDPO Direct Preference Optimisation – A method that directly aligns language models with human\\npreferences through preference optimisation, bypassing reinforcement learning models like PPO.\\nMoE Mixture of Experts – A model architecture that employs multiple specialised subnetworks, called\\nexperts, which are selectively activated based on the input to improve model performance and\\nefficiency.\\nMoA Mixture of Agents – A multi-agent framework where several agents collaborate during training\\nand inference, leveraging the strengths of each agent to improve overall model performance.\\nPEFT Parameter-Efficient Fine-Tuning – A fine-tuning approach for large models that involves adjust-\\ning only a subset of model parameters, improving efficiency in scenarios with limited computational\\nresources. This includes techniques like LoRA, QLoRA, and adapters.\\nAdapters Small, trainable modules introduced into the layers of pre-trained language models, allowing\\nefficient task-specific fine-tuning without modifying the core parameters of the original model.\\nTechniques such as **AdapterFusion** and **AdapterSoup** fall under this category, facilitating\\nthe combination of multiple adapters for complex multitasking.\\nSoft Prompt Tuning (SPT) A fine-tuning technique where a set of trainable prompt tokens are added\\nto the input sequence to guide a pre-trained model towards task-specific performance without\\nmodifying internal model weights.\\nPrefix-Tuning A variation of soft prompt tuning where a fixed sequence of trainable vectors is prepended\\nto the input layer at every layer of the model, enhancing task-specific adaptation.\\nQuantisation The process of reducing the precision of model weights and activations, often from 32-bit\\nto lower-bit representations like 8-bit or 4-bit, to reduce memory usage and improve computational\\nefficiency.\\n104\\nQuantised LLMs Large Language Models that have undergone quantisation, a process that reduces\\nthe precision of model weights and activations, often from 32-bit to 8-bit or lower, to enhance\\nmemory and computational efficiency.\\nPruning A model optimisation technique that reduces the complexity of large language models by\\nremoving less significant parameters, enabling faster inference and lower memory usage.\\nHalf Fine-Tuning (HFT) A fine-tuning method where half of the model’s parameters are kept frozen\\nwhile the other half are updated, helping to maintain pre-trained knowledge while adapting the\\nmodel to new tasks.\\nStructured Masking A technique that masks entire layers, heads, or other structural components of\\na model to reduce complexity while fine-tuning for specific tasks.\\nUnstructured Masking A technique where certain parameters of the model are masked out randomly\\nor based on a pattern during fine-tuning, allowing for the identification of the most important\\nmodel weights.\\nGLUE General Language Understanding Evaluation – A benchmark used to evaluate the performance\\nof NLP models across a variety of language understanding tasks, such as sentiment analysis and\\nnatural language inference.\\nSuperGLUE Super General Language Understanding Evaluation – A more challenging extension of\\nGLUE, consisting of harder tasks designed to test the robustness and adaptability of NLP models.\\nTruthfulQA A benchmark designed to measure the truthfulness of a language model’s output, focusing\\non factual accuracy and resistance to hallucination.\\nIFEval Instruction Following Evaluation – A benchmark that assesses a model’s ability to follow explicit\\ninstructions across tasks, usually in the context of fine-tuning large models for adherence to specific\\ninstructions.\\nBBH Big Bench Hard – A subset of the Big Bench dataset, which consists of particularly difficult tasks\\naimed at evaluating the advanced reasoning abilities of large language models.\\nMATH A dataset created to evaluate a model’s ability to solve high-school level mathematical problems,\\npresented in formal formats like LaTeX.\\nGPQA General-Purpose Question Answering – A challenging dataset that features knowledge-based\\nquestions crafted by experts to assess deep reasoning and factual recall.\\nMuSR Multimodal Structured Reasoning – A dataset that involves complex problems requiring lan-\\nguage models to integrate reasoning across modalities, often combining text with other forms of\\ndata such as images or graphs.\\nMMLU Massive Multitask Language Understanding – A benchmark that evaluates a language model’s\\nability to perform various tasks across diverse domains, such as humanities, STEM, social sciences,\\nand others, typically requiring high-level reasoning.\\nMMLU-PRO A refined version of the MMLU dataset with a focus on more challenging, multi-choice\\nproblems, typically requiring the model to parse long-range context.\\nARC AI2 Reasoning Challenge – A benchmark for evaluating a language model’s reasoning capabilities\\nusing a dataset of multiple-choice science questions.\\nCOQA Conversational Question Answering – A benchmark that evaluates how well a language model\\ncan understand and engage in back-and-forth conversation, especially in a question-answer format.\\nDROP Discrete Reasoning Over Paragraphs – A benchmark that tests a model’s ability to perform\\ndiscrete reasoning over text, especially in scenarios requiring arithmetic, comparison, or logical\\nreasoning.\\nSQuAD Stanford Question Answering Dataset – A popular dataset for evaluating a model’s ability to\\nunderstand and answer questions based on passages of text.\\n105\\nTREC Text REtrieval Conference – A benchmark that evaluates models on various text retrieval tasks,\\noften focusing on information retrieval and document search.\\nWMT Workshop on Machine Translation – A dataset and benchmark for evaluating the performance\\nof machine translation systems across different language pairs.\\nXNLI Cross-lingual Natural Language Inference – A dataset designed to evaluate a model’s ability to\\nunderstand and infer meaning across multiple languages.\\nPiQA Physical Interaction Question Answering – A dataset that measures a model’s understanding of\\nphysical interactions and everyday tasks.\\nWinogrande A large-scale dataset aimed at evaluating a language model’s ability to handle common-\\nsense reasoning, typically through tasks that involve resolving ambiguous pronouns in sentences.\\nRLHF Reinforcement Learning from Human Feedback – A method where language models are fine-\\ntuned based on human-provided feedback, often used to guide models towards preferred behaviours\\nor outputs.\\nRAFT Retrieval-Augmented Fine-Tuning – A method combining retrieval techniques with fine-tuning\\nto enhance the performance of language models by allowing them to access external information\\nduring training or inference.\\n106'))]\n"
     ]
    }
   ],
   "source": [
    "print(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534b6670",
   "metadata": {},
   "source": [
    "### Saving the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "139c736c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GraphDocument' object has no attribute 'page_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      3\u001b[39m output_data = [\n\u001b[32m      4\u001b[39m     {\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpage_content\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mdoc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpage_content\u001b[49m,\n\u001b[32m      6\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: doc.metadata\n\u001b[32m      7\u001b[39m     }\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m graph\n\u001b[32m      9\u001b[39m ]\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mKG_fromtext.json\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     12\u001b[39m     json.dump(output_data, f, ensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m, indent=\u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tiago\\anaconda3\\envs\\Graph_implementation\\Lib\\site-packages\\pydantic\\main.py:991\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    988\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m991\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'GraphDocument' object has no attribute 'page_content'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "output_data = [\n",
    "    {\n",
    "        \"page_content\": doc.page_content,\n",
    "        \"metadata\": doc.metadata\n",
    "    }\n",
    "    for doc in graph\n",
    "]\n",
    "\n",
    "with open(\"KG_fromtext.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba694811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes:[]\n",
      "Relationships:[]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nodes:{graph[0].nodes}\")\n",
    "print(f\"Relationships:{graph[0].relationships}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec1ff4b",
   "metadata": {},
   "source": [
    "## Visualize the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa12734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "\n",
    "def visualize_graph(graph_documents):\n",
    "\n",
    "    # Create network\n",
    "    net = Network(height=\"1200px\", width=\"100%\", directed=True,\n",
    "                      notebook=False, bgcolor=\"#222222\", font_color=\"white\")\n",
    "    \n",
    "    nodes = graph_documents[0].nodes\n",
    "    relationships = graph_documents[0].relationships\n",
    "\n",
    "    # Build lookup for valid nodes\n",
    "    node_dict = {node.id: node for node in nodes}\n",
    "    \n",
    "    # Filter out invalid edges and collect valid node IDs\n",
    "    valid_edges = []\n",
    "    valid_node_ids = set()\n",
    "    for rel in relationships:\n",
    "        if rel.source.id in node_dict and rel.target.id in node_dict:\n",
    "            valid_edges.append(rel)\n",
    "            valid_node_ids.update([rel.source.id, rel.target.id])\n",
    "\n",
    "\n",
    "    # Track which nodes are part of any relationship\n",
    "    connected_node_ids = set()\n",
    "    for rel in relationships:\n",
    "        connected_node_ids.add(rel.source.id)\n",
    "        connected_node_ids.add(rel.target.id)\n",
    "\n",
    "    # Add valid nodes\n",
    "    for node_id in valid_node_ids:\n",
    "        node = node_dict[node_id]\n",
    "        try:\n",
    "            net.add_node(node.id, label=node.id, title=node.type, group=node.type)\n",
    "        except:\n",
    "            continue  # skip if error\n",
    "\n",
    "    # Add valid edges\n",
    "    for rel in valid_edges:\n",
    "        try:\n",
    "            net.add_edge(rel.source.id, rel.target.id, label=rel.type.lower())\n",
    "        except:\n",
    "            continue  # skip if error\n",
    "\n",
    "    # Configure physics\n",
    "    net.set_options(\"\"\"\n",
    "            {\n",
    "                \"physics\": {\n",
    "                    \"forceAtlas2Based\": {\n",
    "                        \"gravitationalConstant\": -100,\n",
    "                        \"centralGravity\": 0.01,\n",
    "                        \"springLength\": 200,\n",
    "                        \"springConstant\": 0.08\n",
    "                    },\n",
    "                    \"minVelocity\": 0.75,\n",
    "                    \"solver\": \"forceAtlas2Based\"\n",
    "                }\n",
    "            }\n",
    "            \"\"\")\n",
    "        \n",
    "    output_file = \"knowledge_graph.html\"\n",
    "    net.save_graph(output_file)\n",
    "    print(f\"Graph saved to {os.path.abspath(output_file)}\")\n",
    "\n",
    "    # Try to open in browser\n",
    "    try:\n",
    "        import webbrowser\n",
    "        webbrowser.open(f\"file://{os.path.abspath(output_file)}\")\n",
    "    except:\n",
    "        print(\"Could not open browser automatically\")\n",
    "        \n",
    "# Run the function\n",
    "visualize_graph(graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Graph_implementation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
