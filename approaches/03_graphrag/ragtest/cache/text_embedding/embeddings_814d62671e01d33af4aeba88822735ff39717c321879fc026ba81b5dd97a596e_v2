{"result": {"data": [{"embedding": [-0.007927711, 0.037817262, -0.12829225, -0.062044073, 0.034600183, 0.031815566, -0.008275017, 0.0031824948, -0.061207037, 0.007059559, -0.008985767, -0.015741061, 0.061647724, 0.057428, -0.003861782, -0.052586216, -0.040488604, -0.0014938294, 0.016713493, 0.07254882, 0.035318613, -0.03134781, 0.045575563, -0.061583955, 0.040791888, 0.002328951, -0.075955786, 0.042688753, -0.031065028, 0.016435621, -0.042761482, 0.049270693, -0.049996603, -0.001984565, -0.063062035, -0.05017005, 0.0261143, -0.0021110752, -0.058002282, 0.039836936, 0.029966803, 0.0035290648, 0.0019704867, -0.020811155, 0.01484623, -0.045284986, 0.05517788, -0.05886286, -0.002635719, -0.098863766, 0.008415092, -0.029238688, 0.04241547, -0.0015908666, 0.062005535, 0.047693934, -0.050230324, 0.0021808748, 0.056701075, -0.0166744, 0.050852094, 0.093285635, -0.08472415, 0.028406644, 0.013326673, -0.02629972, -0.045582443, 0.042472787, -0.01475684, -0.005492971, 0.006458948, 0.014995213, 0.014079691, 0.07654799, -0.06084778, -0.047831953, -0.025389247, 0.00537741, 0.045220207, 0.08623722, 0.05551266, -0.026909642, 0.04912457, -0.00024061136, 0.010978446, -0.04806604, -0.031493913, -0.007984708, -0.039351966, 0.10801716, 0.0042530647, 0.0011151123, -0.011245887, 0.008343058, -0.06464534, 0.038561366, -0.02706664, -0.034258455, -0.03434978, -0.0515195, -0.03804037, -0.028515158, 0.043461796, -0.016337596, 0.024905311, 0.03016956, 0.06622936, -0.046850923, -0.012521387, 0.020091757, -0.018459763, 0.04462261, -0.044421826, 0.021237815, -0.045315314, -0.011312056, 0.0559143, -0.014682811, -0.018623488, 0.058174632, 0.014034906, 0.014688935, -0.03344074, 0.012487352, -0.012117869, 0.04493724, -0.020341001, 0.010532863, 0.0038210691, -0.062323898, -0.0040166536, -0.07126952, -0.0043512816, 0.012571869, 0.04430999, 0.073191114, -0.047817003, -0.006021555, 0.02507741, 0.011905989, 0.024784492, -0.030220432, -0.027754951, 0.015340487, 0.041683093, -0.004670835, -0.038806602, 0.013597969, 0.015996732, -0.010773032, 0.011220042, 0.02748827, 0.0058167824, 0.014008856, 0.052247614, 0.033793375, -0.025424724, 0.0045465613, -0.0003146028, 0.00018950875, 0.07975054, -0.012625011, -0.085159436, 0.04406358, -0.018467898, 0.02886969, 0.009462498, 0.02763412, 0.04327484, -0.0037953975, -0.04104464, -0.04071563, 0.008254487, -0.0018160038, 0.045294736, -0.011808146, 0.05312443, -0.0064273905, 0.056661934, 0.0019858154, 0.00032917428, -0.048429262, 0.062391043, 0.03184896, -0.009951001, -0.034976684, 0.0065813567, -0.023569046, -0.0053895945, -0.003696419, -0.034854345, -0.022817183, -0.08491606, 0.016082905, -0.0670542, -0.06043693, 0.03871666, -0.0072887084, 0.064630784, -0.043282025, -0.053609975, -0.012070609, -0.09548426, 0.0018266308, -0.021863671, 0.104341626, 0.010184799, 0.023825368, -0.01863348, 0.016895654, 0.10308395, -0.009538493, 0.0013174533, 0.028302997, -0.05310884, -0.02672857, -0.055470023, -0.030865561, -0.02079987, 0.010892153, -7.998396e-05, 0.0038913644, -0.0176533, 0.024631808, -0.00016219157, 0.022119109, -0.014771727, 0.065198354, -0.03826477, -0.008373478, -0.02652533, -0.06792859, 0.03165081, -0.039759938, 0.0035861293, 0.022899307, 0.010603993, 0.077527896, -0.013604671, -0.0013814773, -0.02849383, 0.022576163, 0.019305883, -0.00479856, -0.0017335038, 0.009705239, 0.012035476, -0.0038491085, -0.018845856, 0.01683539, 0.019095097, -0.0019370379, 0.044349212, 0.03090343, 0.04103438, -0.014290362, 0.04138751, 0.014354807, 0.049329206, 0.01773481, 0.008079927, -0.039297562, 0.0171215, -0.009895894, -0.027292298, -0.023411937, -0.015180123, -0.048294425, 0.014373786, -0.006625811, 0.0007470194, 0.03244416, 0.023669338, 0.018537495, -0.014835384, -0.035397038, 0.014631886, 0.017467089, -0.011010803, -0.024540912, -0.00918745, -0.06291323, 0.00951814, 0.0059180767, 0.010978697, 0.055020478, -0.001488446, -0.014032466, 0.03608596, -0.0035949347, 0.044019144, -0.032723907, -0.019357769, -0.029583322, -0.019155683, 0.021813426, 0.025609571, 0.020169133, 0.03735082, -0.04731519, -0.027982129, 0.0032421078, 0.06158169, 0.03660497, -0.001108645, -0.04273405, 0.00033396253, -0.031691372, 0.0074076597, -0.0008669772, -0.06559551, 0.009331572, -0.02296693, -0.015953833, 0.003223247, 0.080591805, 0.0129124485, 0.0023037274, 0.025192853, 0.014024525, 0.007989236, -0.054305922, 0.034750126, -0.058083557, -0.016704373, 0.080524996, -0.0004430283, 0.0020035983, 0.019845594, -0.008791946, 0.063873574, 0.003296127, 0.037505485, -0.027847886, 0.01197637, 0.010068139, 0.0040063947, -0.00037450466, 0.015352084, -0.0004566483, 0.0727316, -0.038585734, 0.015000715, -0.040487945, -0.02014587, 0.030649258, -0.012936652, -0.004860936, 0.012392874, 0.01366107, -0.019091126, 0.009105402, 0.006445101, -0.014752364, -0.00947091, 0.0019385598, 0.0075477986, 0.017911088, -0.03726297, -0.01981772, 0.03401755, -0.007932285, 0.0024170072, 0.011573141, 0.013988701, 0.014554039, 0.07468279, -0.0027986122, -0.03236287, -0.0073982966, -0.015659029, -0.017954575, 0.0009930751, 0.036830075, 0.010233012, 0.02749143, -0.05698929, -0.022563308, 0.03790341, -0.04626744, 0.01797789, 0.001805575, 0.033077363, -0.008431303, 0.044230066, 0.0031762074, -0.004103965, -0.0007805628, -0.0070338026, -0.06072436, -0.018172363, -0.037210345, -0.046754584, -0.0071584256, 0.010242305, -0.029287279, 0.016664822, 0.069402054, -0.029072104, 0.05361246, -0.024394616, -0.05627456, 0.013674971, -0.015471753, -0.0071818386, -0.014030516, -0.023397263, -0.046474457, -0.03129153, -0.006879, 0.025486475, 0.09513826, 0.03109536, -0.03090789, 0.010277079, 0.0064313374, 0.03695707, 0.010515116, 0.0011465054, 0.01907723, 0.0232876, -0.033897858, 0.0056148837, 0.01252593, 0.032129258, 0.015205227, 0.031303044, 0.02292749, 0.037432846, 0.004924151, -0.01372128, 0.006030477, 0.066366084, 0.006599119, -0.018775525, 0.021990582, -0.030387456, 0.06668596, 0.0061746603, 0.01776727, 0.012680392, -0.053401448, -0.003104903, -0.08021166, 0.019499991, 0.07994298, 0.08453309, 0.001840709, -0.061857864, 0.046769, 0.018608745, 0.043068495, 0.0064584934, 0.0003961741, 0.06572992, -0.0042992784, 0.023007855, -0.039587077, -0.009616012, 0.029606396, -0.00019014094, -0.01665966, 0.0016590114, 0.016231384, 0.01846708, -0.029914323, 0.024178745, 0.004424319, 0.046020895, 0.009916082, -0.045968596, 0.0410625, 0.012448823, -0.05499761, -0.043424416, 0.032282505, -0.01047368, 0.010212923, 0.021964567, 0.040524136, -0.0039999196, -0.03256358, -0.012668243, -0.00712697, -0.01588168, 0.07588222, 0.03369552, -0.018298574, -0.012507683, -0.06848375, 0.013793898, -0.0108294785, 0.024371838, -0.024491038, -0.104212426, -0.046408202, -0.013002067, 0.0062001515, 0.03879825, 0.030582074, 0.023196077, 0.075785935, -0.040631205, 0.056133527, 0.044331472, 0.005610181, 0.029355686, -0.057290327, -0.066662304, 0.05618258, 0.023367863, -0.01020811, -0.029114956, 0.0719295, 0.019812077, -0.03505829, 0.047966313, 0.05610355, -0.0646067, 0.032161262, 0.022510432, -0.015850473, -0.0039321072, -0.022903988, -0.05589456, -0.0068834135, 0.042753518, -0.042752557, 0.00807481, -0.016327372, -0.021973956, 0.012527793, -0.025983272, -0.042439245, 0.027916105, 0.023472918, -0.051335312, 0.012995697, 0.04403928, 0.030868895, -0.0007027908, 0.021810394, -0.0485148, 0.0061749076, 0.042090017, -0.033257823, -0.0013602192, -0.003076731, 0.02121931, -0.07192227, 0.025753736, -0.07995328, 0.036011983, -0.05192871, 0.03407262, -0.0013642176, 0.0003640885, -0.04985211, 0.019352838, -0.05136432, -0.049758155, -0.04374684, 0.057969965, 0.040311456, -0.037319772, 0.0110491915, -0.020238286, -0.037753724, -0.0047026183, 0.06067057, 0.04208893, -0.057100568, 0.036446784, -0.054605883, 0.002126176, -0.046557598, 0.02307297, -0.008426115, -0.021625275, -0.030662816, 0.026668137, -0.042888235, 0.050730262, 0.0416687, -0.008908105, 0.012417037, -0.044789184, 0.010771433, -0.028481446, -0.021353016, 0.0041658347, -0.0014412451, 0.014745458, 0.006444468, 0.015911475, 0.011933133, -0.01737914, -0.027180351, -0.01145747, -0.055407204, -0.060611278, 0.046308503, 0.03972736, -0.040461324, 0.033800296, 0.07407888, -0.009756816, 0.049060345, -0.018201962, -0.047673658, 0.032744847, -0.0044148983, -0.053390186, 0.02089095, 0.035908096, -0.0074829976, 0.02410311, -0.043748267, 0.042593587, -0.029088212, -0.017101776, -0.046990715, 0.035112653, -0.040575914, 0.011126777, 0.065160334, -0.031819012, -0.022084916, 0.042469047, 0.08698846, -0.046780277, -0.012676946, -0.0874468, -0.018617515, -0.06649693, 0.012865044, -0.012247006, 0.010134598, 0.023755793, 0.044978138, 0.018389525, -0.039681308, 0.0036772839, 0.010000832, 0.016771954, -0.04529662, 0.027543494, 0.005159845, 0.03549933, -0.0075432556, 0.050536465, 0.03843652, 0.009658431, -0.0071123666, 0.0048419004, 0.0059129545, 0.03036165, -0.020483555, -0.023936803, -0.0073508904, 0.03291684, 0.016293928, 0.01290189, 0.030834252, 0.032680593, 0.012558709, 0.017572992, 0.01116478, -0.014182651, -0.04899486, 0.038003072, 0.0065334113, 0.052029718, -0.0039833225, 0.026379082, 0.02650169, 0.015469345, -0.011361923, 0.029153392, -0.05316318, -0.032555755, -0.02683147, 0.04186559, -0.05963775, 0.0015274925, -0.017036738, -0.019878348, 0.0020069587, -0.06738261, -0.036750447, -0.023268865, 0.024882032, 0.0013648629, 0.0036042086, -0.018215619, -0.013928782, -0.0050217924, -0.019776767, 0.0040387944, 0.03830041, 0.012970933, 0.051560186, -0.018555563, -0.0014700938, -0.005961055, -0.047403317, 0.041009136, -0.041777458, 0.0022724997, -0.032950375, -0.05134896, -0.011276092, -0.012192036, 0.030950258, 0.015972253, 0.031514503, -0.048885252, -0.05158058, -0.005087764, 0.060712717, 0.01517377, -0.017264642, -0.044363927, -0.010341475, 0.018262077, -0.0048680594, 0.019760327, -0.04481046, -0.0055103004, -0.011102569, -0.0026330252, -0.029190984, -0.037290253, 0.09242377, -0.046530057, 0.0128879575, -0.021774279, -0.0678312, -0.0148890605, -0.026276398, -0.0037791207, 0.038527697, -0.007915034, 0.022239631, -0.021337532, -0.018093873, -0.057714183, 0.063933544, -0.0010364788, -0.035858747, 0.022121504, -0.041953813, 0.0006235823, 0.001671578, 0.0010884729, 0.004492979, 0.007557533, 0.04875942, 0.08475545, 0.022806859, -0.0045182486, -0.050163515, 0.02990225, -0.06352243, -0.07967389, -0.021674082, -0.03569792, -0.033628542], "index": 0, "object": "embedding"}, {"embedding": [0.09701411, 0.043393172, -0.19682138, -0.041245617, 0.070461884, -0.04496299, 0.055738494, 0.03410348, -0.007194154, 0.008758667, -0.038345847, 0.0073899976, 0.08150286, 0.028325677, -0.008750081, 0.009424837, 0.0053285398, 0.029788535, -0.011093883, 0.010667142, 0.0063930983, 0.01607115, -0.0032491283, 0.018956784, 0.0011969478, -0.0025083125, -0.051260747, 0.007034185, -0.035123356, 0.01829704, 0.025380468, -0.020631336, -0.05810366, -0.021263627, -0.055912204, -0.018836021, 0.034098577, 0.012084037, 0.03846344, 0.042858407, -0.007757577, -0.014203492, 0.036571812, -0.010832341, 0.04789197, 0.030370327, 0.035500206, -0.04479677, 0.056341555, -0.064816244, 0.05138698, 0.0018673773, 0.011119997, 0.016468989, 0.078955695, 0.0023386448, 0.025460642, 0.018685611, 0.0136380475, -0.08216751, 0.05377317, 0.028866211, -0.06999221, 0.025853869, 0.016276859, 0.0014905503, -0.07624039, 0.052846152, 0.029571915, -0.04228722, 0.007516284, 0.026411394, -0.03286247, 0.0887882, -0.013094373, -0.05579072, -0.04760364, -0.06938502, -0.006879035, 0.012279462, 0.0059535126, -0.027798021, 0.059730746, 0.0013855222, 0.012819994, -0.008924332, -0.016806241, 0.01709082, -0.013126529, 0.13892628, -0.0295849, 0.013477461, 0.044115946, 0.009215037, -0.062614955, 0.0676571, -0.0058560446, 0.0017423321, -0.06963566, -0.047507923, -0.039576016, -0.028452262, -0.026836377, -0.03924599, 0.006001344, 0.02513084, 0.009912145, -0.00065556157, -0.014934645, 0.024857625, -0.039174065, 0.015318463, -0.006988484, 0.016519388, -0.036434174, -0.0056264703, 0.023640156, 0.012603299, 0.006895873, 0.036873493, -0.012237611, -0.030699257, 0.05045814, 0.03350459, 0.0037783324, 0.074137606, -0.06710616, -0.013739524, 0.025583498, -0.013496522, -0.0029920514, -0.013171919, 0.012996788, 0.0033625388, -0.0050300164, 0.07760285, -0.013038433, -0.040676318, 0.018595189, 0.021011174, 0.034107376, -0.059827518, 0.030147439, -0.04324637, 0.008450772, -0.04410164, -0.02776074, -0.016358877, -0.0054441234, 0.0030131857, 0.014646708, 0.03659853, -0.00244324, -0.001730714, 0.09629652, -0.041721806, -0.007070003, -0.0020822203, 0.03043437, 0.005957438, 0.05783056, 0.015750743, -0.016164547, 0.03594702, 0.001443084, -0.03342894, -0.00894993, 0.042712234, 0.02339936, 0.03770996, -0.08410395, -0.046939906, 0.015232013, -0.012467085, 0.05982927, -0.0037390396, 0.036272574, -0.032168187, -0.022122504, -0.004859199, 0.022187183, -0.008922189, 0.05265254, 0.043548606, -0.0028991187, 0.021265857, -0.029208226, 0.023051467, -0.022998966, -0.017976267, -0.0045136386, 0.015666386, -0.09745881, -0.00044221076, -0.045292128, -0.00826718, 0.01387435, -0.06379871, 0.01605787, -0.031892627, -0.031095115, -0.010276512, -0.04919577, 0.038462996, -0.03754066, 0.018664902, -0.008693888, 0.04641715, -0.011109627, 0.040309314, 0.03861574, 0.0027730803, 0.017692493, 0.016458128, -0.0063316845, 0.021100005, -0.0023932266, 0.0022939595, -0.013212834, -0.017694362, 0.029231193, -0.030358817, 0.054595772, -0.009974639, 0.011724014, -0.024090357, -0.005406285, 0.026807941, -0.010860975, 0.0634026, -0.025749266, -0.027121188, 0.024585368, 0.016414953, 0.005933391, 0.014439768, 0.0016830958, 0.036466014, -0.03291282, 0.05712286, 0.021070052, 0.031481266, -0.021874595, -0.0059822304, -0.055479303, 0.010590698, -0.0066355714, -0.04562304, -0.0029381167, 0.019558968, -0.043369513, 0.007561476, 0.050186265, 0.016010758, 0.042955637, 0.0092357155, -0.02839335, -0.005135352, 0.0573089, 0.023179725, 0.054058503, 0.008334015, 0.0026009611, -0.023405733, 0.006821316, -0.038035404, 0.001217449, -0.004001689, -0.024238914, 0.0057239537, -0.007582976, 0.0025190068, 0.04753626, 0.036847208, -0.0025114506, 0.024676077, -0.012288051, -0.039384913, -0.01005246, 0.017286582, -0.06532697, -0.062979914, -0.061687697, -0.039532933, -0.02945365, 0.008907576, 0.042792823, 0.010249621, -0.0030088043, 0.012905105, 0.01664323, 0.027698323, -0.0041158954, 0.015171075, 0.008544562, 0.028102068, 0.065671206, -0.013237096, -0.0088244835, -0.0052611968, 0.011513217, 0.018119661, 0.06963874, 0.041335728, 0.0038910003, -0.048882514, -0.043571636, -0.040011305, 0.045879126, -0.028916223, -0.08207416, -0.014413194, -0.03639632, -0.013465164, -0.01989483, 0.029417336, 0.072478585, 0.02723084, 0.039189294, -0.019544033, 0.01689127, -0.0069226385, -0.03565737, -0.042864762, 0.03016637, 0.029319681, -0.02937398, -0.0068080705, -0.04161502, -0.024489442, 0.06845494, -0.005055585, 0.023987912, -0.05537608, -0.031889405, 0.031208381, -0.010262146, 0.021945309, 0.021870757, 0.0027212107, 0.042105567, -0.0338458, 0.020062111, -0.04969505, -0.014155315, 0.020237995, -0.016759127, -0.047895513, 0.022860056, -0.0067455703, -0.05303971, -0.046086762, -0.028256096, -0.030705724, 0.039272454, 0.009140163, 0.07306315, -0.0020649398, 0.024612539, 0.012379997, 0.016389903, -0.03177308, -0.021104401, 0.014063723, -0.028564405, 0.009709451, 0.05489656, -0.031642724, -0.01178133, 0.0005218399, 0.018875355, -0.012131637, -0.021342058, 0.02483073, -0.05756196, 0.041333623, -0.05367275, 0.00043000656, 0.004602549, 0.010145625, 0.013965508, -0.009819062, 0.044789862, -0.02864056, 0.068152405, -0.012033769, 0.0248002, -0.051154763, 0.0046534687, 0.09229145, -0.002038558, -0.01700484, -0.016189238, -0.018201005, 0.03927471, -0.048947006, 0.0012943206, 0.039783932, -0.021770155, 0.036432598, -0.055030122, -0.06053214, 0.0048474856, -0.017929403, -0.00816723, -0.0208136, -0.05321456, 0.0039377883, -0.002619698, 0.009644305, 0.022195844, 0.02715002, -0.018081868, -0.063067734, 0.019651169, 0.015276375, 0.056202304, -0.0020723501, 0.008680225, -0.015546685, 0.034830153, 0.029608205, 0.007455057, -0.038750093, 0.02174997, 0.027187804, 0.01707671, 0.038091525, 0.043562457, -0.025251856, 0.023890523, 0.018722717, 0.049800005, -0.028599748, -0.002414378, 0.00046167336, -0.013085956, 0.0042790547, -0.0011230547, 0.018686835, 0.047314875, -0.012907023, -0.062811494, -0.030525725, 0.0104399845, 0.06717382, 0.04352943, -0.043480854, -0.0038125275, 0.064241484, -0.002611183, 0.017250428, 0.048230834, 0.023551837, 0.08133444, -0.025113538, 0.031028284, -0.023317186, 0.019708712, 0.018236026, 0.050258104, 0.029021833, -0.07846704, 0.011316867, -0.013701706, -0.016131856, 0.0020823362, -0.04854182, 0.03940412, 0.037957206, -0.035123836, -0.038336627, 0.028847096, -0.025261892, 0.007705431, 0.032052767, 0.057951745, -0.006528059, 0.0041651484, 0.020060863, -0.0067811213, -0.04103081, -0.01879549, -0.045769688, -0.04392941, 0.021809475, 0.02479157, -0.010516439, -0.014905074, -0.00074616453, 0.0552368, 0.008745091, 0.013638816, -0.027491322, -0.020814441, -0.056932453, -0.046260912, 0.06355016, 0.01205025, 0.0031651743, 0.034221835, 0.058981083, -0.0369683, -0.04666501, 0.07232144, -0.029433414, 0.05140887, -0.072580166, -0.08372476, 0.02799889, -0.015707947, 0.0003166211, -0.013381876, 0.0042963987, 0.014894503, -0.036666688, 0.051921625, -0.013664817, -0.03531803, -0.00462218, 0.031834263, 0.01629324, -0.05006336, -0.014626443, -0.061138224, 0.0034269027, 0.012831159, -0.02311551, 0.036491215, -0.018145619, 0.04272459, 0.019838616, -0.010022019, 0.024691986, 0.005430437, 0.04326968, -0.027427012, 0.0161757, 0.019184794, 0.039989796, -0.0027719832, 0.04607394, 0.0056825574, -0.016115045, -0.008163235, 0.0028894993, 0.019239858, 0.016103024, -0.017050311, -0.087324984, 0.028717753, -0.05353542, 0.009258266, -0.06295617, 0.022289002, -0.003934896, -0.041707758, -0.07260618, 0.016371671, -0.03972531, -0.015795317, -0.036143076, 0.051532026, 0.0151161505, 0.011233146, 0.002244487, 0.018466715, -0.002999049, 0.010427866, 0.0488602, -0.0006140653, -0.06371243, 0.021641724, -0.064406216, -0.0047618253, -0.028999628, 0.011208617, -0.006854959, -0.03418312, -0.10437937, -0.013076353, -0.018029377, 0.014845461, 0.021826165, -0.023780348, -0.025313195, -0.050212458, -0.011801036, 0.029037463, -0.015061541, -0.016577706, 0.023548704, -0.009965175, 0.002938915, -0.0017224953, 0.008938048, -0.022387343, -0.044373926, -0.09621811, -0.028975306, -0.01743705, 0.0025292793, 0.038268987, -0.05243915, -0.017643478, 0.09685432, 0.00926217, 0.08480864, -0.0067308876, 0.015079853, -0.022043841, -0.016555918, 0.033395205, 0.01872646, -0.0030311206, -0.080868244, 0.041235942, -0.035480853, -0.03790447, 0.004996049, -0.021496486, -0.04694848, 0.0892547, -0.03885064, -0.0040501473, 0.0041203373, -0.013709484, -0.040045336, 0.02402415, 0.020780358, -0.05842265, 0.0006198282, -0.049427763, -0.022000793, -0.030160548, 0.028192181, -0.0083004385, 0.03400152, -0.008789546, 0.02608103, 0.009852742, -0.01773262, -0.04239694, 0.0055842753, 0.015793612, -0.037742276, 0.038187925, 0.055784557, 0.0011419349, -0.032630734, 0.110187426, 0.08923052, 0.030577568, 0.005294188, -0.06319952, 0.027745748, 0.025817962, -0.022800377, -0.007247243, -0.012836637, 0.008656442, -0.007271509, -0.03185556, 0.024149638, -0.008501388, -0.040080983, 0.01658397, -0.032503895, -0.014773192, -0.026867002, 0.048412737, 0.009362946, -0.00055777555, 0.013734915, 0.029267967, 0.00923265, 0.056859195, 0.071512535, 0.015027537, 0.013593086, -0.00077715574, 0.037089106, 0.03164211, -0.067887865, -0.017943842, -0.020072332, 0.027906219, 0.03164998, -0.031698436, -0.012253368, -0.034878187, -0.037764, 0.019614145, -0.017426318, -0.0424468, 0.024619859, 0.012259737, -0.03587581, 0.018923167, 0.004453674, -0.03949193, 0.014522, 0.019562908, -0.0063952627, -0.025928786, -0.029807135, 0.011236914, -0.028013824, 0.0042761136, -0.013851949, 0.035550665, 0.026193477, -0.0024904879, 0.04623843, -0.015538538, 0.013487982, 0.018636227, -0.064096935, -0.01888929, 0.05674132, 0.041257247, 0.014047411, -0.06228236, -0.023458503, 0.005947587, -0.026625464, 0.02187319, -0.052480165, 0.0018467597, -0.027869895, -0.02388637, -0.0056115887, -0.048967674, 0.04019954, 0.028140392, -0.011031966, -0.05160657, -0.014488234, -0.04062679, -0.023000216, -0.059006356, 0.02734283, 0.009310496, -0.03221412, -0.0012213958, -0.017259417, -0.0005376162, 0.04102634, 0.013983454, -0.08444781, 0.040730607, -0.023053618, -0.013322444, 0.004736628, -0.021158325, 0.05943703, 0.003076465, 0.02951572, 0.1476239, 0.013685253, 0.028581992, -0.081656486, 0.025690893, 0.0030536337, -0.043557547, -0.019779893, 0.0008665751, 0.008527395], "index": 1, "object": "embedding"}, {"embedding": [0.061958358, 0.053372238, -0.18356057, -0.04527216, 0.0371327, -0.06694251, 0.071426645, -0.011694231, -0.0490387, 0.008613741, -0.0206099, 0.055394955, 0.082673125, 0.019691786, 0.030665636, 0.025713122, 0.00046836937, -0.050591283, -0.009836558, -0.013053675, 0.004747503, -0.057450175, -0.029460156, -0.015551963, 0.03976181, 0.023959946, 0.005702056, -0.017723588, 0.01659687, 0.0052205333, 0.063009575, -0.04564317, -0.010818892, -0.043066222, -0.008956647, -0.008430378, 0.10366039, 0.04063304, 0.018276153, 0.04313024, 0.027883098, -0.052025218, 0.041634504, 0.0075494666, 0.033133127, -0.015678141, 0.0565665, -0.051927354, 0.023820061, 0.0023097573, 0.020071462, 0.019301498, -0.01896685, -0.01312378, 0.057639524, 0.024128035, 0.009494897, 0.04639943, 0.05352277, -0.06681472, 0.04984536, 0.009096136, -0.10495021, 0.057354067, -0.003789174, -0.019772412, -0.028863018, 0.032444015, -0.04077308, -0.030672284, 0.031677425, 0.00760358, -0.00613427, 0.00011428232, -0.049036134, 0.00011838618, -0.024722528, -0.023121992, 0.00666334, -0.0033690021, 0.0077505214, 0.024288023, 0.029638143, -0.008090428, 0.026436085, 0.025003336, -0.019384122, 0.03248198, 0.0031037044, 0.07180509, -0.01402192, 0.033517852, 0.014417601, 0.065368414, -0.033479102, 0.013195776, -0.022050856, 0.0043546367, -0.0031080516, -0.014128036, 0.006352869, -0.02465844, 0.023875296, -0.012841329, -0.03019435, 0.08389656, 0.0398157, -0.029971212, -0.024756135, 0.0011016349, -0.05164581, 0.046159893, -0.016018806, -0.042537354, -0.03602122, -0.03950389, 0.11418455, -0.03914113, -0.0044452385, 0.034914132, -0.017499529, -0.032347724, 0.008798061, 0.033976138, -0.02354165, 0.017356578, -0.053791624, 0.0012205971, -0.028206084, -0.07174273, -0.0581946, -0.03173175, -0.022296423, 0.016407479, 0.009992111, 0.086872555, -0.014907762, 0.0008268882, 0.013268755, -0.007877514, -0.011465367, -0.015787596, 0.016805025, -0.025076563, 0.00913538, -0.017517006, 0.023712939, -0.0069405274, 0.005857822, 0.006349451, 0.042520083, 0.04451843, -0.022986857, -0.0049596285, -0.01097167, -0.018091353, 0.011637666, -0.0056124623, 0.041785676, 0.013041948, 0.037232798, 0.014464704, -0.03155244, 0.0517189, -0.0033433766, -0.0029418934, 0.02694728, 0.03262651, 0.0029985602, 0.016764838, -0.056133296, -0.06021529, 0.038686197, -0.008717496, 0.047707427, 0.009395665, 0.07579398, -0.025355134, -0.002343172, -0.029906228, 0.045877773, -0.041904926, 0.031198267, 0.024186227, -0.0061674574, -0.021469168, 0.018697333, 0.032094978, -0.007305808, -0.012877215, 0.0309399, 0.067785606, -0.09871442, -0.040760133, -0.044478014, -0.032243326, 0.05207347, -0.059036523, 0.012318432, -0.007123289, 0.004620976, -0.05094796, -0.03078676, 0.042932082, -0.031612914, 0.035965167, 0.041360576, 0.026668211, -0.016014341, 0.005004282, 0.03899249, 0.020825425, 0.024378711, 0.023788193, 0.032277815, -0.0062673665, -0.026284998, -0.021996567, -0.015860746, 0.01763225, 0.011597515, -0.020759922, 0.01931603, 0.025211027, 0.0016901352, -0.032734204, -0.047679614, 0.009360591, -0.029613331, 0.011850098, 0.030183168, -0.03342141, 0.024295492, -0.014466755, -0.027516834, 0.01908383, 0.011648599, 0.065837465, 0.006273041, 0.058193363, 0.06384522, 0.015259612, -0.010046638, -0.007624424, -0.06828765, -0.014099531, 0.0067812093, -0.057816826, -0.021120118, 0.07512572, -0.037707064, 0.05633534, 0.02431155, 0.036318555, 0.031204047, 0.020552227, -0.042024784, -0.01907294, 0.048958946, 0.007774101, 0.028461978, 0.01293428, 0.014883485, -0.042380434, -0.05356135, 0.01264274, -0.055165406, -0.052826222, 0.004080213, -0.022301547, 0.020276092, 0.044873245, 0.05486598, 0.038889915, 0.022473754, 0.009486453, -0.051105883, -0.062299155, -0.023400625, 0.010975807, 0.012445658, -0.0761222, -0.060647074, 0.00033232296, -0.024760844, 0.045960855, 0.04605575, -0.030447222, 0.05177788, 0.030935675, 0.017036952, 0.016840188, 0.023305498, -0.021499865, 0.0060116425, 0.0536055, 0.078814454, -0.035065856, -0.0122117745, -0.011105421, 0.04004643, 0.022306908, 0.03806027, 0.018329933, -0.0050397757, 0.010501443, -0.023259576, -0.012086838, 0.006412562, 0.027260194, -0.0353256, 0.0124880215, -0.066158235, 0.026164846, 5.2487565e-05, -0.06696955, 0.016122708, 0.018756242, 0.033769455, -0.038015764, 0.014820195, -0.009902863, -0.0021508494, -0.06873119, -0.019130474, 0.022512132, -0.0011251442, 0.04372097, -0.029076122, -0.017551629, 0.071857005, 0.02522801, 0.017803222, -0.041690458, -0.046900656, -0.021601064, -0.0040487773, 0.01325206, 0.032278493, 0.011397158, 0.037332013, -0.023222288, -0.016166162, -0.051504515, 0.03027329, 0.0027634425, -0.034669165, -0.05724516, -0.007128563, 0.02501476, -0.022534858, 0.009263522, -0.036805548, -0.023065973, 0.026994385, -0.024281073, 0.0036777027, 0.008986125, -0.0015693533, 0.023118192, 0.044138264, -0.01898604, -0.009300811, -0.05982057, -0.037334085, -0.0023428802, 0.03984006, 0.005504578, -0.022602154, 0.006035777, 0.005598468, -0.0437027, -0.024771448, 0.025442721, -0.025376134, 0.03217173, -0.010549656, -0.007839143, -0.026256168, 0.017087914, 0.016007166, -0.033912756, 0.005074223, -0.0063041677, -0.03903613, 0.015741082, 0.0047130524, -0.07032891, -0.0122804055, 0.02392438, -0.008040357, -0.058868427, -0.02044364, 0.023904992, -0.0032925918, 0.0047743726, 0.039535966, -0.012109787, 0.0022867396, 0.011309324, -0.046139672, -0.009559532, 0.00965095, 0.0041660834, 0.0105106495, -0.028638132, -0.049070735, -0.011438703, 0.007438576, 0.045689013, 0.029921697, 0.06421008, 0.011241824, -0.09628432, 0.05730101, 0.0032588523, -0.005366447, 0.029914223, -0.02958368, -0.008169066, 0.006269672, 0.03924515, -0.01254186, -0.020783473, 0.03344844, 0.017523307, 0.05091526, 0.056663394, 0.016070884, -0.12382942, 0.068539076, -0.0026832472, 0.073312275, 0.003827547, 0.004779901, -0.02401203, -0.028114319, 0.0029698545, 0.00069459283, 0.009597218, 0.026493713, -0.00037033705, -0.019327512, -0.00041243678, 0.010956451, 0.03659733, 0.018489804, 0.013783435, -0.040739797, 0.018066125, 0.033055294, -0.0029940596, 0.09214461, 0.018402118, 0.07382473, -0.044250134, 0.02375759, 0.020564014, 0.041930083, 0.040134083, 0.039933648, 0.018608222, -0.10744097, 0.014767033, 0.0045448043, -0.023173807, -0.029368792, -0.030469753, 0.021565933, 0.07250357, -0.008891095, -0.028086644, 0.013767219, -0.0069267415, -0.04275459, -0.06313458, -0.030422717, -0.015858307, 0.042896155, 0.081503235, -0.010365132, -0.014971191, -0.0030121969, -0.07540029, -0.027831737, 0.047003746, 0.009555756, -0.06711548, -0.04842973, 0.023164202, 0.023911534, 0.04961828, 0.035617404, 0.03293129, -0.02772878, -0.014415907, 0.005503339, 0.056078304, 0.051537402, -0.011295983, 0.018279625, 0.05075662, 0.030842843, -0.01563509, -0.00951411, -0.065079264, 0.049073167, -0.039886784, -0.056790374, -0.018899126, -0.026158614, 0.07212349, 0.010990957, 0.035901114, -0.03738535, -0.061628714, 0.046237685, 0.07105787, -0.034375664, -0.021626012, 0.03125324, -0.041433413, -0.0074047344, -0.07916651, -0.06028427, 0.038376365, 0.021148933, -0.052774034, 0.010285892, -0.034159347, 0.021865536, 0.0064133643, -0.070693366, 0.024666408, -0.019072859, -0.025641218, -0.021436647, 0.02346003, 0.005136787, 0.012040786, 0.012345844, 0.004616134, 0.03390416, 0.015708767, 0.025914352, -0.037616845, -0.026379416, -0.012681483, -0.010136363, -0.04986739, -0.0039581587, -0.051505644, 0.023354625, -0.032570574, 0.032111526, -0.051225323, 0.033336062, -0.03602489, -0.02207966, 0.005282964, -0.003264236, 0.00800616, 0.060455803, 0.020111073, 0.0036246113, -0.019095667, -0.021654533, -0.0013363261, 0.023355667, 0.02503332, -0.032049727, -0.07931914, 0.00843444, 0.048988048, -0.019797606, -0.015834358, -0.011878961, -0.039493956, -0.04335934, -0.04689583, 0.025192028, -0.050189838, 0.02065152, 0.02436008, -0.0020048877, 0.0182422, -0.06948315, -0.026459675, 0.03733248, -0.02867497, -0.043391135, -0.010507001, -0.0210133, -0.03754063, -0.029521123, 0.013242369, -0.0324037, 0.0022340007, -0.050054714, -0.04132355, 0.01891682, 0.013456909, 0.084677055, -0.048693355, 0.017429288, 0.053174265, -0.017350484, 0.0242602, -0.019564692, 0.022147609, 0.004653351, 0.025523806, -0.009632346, -0.017678311, -0.010012035, -0.026054, 0.05462061, 0.05086449, -0.048935656, -1.151747e-05, -0.05895309, -0.06397097, 0.028739547, -0.08703421, 0.01919397, -0.043400776, -0.011778036, -0.004564849, 0.035840493, 0.0512386, -0.022773562, -0.032493554, -0.05083111, -0.02835415, -0.041616455, 0.02118233, -0.01971258, 0.022486344, 0.0077945483, 0.048032824, 0.01639939, -0.01137384, -0.025878359, -0.021075398, 1.594534e-05, -0.012607486, 0.031850148, 0.073642924, 0.051163856, 0.009566731, 0.09034925, 0.060880534, -0.0009981262, -0.00065374945, 0.005996367, 0.02485217, -0.000994567, 0.016777957, 0.009852525, -0.028416036, -0.0055146883, 0.017040547, 0.036473274, -0.008615472, 0.04561877, -0.020975007, -0.01672937, -0.013001759, -0.03499023, -0.006411542, 0.086352535, -0.025664955, -0.006934883, -0.00049952953, 0.07946337, 0.034824625, 0.014781529, 0.0071594403, 0.00044186035, 0.023130722, 0.053213816, -0.0072563626, 0.039393995, -0.024115192, -0.024119936, 0.0056321146, -0.03014834, -0.045630198, -0.018037485, -0.007434391, -0.0026604393, 0.004285059, -0.03798355, 0.013280494, -0.011993166, 0.05975858, -0.018059326, -0.024689825, -0.00321153, 0.0121249445, -6.287581e-05, 0.0019815734, -0.016336393, 0.014344111, -0.017431676, 0.0017229163, -0.004280439, -0.016429164, 0.0025342933, -0.02222421, -0.06921381, 0.024855277, 0.010739988, 0.030468665, 0.024582718, 0.040473223, -0.011542979, -0.070259504, -0.02601321, 0.008827701, 0.030162087, -0.0011453709, -0.040987402, -0.026505623, 0.028023204, -0.05211807, 0.0028534096, -0.03260106, 0.04110765, 0.017205153, -0.029908545, -0.05389711, -0.058639172, 0.07327386, -0.01522595, 0.021506581, -0.0020300343, 0.021927338, -0.05662171, -0.007832583, -0.047250267, -0.008407682, 0.003841448, 0.019128025, 0.005655963, 0.004655087, -0.045951, 0.05751619, -0.007061747, -0.034739695, -0.038724102, 0.0059165293, -0.048841596, 0.0046648853, -0.052164625, 0.0417256, 0.028973956, -0.0028857572, 0.08001454, -0.02247235, 0.034715842, -0.034876056, 0.07577809, 0.013682219, -0.03996965, -0.032017242, -0.0064487346, -0.0039159996], "index": 2, "object": "embedding"}, {"embedding": [0.0055931252, 0.050098255, -0.15820844, -0.10839814, 0.05240986, -0.014180767, 0.078571156, 0.00052462355, -0.01036276, -0.0054855854, -0.01253135, 0.057763055, 0.13739276, 0.012915742, 0.04139873, 0.019060826, -0.024696194, -0.036708675, 0.012898497, 0.052975554, 0.009319033, -0.028966874, -0.012878228, -0.0027625603, 0.036761448, 0.027077295, -0.0075421757, 0.006886727, 0.012205103, 0.036824264, 0.038640596, -0.0103427125, -0.004309548, -0.029388199, -0.019741258, -0.029806992, 0.06393187, 0.0014730117, 0.02631268, 0.072538264, 0.027855571, 0.013595708, 0.0077814925, 0.020739695, 0.018823273, -0.04195044, 0.06989585, -0.036763117, 0.0024799106, -0.032765463, 0.05360854, 0.03706479, 0.020815955, 0.06591203, 0.053501613, 0.060240854, 0.027753057, 0.035353113, 0.014429764, -0.08132444, 0.00948776, 0.020351129, -0.10023943, 0.014672816, 0.005742121, 0.02186909, -0.051589187, 0.028751904, 0.002095551, -0.015620515, 0.046025198, -0.02061387, -0.008050305, 0.011890632, -0.028359443, 0.0059875012, -0.039184578, -0.032495633, -0.040562704, 0.026199529, -0.012652804, 0.004702935, 0.018326037, -0.007617531, 0.034082677, 0.022483613, -0.010706784, 0.0017947702, 0.0069572995, 0.06869993, 0.011292283, -0.004529227, 0.005061764, 0.03908182, -0.013310034, -0.0003108882, -0.051315926, 0.031215224, 0.011254239, -0.029157752, -0.02360439, 0.0046690977, 0.011004556, -0.013980479, -0.031528093, 0.08022755, 0.029978324, -0.055092677, -0.0032823968, -0.009605148, -0.049124084, 0.00031747017, -0.031930592, -0.043898515, -0.010968595, -0.07279517, 0.06579332, -0.059231985, 0.021779858, 0.0059505547, 0.0015548001, -0.0007156563, 0.029862424, 0.037066694, -0.0055331253, 0.022881368, -0.06279269, 0.0041459026, -0.033016838, -0.08252667, 0.0061827106, -0.014432226, -0.043268174, 0.01113313, 0.034266938, 0.09181481, -0.03914722, -0.038913865, 0.019996187, 0.033221103, 0.004255761, 0.041685212, 0.02270614, -0.03851865, 0.034612235, -0.04066582, 0.01049296, 0.018779114, -0.005675978, -0.0101006655, 0.021430267, 0.057215143, -0.02440891, 0.015052603, 0.006247175, -0.007495754, 0.004088795, 0.016686313, 0.03538331, -0.019018227, 0.04503448, 0.012533742, -0.039564874, 0.044522632, 0.008568986, 0.0578125, 0.054294426, -0.004652686, -0.01103794, 0.032765415, -0.054533124, -0.067178324, 0.0030004685, -0.03661517, -0.0007119148, -0.013634995, 0.061067987, -0.034260895, 0.019625241, -0.0007691651, 0.07163351, -0.053658202, 0.014476465, 0.028412335, 0.0012062665, -0.00071735226, 0.014617131, 0.019647017, 0.00449181, -0.03297345, 0.019018516, 0.027503263, -0.10150149, -0.028882897, -0.059876025, -0.066616744, 0.020542847, -0.083541915, -0.0019065044, 0.021032965, -0.046889022, -0.02265246, -0.0555359, 0.021755194, -0.028597089, 0.06243757, 0.027407758, 0.017834878, 0.009963221, 0.018274296, 0.046038356, 0.0088324975, 0.020637788, -0.010548251, 0.018918967, -0.006465147, 0.0033315774, 0.011972336, -0.026854593, 0.008354758, 0.05064105, -0.003410622, 0.00086573453, 0.027537597, 0.043063924, -0.01442292, -0.033262484, 0.0052747615, -0.030523345, 0.01063638, 0.0036889568, -0.05985355, 0.030524438, 0.018829303, 0.0024521418, 0.031047897, 0.025378427, 0.027028924, -0.016868142, 0.02201122, 0.03297254, -0.0061665038, -0.013116848, -0.017317085, -0.069857135, -0.036135264, 0.01102465, -0.002606053, -0.012126975, 0.06068659, -0.0356611, 0.03074315, 0.034524594, 0.04289874, 0.05073588, 0.018452654, -0.0072976965, -0.03353501, 0.02574194, 0.003061579, 0.040689543, 0.025193641, 0.0038671955, -0.027862232, -0.044105623, -0.0084279375, -0.0027284627, -0.050120853, 0.0038545986, -0.023073532, 0.015704043, 0.0077001415, 0.0088293785, 0.01893757, 0.012049803, 0.035630703, -0.026665535, -0.031167554, -0.036042914, -0.011687436, 0.006548069, -0.05954783, -0.053231217, 0.0060426523, 0.014644237, 0.053386126, 0.016895514, -0.009035519, 0.026294539, 0.029978313, 0.040623706, 0.03667473, -0.0030424788, -0.0016456621, -0.018808497, 0.03217371, 0.061051775, -0.03204955, 0.013500806, -0.009999148, 0.030220408, 0.013332146, 0.04371466, 0.0493574, -0.009778545, 0.027736008, -0.028173355, -0.004382307, 0.023325818, 0.009825115, -0.05140759, 0.025632527, -0.08720623, 0.0034931866, 0.0162022, -0.061374668, 0.04044344, 0.032739352, 0.06035693, -0.013459218, 0.031042174, -0.017860137, -0.026136586, -0.030945601, -0.00012572767, -0.007999782, 0.0052200314, 0.014010451, -0.0063696336, 0.0081224395, 0.09690938, 0.027912816, 0.019356009, 0.010087909, -0.09603261, 0.006215211, -0.00060784613, 0.014298084, 0.016788445, 0.047255494, 0.009774763, -0.029000599, -0.024037259, -0.042973787, -0.0002928793, -0.020787409, -0.035491645, -0.05212467, 0.037844833, 0.047042105, -0.039642584, -0.003496905, -0.037563276, -0.011340663, 0.024374971, -0.0051038587, 0.046511754, 0.018678848, -0.020377457, -0.002376697, 0.04248394, -0.0116780875, -0.019504044, -0.030726045, -0.028345509, 0.027981706, 0.044683374, 0.00015188043, -0.009186984, -0.014873289, 0.027612813, -0.010122692, -0.040288374, 0.034939393, -0.0054919953, 0.04422281, -0.0069152554, 0.017257221, -0.049183782, -0.007914692, -0.0159898, -0.00048620877, -0.016881011, 0.008027875, 0.023087082, -0.005655005, 0.02718153, -0.06887054, 0.0041002096, 0.021752514, -0.0024849547, -0.004439507, -0.025789915, 0.01856798, 0.024687946, -0.0010881296, 0.06744968, 0.02503188, -0.018098421, 0.0016762349, -0.04953328, -0.04407724, 0.010180403, 0.009443507, 0.015566328, -0.070502244, -0.06837727, -0.021093056, -0.01918879, -0.0012454382, 0.033943333, 0.055386886, -0.0013153955, -0.05225452, 0.015464625, -0.025134303, 0.044377275, 0.013727766, -0.03541609, -0.008998839, -0.00402634, 0.03275897, 0.0060687223, -0.0045507015, 0.0007564015, 0.037650954, 0.048559807, 0.06388187, 0.009465046, -0.11317074, 0.06393104, 0.012266336, 0.06449075, -0.024017217, -0.025011647, 0.009909381, -0.024530191, 0.032031365, -0.037327964, 0.008284178, 0.000988654, 0.018833645, -0.008379618, -0.0077797817, -0.0059981993, 0.041926734, 0.01826168, -0.0061487416, -0.06545181, 0.034932572, 0.011158309, 0.015310902, 0.07736283, 0.023347754, 0.085630454, -0.00024020167, -0.0033625038, 0.017670047, 0.014264272, 0.002221473, 0.019696025, 0.02403389, -0.06954709, 0.021829994, -0.0056347437, -0.05284323, -0.03533357, -0.042545505, 0.028641816, 0.035147525, -0.027857045, -0.005814019, -0.006340761, -0.029509163, -0.064235434, -0.031648204, 0.005451859, -0.02428114, 0.039442092, 0.02302837, -0.02111953, -0.06325499, -0.041896585, -0.075242534, -0.04026754, 0.040165313, 0.022217406, -0.037486166, -0.0321878, -0.03362457, 0.011148746, 0.05311464, 0.027614586, 0.025017146, -0.012120236, -0.022160107, -0.0015518082, 0.09137058, 0.045655515, 0.0038308136, -0.0013031871, 0.04417842, -0.019360095, -0.0083441315, -0.029296054, -0.016705215, 0.043860517, -0.012457531, -0.052595347, -0.00034508226, -0.02554542, 0.052249953, -0.0010982171, 0.0096266605, -0.0071559185, -0.09226865, 0.009256757, 0.026095558, -0.027948115, 0.01435485, 0.02244509, -0.026000967, -0.02353132, -0.07957778, -0.03402883, 0.049431916, 0.047609296, -0.041095268, 0.01420363, -0.029911824, 0.043145422, 0.03981683, -0.037587732, 0.013007305, -0.005812796, 0.017252725, -0.037578616, 0.050415915, 0.018061936, -0.023020158, 0.009434598, 0.008331761, 0.046949826, 0.011014297, -0.026500719, -0.043341305, 0.00041827693, 0.013724135, -0.029179107, -0.011525047, 0.0070467386, -0.051049147, -0.0057913894, -0.024438046, 0.017948827, -0.065910965, 0.0074145184, -0.037191294, -0.01701732, -0.029995756, -0.030434344, 0.018080406, 0.07438868, 0.035565272, -0.063166454, -0.0046300082, 0.013047125, -0.0053689126, 0.036868613, 0.016117128, -0.003114985, -0.08116667, 0.057663467, 0.039739106, 0.006610509, -0.028532946, 0.020781454, -0.01092027, -0.03359424, -0.079158224, 0.006183368, -0.036410265, 0.032830533, 0.033865582, 0.006128818, 0.018271133, -0.06729734, -0.02505044, -0.025082046, -0.011028623, 0.004633275, -0.013265471, 0.004070311, -0.04373494, 0.015461003, -0.013266702, -0.061382625, -0.009598125, -0.036684882, -0.022696957, -0.0046544108, 0.0042155846, 0.056756984, -0.040724847, 0.011108625, 0.046362173, -0.009474221, 0.054015104, 0.015527753, 0.013972687, 0.019607762, 0.022541964, -0.0012566618, -0.03183153, 0.009086428, -0.026182458, 0.037863664, 0.028374268, -0.0400504, 0.052144572, -0.068144575, -0.036292247, 0.016841624, -0.08366838, 0.010247813, -0.01797024, -0.039703388, 0.0056344573, 0.035798933, 0.05199397, -0.0573549, -0.0148570845, -0.050150216, -0.059796546, -0.05408532, 0.029408036, -0.036435667, 0.015015295, 0.004229274, 0.019455899, 0.016205715, -0.009799656, -0.009016653, -0.002144884, 0.02826007, 0.008188599, 0.036140442, 0.05426822, -0.012212429, -0.009649138, 0.080084056, 0.07010187, 0.029629534, -0.0008630739, 0.009808258, 0.024413629, 0.0046845833, 0.040173206, -0.056928728, -0.018700326, -0.006178972, -0.000105303596, 0.016113127, -0.029107563, 0.046153188, -0.027426288, -0.036897182, 0.0114362035, -0.041029938, -0.011301374, 0.0647269, -0.015059439, 0.014357798, 0.006951124, 0.06164786, 0.055027597, 0.035301946, 0.0029110888, 0.013786472, 0.016004855, 0.029500341, -0.0327921, 0.052225895, -0.061478548, -0.04533663, -0.042758018, -0.0063884878, -0.06166915, -0.07091899, -0.027990378, -0.0144383665, -0.014685887, -0.029195558, 0.021087391, -0.0055980887, 0.04655649, 0.0029609802, -0.029756494, -0.0016848913, 0.03911254, -0.0054701287, -0.0027311852, -0.054112703, -0.04136589, 0.011779466, 0.0023425475, 0.02847608, 0.00084437674, -0.01306282, -0.09252847, -0.04695427, 0.032148167, -0.011210067, 0.027839312, 0.012341177, 0.016575895, -0.037241146, -0.049822383, -0.023132429, 0.026805269, 0.041012738, -0.041637003, -0.019274011, 0.023673149, 0.050196223, -0.041294858, -0.048795193, -0.0976989, 0.039743505, 0.029191336, -0.052427772, -0.011236078, -0.035559513, 0.0312233, -0.028270891, 0.029457452, -0.014300486, 0.011594899, -0.068912394, -0.0003109815, -0.029139468, 0.029658357, 0.005376635, 0.038766105, 0.0033757037, -0.006590781, -0.035180885, 0.075249515, 0.029982837, -0.014418836, -0.036826506, -0.034915734, -0.06268905, -0.025904942, -0.04741601, 0.059843954, 0.012764877, 0.002122544, 0.096999645, 0.01730708, 0.036425374, -0.034169726, 0.08892144, -0.006204445, -0.059034, -0.033343896, 0.007338291, -0.020345211], "index": 3, "object": "embedding"}, {"embedding": [0.062016536, 0.057946812, -0.16349855, -0.03761383, 0.0046161613, -0.10579854, 0.030412458, 0.029892262, -0.04273446, 0.011276346, -0.05619776, -0.0028326765, 0.071672566, 0.024694065, -0.013771277, -0.0071149957, 0.007270138, -0.050640527, 0.020365061, 0.009855307, -0.04455423, -0.051203437, -0.049400125, -0.007735428, 0.01495979, 0.040704556, -0.020194232, 0.016687866, -0.05760522, -0.0003231006, 0.02197361, -0.019033305, 0.025937721, -0.03505417, -0.056205966, 0.0015015553, 0.03377546, -0.023405945, 0.02390203, 0.05177868, 0.00037895044, -0.057082858, 0.018884474, -0.02151833, 0.05750364, -0.04895089, 0.0752907, -0.059554033, 0.04668088, -0.05140627, -0.012441616, 0.012890142, -0.041987315, -0.030789284, 0.055833776, 0.009478024, -0.066096544, 0.04668487, 0.06870574, -0.006622027, 0.009408937, 0.057113584, -0.04438592, -0.013882797, 0.010368863, 0.048290793, 0.021374578, 0.038990103, 0.022934493, -0.01778043, 0.07823181, 0.0028344216, 0.06258282, 0.00087628537, -0.037545793, 0.00983067, -0.03161932, -0.014811814, -0.017240867, 0.037045915, -0.011203869, -0.033935208, 0.056691125, -0.0066407323, 0.0057280352, -0.017186254, -0.021086365, 0.051435065, 0.010016665, 0.02104191, -0.046489723, 0.05286863, 0.025142392, 0.05171498, -0.09196482, -0.018002648, -0.018175192, 0.028787566, 0.030622853, -0.04177409, 0.046429556, -0.0011738492, -0.034996867, 0.013169851, -0.03295928, 0.078699805, 0.052708726, 0.0003463068, 0.020214576, -0.051635392, -0.13322763, -0.0014617195, -0.009283126, 0.0005513461, -0.0303643, -0.019044554, 0.039322637, -0.072096705, -0.013777294, 0.047934256, 0.000244522, -0.0063259657, -0.039893437, 0.03334816, 0.028060006, 0.020563807, -0.019373486, -0.015586518, 0.013168663, -0.05414935, 0.015582809, -0.04471917, 0.01430603, 0.01810885, 0.012661077, 0.04864716, 0.018260172, -0.05024685, -0.011416329, -0.026805755, -0.02099048, -0.037993465, -0.009851614, 0.00660515, 0.049657423, -0.054415245, -0.029891152, -0.004606491, -0.058861554, 0.04166864, 0.026115132, 0.02867864, 0.015741624, 0.017793711, -0.0010433756, -0.05597508, 0.06239388, 0.025023544, 0.03584232, -0.0026580403, 0.007888519, -0.035145503, -0.024149477, 0.034065846, 0.04344299, -0.0144619, 0.0022103887, 0.019052915, 0.007112347, -0.021710088, -0.053565886, -0.07709992, -0.02148193, -0.016997192, 0.00701392, -0.036950696, 0.05175647, -0.046154447, -0.0135679105, -0.0116802715, 0.072244294, -0.0004207254, 0.045190968, -0.01076923, 0.011363014, -0.032762375, 0.023023693, 0.0006536432, 0.001259161, 0.04438187, 0.0225455, 0.042747185, -0.05314942, -0.0102684805, -0.08901165, -0.006819605, 0.05737683, -0.013091009, -0.0043283026, -0.07052808, -0.0047708573, -0.00011213162, -0.050509196, 0.054235265, 0.017927539, 0.015588702, -0.015047297, 0.056229968, 0.011239993, 0.01772677, 0.04273917, 0.00813622, 0.004861951, -0.0037919518, -0.0032687131, -0.007299616, -0.0005417315, -0.016704047, 0.0032741965, 0.0479919, 0.04166431, -0.023482746, 0.0097385505, -0.032503616, 0.02810743, 0.0027330793, -0.056555312, 0.009165274, -0.0027609668, -0.015381627, -0.03089928, -0.0598861, 0.061011348, 0.011898328, 0.012730886, 0.044408984, -0.019098924, 0.06271048, -0.052987568, 0.054283626, 0.07582297, -0.013803814, 0.0024070994, -0.019166645, -0.06687085, -0.019514423, 0.017914038, -0.053464577, 0.00050245144, 0.07143657, -0.018213844, 0.056073718, 0.038118806, 0.03768933, 0.036000032, 0.016665887, -0.062790655, 0.00843723, 0.024410896, -0.0038406262, 0.00214746, -0.0005802915, 0.007128871, -0.0048518274, -0.065087244, -0.036422536, -0.00058898417, -0.067590766, 0.049028277, -0.0070040985, -0.047436632, -0.003986906, 0.016338458, -0.0025762026, 0.02188522, 0.032483865, -0.009334519, 0.0014392617, -0.033421952, 0.04040335, -0.020961309, -0.028170148, -0.07985223, -0.029100979, 0.033133794, 0.025423948, 0.07545026, -0.016924005, 0.041507468, 0.037138537, -0.009738835, 0.022822946, -0.059521805, -0.030182403, 0.01714493, 0.059188884, 0.065988585, -0.06480808, -0.011172572, -0.02318561, 0.06007968, 0.03879956, 0.089854985, 0.034523387, -0.024625191, 0.016133713, -0.06519441, -0.021856206, 0.02728822, 0.013382583, -0.026218176, -0.013543038, 0.009017125, 0.030269599, -0.005688729, -0.015192677, 0.040946495, 0.032147635, 0.038117845, -0.03976152, -0.002118242, -0.059080318, 0.0009104694, -0.06610452, 0.00976025, 0.035072673, 0.021180606, -0.011030481, -0.033597376, -0.00881026, 0.038134348, 0.04728997, 0.033510152, -0.02000988, -0.06226515, -0.006230456, 0.00074816914, -0.021448415, 0.008871853, 0.047856916, 0.026377667, -0.028441794, -0.033464134, -0.036243312, -0.034416713, -0.01620962, -0.018406928, 0.009718046, 0.032268982, 0.012936834, -0.021188207, 0.018363088, -0.015980013, 0.025059544, 0.008640527, 0.004962267, 0.018711008, -0.0081442725, 0.0039609172, 0.019383542, 0.09493517, 0.0105000455, 0.01918442, 0.0009212502, -0.007587582, 0.011459801, 0.004021974, -0.006912042, -0.026224105, -0.023738671, 0.02134232, -0.033899363, -0.026443485, 0.031750433, -0.005692221, 0.00727955, -0.051458664, -0.014440067, -0.009477369, 0.026574517, -0.019398632, -0.008364313, -0.005299035, -0.030501451, 0.0014177008, -0.01689936, 0.04743286, -0.016535355, 0.017097816, 0.05182284, -0.033755604, -0.03363192, -0.015192298, -0.004307623, 0.011143669, 0.0003600131, 0.04651219, 0.0016978469, -0.037372954, -0.00867367, -0.03363337, -0.026589956, 0.05457916, 0.0015295887, -0.0067412322, 0.0019790235, -0.0595961, -0.03500603, 0.03281833, -0.010185647, -0.010360633, 0.05694723, 0.02797291, -0.06951053, 0.02193644, 0.019070858, 0.03238963, -0.0011350967, -0.024961688, 0.013679018, 0.013819357, 0.01819972, -0.037838493, -0.04162562, -0.010509595, 0.008230833, 0.052521717, 0.05571388, 0.042588487, -0.05586178, -0.00032325825, -0.0026024163, 0.07667906, -0.030200351, 0.008549794, -0.01632617, 0.053789083, 0.009443608, 0.006494421, 0.031248402, 0.009356685, -0.04158795, 0.010128332, -0.06502687, 0.027715733, 0.077374645, -0.0022630247, -0.03194417, -0.07324673, 0.036948282, -0.0108318515, 0.0045597, 0.037090477, 0.014440545, 0.028355226, 0.019519156, 0.03823584, 0.023876132, 0.041684594, 0.028878758, -0.012575823, 0.007966062, -0.08880302, 0.0031506547, -0.01098751, -0.0068945866, -0.005256379, -0.046191916, 0.05515194, 0.03304347, 0.0093945805, -0.032542203, -0.006336742, 0.04558073, 0.005403405, -0.003745972, 0.0014157321, 0.0005049609, 0.00427986, 0.05134814, -0.03784319, -0.011704828, -0.02496552, -0.035356224, 0.009475381, 0.027846659, 0.021416703, -0.019604353, 0.015138249, 0.025963666, 0.02233903, 0.04278524, -0.019657612, 0.05255005, -0.018046154, -0.008192962, 0.00050487625, -0.022427293, 0.009676129, -0.012005579, 0.015775537, 0.055505823, -0.014465162, 0.013409461, 0.033878688, -0.036976445, 0.07371729, -0.012851264, -0.018081965, 0.018832467, -0.0318821, 0.04399428, 0.036653668, 0.0052154534, -0.008090989, -0.020023644, 0.020743709, 0.021642713, 0.011198104, -0.013224709, 0.045413353, -0.03880977, -0.02746922, -0.027084043, -0.10095739, 0.005207467, -0.011625583, -0.053805124, 0.06579536, -0.03388506, 0.000384095, 0.040847033, -0.06691798, -0.027027585, -0.017002065, -0.016871752, -0.030206637, 0.040670972, -0.005169061, 0.018414086, 0.00987865, -0.015478643, 0.04851199, -0.03595341, -0.005292283, -0.019624172, 0.05658795, 0.02086078, -0.003785985, -0.037016056, 0.0023620012, 0.0026115314, 0.022145543, -0.016814586, 0.102961816, -0.03201266, 0.010761441, -0.07624077, 0.022481961, 0.011792951, 0.025709148, -0.02798743, 0.03566003, -0.005840299, -0.04137335, -0.019502869, -0.017697772, 0.017192805, -0.013165723, 0.08167151, 0.0097525185, -0.109699346, 0.06726754, 0.003913001, -0.022028882, -0.034021135, 0.019905936, 0.034643676, -0.029419864, -0.048586905, -0.018017713, 0.01140477, -0.025453392, 0.031180805, -0.03391142, 0.02164502, -0.04754126, -0.009811183, 0.002051035, -0.028437503, 0.0403495, -0.039488126, -0.0047491523, -0.013375695, 0.00086585374, -0.022544377, 0.052997507, 0.008426387, -0.033786803, 0.0029880095, 0.02550631, 0.024649587, 0.052335683, -0.012099267, -0.018233003, 0.026488125, -0.007878232, -0.0016492255, -0.0017237917, -0.039342895, -0.035300396, 0.0052278102, -0.01426503, -0.007705569, 0.027504558, -0.006670589, 0.04107876, 0.017012313, -0.013097977, 0.006041137, -0.016727107, -0.032811303, 0.008590172, -0.015519014, -0.013770204, -0.034257673, -0.023220623, 0.000961162, 0.044461798, -0.032514587, -0.04803698, 0.021964693, 0.013140758, -0.043601643, -0.01642367, 0.10502348, -0.038141333, 0.0281462, -0.035586897, 0.13373072, 0.043148454, -0.0046079047, -0.018099131, 0.008011017, 0.009025087, 0.027937027, 0.016885703, 0.0822088, 0.03690868, -0.029426178, 0.051730134, 0.07911364, 0.02660032, 0.04626728, 0.009180996, 0.020807993, -0.012085637, -0.011589961, 0.002399885, -0.018334812, -0.010139973, -0.015263257, 0.010987494, 0.0058433316, 0.009491376, 0.019258374, -0.017767293, -0.021366121, -0.018719494, -0.035923038, 0.063549064, -0.0026860835, 0.0007158683, -0.022257416, 0.024511736, -0.0030729424, 0.018326348, -0.0007530787, -0.040393848, 0.0036689932, -0.008634027, -0.017502645, -0.024365723, -0.024499679, -0.00621267, -0.012886803, -0.017254343, -0.026907582, 0.031618904, -0.007244312, -0.033457953, 0.0059192786, -0.045015186, 0.052137464, -0.018221498, 0.071217045, -0.048872594, -0.029330583, -0.021489358, 0.06382183, -0.0050789188, 0.017626977, -0.009241535, -0.0009569448, -0.028215965, 0.0307634, -0.034691017, -0.035677318, -0.004890339, -0.00533682, 0.020445433, 0.037540086, -0.038490143, 0.0114413705, 0.029130522, 0.05936344, -0.020824563, -0.022766257, -0.07331673, 0.0027633938, 0.043456502, 0.00839521, -0.004769788, -0.022511248, 0.046444878, -0.01619219, 0.01740925, -0.01971658, -0.0016554514, -0.013447255, 0.005041782, -0.06300629, -0.024932485, 0.10150459, -0.045311805, -0.02093641, -0.035564266, 0.018351642, -0.055433553, 0.01288217, -0.054731272, -0.013706981, -0.011073462, -0.02653274, -0.005750726, -0.02646145, -0.054801516, 0.031012943, -0.0024800745, -0.027170712, 0.013339599, -0.02520617, -0.083512664, -0.025916893, -0.041250598, 0.06815053, 0.02509123, 0.009780502, 0.13096546, 0.0021677688, 0.02927366, -0.01647504, -0.035259467, 0.017717391, -0.0043462305, -0.048538614, -0.04165899, -0.026421746], "index": 4, "object": "embedding"}, {"embedding": [0.068090506, 0.04018458, -0.18768406, -0.020016875, 0.073060356, -0.04408119, 0.06529904, 0.030898206, 0.015692106, -0.031446293, -0.013295989, 0.0013800106, 0.06447972, 0.031688817, 0.016250981, 0.048208356, 0.027786124, -0.027038097, 0.016552044, 0.01449732, -0.016035218, 0.019857103, -0.015808204, -0.0015222839, -0.057522375, 0.006829273, -0.05209272, -0.0051168883, -0.006632899, 0.0043971944, 0.057775922, -0.022282083, -0.005088758, -0.032106984, -0.059747513, -0.029260572, 0.077065095, 0.0065591913, -0.0008025733, 0.047733903, -0.010844723, 0.00976015, 0.048910458, -0.04668432, 0.042309485, 0.00024656128, 0.03627986, -0.04660324, -0.0065418957, -0.02627727, 0.025464546, 0.016429186, -0.02489649, -0.030403709, 0.08345146, 0.0017246708, -0.024034562, 0.039295018, 0.012408814, -0.09096571, 0.041573137, 0.040466398, -0.0771665, 0.06345906, -0.04567116, 0.012384971, -0.067175604, 0.042948272, -0.025584193, -0.047274366, -0.0053891023, -0.009574951, -0.013578678, 0.058042314, -0.059754767, -0.005313933, -0.016285704, -0.020785064, -0.0036371509, 0.0045246743, -0.0015057344, -0.05959861, 0.062384956, 0.0023340653, -0.00014280048, 0.01898764, -0.022351129, 0.013147203, -0.013334279, 0.11308031, -0.030027192, 0.022639468, 0.019973896, 0.014237642, -0.005881271, 0.039509922, -0.0076787476, 0.009326597, -0.02290069, -0.03394082, -0.04312453, -0.04485642, 0.002936603, -0.008546145, 0.008975514, 0.061364844, 0.0288752, 0.0071014143, -0.0181279, -0.0018743068, -0.032756515, 0.0132790245, -0.030106297, -0.014875319, -0.029576916, 0.0022141729, 0.030903684, -0.012125805, 0.0010853831, 0.015335501, -0.04268114, -0.01854445, 0.003962689, 0.01743378, -0.0064253956, 0.031911325, -0.12140316, -0.02126264, -0.01532887, -0.01863479, -0.03873161, -0.034482006, -0.025773786, 0.022858255, 0.031806756, 0.083230086, -7.597383e-05, 0.0051456084, 0.024089856, 0.003792492, 0.0079787625, -0.030630004, 0.0020618632, -0.0049001253, 0.027059974, -0.004833619, -0.011071069, -0.0046713552, -0.049604535, -0.014726117, -0.0062359995, 0.0708381, 0.010917542, 0.040166676, 0.018602416, -0.061374843, 0.03859741, -0.0052235504, 0.009615454, 0.046547975, 0.033976898, 0.01368608, -0.04373402, 0.027750464, 0.016072996, 0.02831067, -0.0046452233, -0.005024908, 0.0199233, 0.03560714, -0.09941818, -0.05498702, 0.022935653, 0.002254697, 0.0530921, -0.043225694, 0.083883315, -0.031518597, -0.022928117, -0.003256462, 0.055652466, -0.027746756, 0.02980085, 0.037370466, 0.016325857, 0.022991432, -0.004461413, 0.021166924, -0.008079493, -0.010408911, 0.013849719, 0.018348116, -0.10645122, -0.0075150863, -0.027996046, -0.008788172, 0.053034317, -0.06781892, 0.01678125, 0.009847686, -0.023377249, -0.044250138, -0.02413767, 0.0014716368, -0.049067758, 0.06031954, 0.01664758, 0.037970543, -0.013566737, 0.0014974565, 0.012360683, 0.045705084, 0.012694026, 0.0588638, 0.029576495, -0.02176092, -0.009169972, 0.0044425526, -0.0072663845, 0.008384538, 0.03928527, 0.031098222, 0.010319903, 0.00068594166, -0.039816957, -0.03675669, -0.009540914, 0.003638461, -0.03962734, 0.0712077, 0.05333818, -0.06600181, 0.039826445, -0.0129470695, -0.019385176, 0.019739497, 0.014102855, 0.015936656, -0.025672862, 0.08103807, 0.018859105, 0.026466299, -0.026646907, -0.030298166, -0.09042346, -0.00861577, 0.022358466, -0.078801125, 0.010044445, 0.0258298, -0.016237041, 0.03491557, 0.06085613, 0.0055429675, 0.010714463, 0.0023080173, -0.04330102, 0.0072460594, 0.026143696, 0.027516268, 0.05739708, -0.015672542, 0.010040177, -0.00974312, -0.020454291, -0.076026015, 0.004258291, -0.008270771, -0.028080517, -0.025866339, 0.013040993, 0.013477692, -0.0010658831, 0.072625205, -0.032258954, 0.041068807, -0.03640239, -0.03255943, 0.0067666597, 0.041077662, -0.0074037123, -0.08085117, -0.062457025, 0.022363786, -0.0071188505, 0.03787728, 0.021314973, 0.012920267, 0.0056568473, 0.028939871, 0.007252347, 0.040970277, 0.017121917, -0.0057153716, 0.009254364, 0.029177796, 0.047824133, -0.07703584, -0.047159825, 0.019107506, 0.0018797367, 0.039235704, 0.046942502, 0.054949652, -0.022173118, -0.037215598, -0.029491352, -0.048190024, -0.0060037593, 0.0062379534, -0.030267926, -0.004975263, -0.08971344, -0.014956928, -0.028892148, -0.0122895185, 0.034989946, 0.036341865, 0.04519663, 0.0052876556, 0.04712265, -0.0064751618, -0.030136924, -0.053922433, 0.01959476, 0.059505265, -0.0178059, -0.005764434, -0.053587902, -0.014889226, 0.09543266, 0.014052442, 0.019381678, -0.049357716, -0.026031472, 0.030724183, -0.0019651777, 0.007873427, 0.037305925, -0.0019781173, 0.07103899, -0.04985783, -0.0069485926, -0.037829813, 0.009781001, 0.03314868, -0.006345226, -0.018522726, 0.03673527, 0.004550112, -0.03807988, -0.020949226, -0.065925375, -0.058614176, 0.06968014, 0.0030328624, 0.024401458, 0.051938735, 0.040047914, 0.022750365, -0.0014541808, -0.03156331, 5.630116e-05, -0.026044095, -0.03592111, 0.020965943, 0.04350245, 0.014438398, -0.020363448, -0.012377862, 0.012068576, -0.049584176, -0.027840354, 0.042491935, -0.03527507, 0.032301057, 0.00638144, -0.03164738, 0.0033117824, 0.003196126, 0.042671196, 0.032747574, -0.04334613, 2.795599e-05, 0.039084848, 0.014680648, 0.038563043, -0.047188073, 0.021770656, 0.06402826, 0.019829266, -0.022518806, -0.014171805, -0.02974653, -0.03052219, -0.013738793, 0.023796529, -0.013605852, -0.017437682, 0.011739712, -0.03857681, -0.040106293, -0.015828433, 0.004093692, -0.0043737264, -0.011293859, -0.06324762, -0.023032602, 0.017931461, -0.00048251596, -0.008328881, 0.04634527, -0.02452473, -0.03969235, 0.032798864, 0.001606584, 0.030445278, -0.029470688, 0.008149263, -0.026510978, 0.015533869, 0.003142631, -0.0016901897, 0.0060077626, 0.02885744, -0.018838841, 0.008998883, 0.035450295, 0.018528452, -0.097094215, 0.051778495, -0.012013815, 0.05573189, -0.028794752, -0.009641166, 0.003947933, -0.049642276, 0.0037116297, -0.028354323, 0.049749523, 0.041275002, -0.011578829, -0.0633477, -0.007964645, -0.0118136825, 0.048722528, 0.035401903, -0.0007209564, -0.044884644, -0.012049413, -0.023472989, 0.0006358629, 0.08337009, 0.05133806, 0.07064094, -0.052983772, 0.013025981, -0.005597405, 0.036988843, 0.026860723, 0.015960252, 0.02214038, -0.06328543, 0.029537065, 0.040798463, -0.0064090546, -0.008577552, -0.042044472, 0.018935392, 0.056395803, 0.02310638, -0.039527733, 0.035069812, -0.0045246454, -0.0019750865, -0.03630715, -0.005752826, 0.0010536009, 0.023676043, 0.02092075, -0.00286906, -0.056291416, 0.0012536868, 0.01040992, -0.019581929, 0.0045956583, -0.0015475757, -0.01604144, -0.004471871, -0.024074042, 0.047576364, 0.040775206, 0.023610031, -0.036304407, -0.013878893, -0.03689623, 0.010220482, 0.048964866, 0.024154702, -0.020489542, 0.01619179, 0.06848159, -0.018832758, -0.041354053, 0.008802344, -0.034753453, 0.057583597, -0.07275879, -0.07483702, -0.002013413, -0.011668004, 0.009410773, -0.0107520325, 0.036898006, 0.0569093, -0.043267503, 0.036330268, 0.0020132093, -0.03153198, -0.016688937, 0.05684508, 0.00353636, -0.049908463, -0.0369409, -0.06165986, 0.037185114, -0.013807853, -0.07142202, 0.015985645, 0.035823394, 0.050103694, 0.05444168, -0.011554701, 0.0053739226, -0.0077581247, 0.033213686, -0.03696478, -0.016115874, 0.03639097, 0.03937856, -0.01609101, 0.068843365, 0.01977448, -0.0055310386, 0.006538018, -0.024767814, -0.0007068032, 0.005098875, 0.0022248956, -0.054940753, -0.010869174, -0.06435004, 0.0019296982, -0.04278745, 0.015254043, 0.004920249, -0.0071407054, -0.0759319, 0.00085212366, -0.057487663, -0.017162878, 0.025914473, 0.072923675, 0.05187724, 0.0076372973, 0.0044514243, 0.012957861, -0.021125622, 0.016069643, 0.027281806, 0.0028037194, -0.054974362, 0.0028660065, -0.01254222, -0.0028460114, -0.029534923, -0.017639348, 0.0068356455, -0.048415218, -0.08278042, -0.010772305, -0.03227236, 0.040742397, 0.018297166, -0.0521334, -0.01570587, -0.06599357, -0.011862088, 0.06208957, 0.018057982, 0.0070090117, 0.0105512785, 0.023213249, -0.0289874, 0.018396415, 0.00854972, 0.021789288, -0.029347839, -0.06804454, -0.024633927, 0.010202073, 0.019288708, 0.069867, -0.079842396, 0.0469368, 0.07015896, -0.0070131132, 0.061036706, -0.00651846, -0.0052663903, -0.026604086, -0.021527495, 0.014150533, -0.010013841, -0.036051948, -0.03785803, 0.04666353, 0.027021274, -0.004494453, 0.028258454, -0.03800231, -0.04794934, 0.04139209, -0.05411769, -0.005301967, -0.024766358, 0.024064459, -0.035064735, 0.03260163, 0.013794722, -0.031368624, -0.027931532, -0.017962674, -0.020180099, -0.026960354, 0.051814284, -0.029506527, -0.008493621, -0.030152302, 0.023725646, 0.007249255, -0.022985984, 0.01854853, -0.0075270114, 0.0074919257, -0.04326754, 0.05318646, 0.050560866, -0.020365667, -0.026050802, 0.09662658, 0.09623727, -0.034807045, 6.5914806e-05, -0.043560747, 0.02540274, 0.019967526, -0.0057169623, 0.0013253033, -0.012857009, -0.0004588161, 0.014303755, 0.0005116671, 0.006848044, 0.009388493, -0.010971446, -0.0030303472, 0.006553839, -0.029434174, -0.009861822, 0.049837712, 0.03284269, 0.016386323, 0.008890633, 0.050510753, 0.014649195, 0.03619526, 0.07522728, 0.033029184, -0.004724655, 0.009982378, 0.011334282, 0.018162267, -0.04649317, -0.05432187, -0.061226398, 0.00069105887, -0.016029108, -0.017150443, 0.002409205, -0.03508653, 0.003627439, -0.032756068, 0.0346814, -0.035425667, 0.047453016, 0.043708425, -0.044141315, 0.032063827, 0.024515508, -0.020515336, 0.032092176, 0.014198998, 0.036098447, 0.020854035, -0.009706482, 0.0065763304, -0.04783035, -0.011299251, -0.037059914, 0.01184879, -0.017685022, -0.014059538, 0.03564609, 0.0409721, -0.025117623, 0.007200838, -0.05361858, -0.0030344042, 0.022814881, 0.040620264, 0.0024173402, -0.022795046, -0.010475831, 0.033164024, 0.005301661, 0.0031690367, -0.09074787, -0.0004960128, 0.0022545124, -0.021412984, -0.04132758, -0.038133927, 0.021562718, 0.019229451, 0.0004866463, -0.027242478, 0.005909668, -0.05916237, -0.031655017, -0.04722614, 0.012095628, 0.022684414, -0.0029923755, -0.023284743, -0.004438976, -0.013131085, 0.027456919, 0.03570298, -0.06412403, 0.005984774, -0.0064221756, -0.05341584, -0.024486575, -0.018455405, 0.040053558, 0.004242503, 0.027305977, 0.13979366, 0.030952752, 0.015474686, -0.03763499, 0.06524616, 0.017397793, -0.036829636, 0.0036834644, -0.0063483925, -0.017592385], "index": 5, "object": "embedding"}], "model": "nomic-embed-text:v1.5", "object": "list", "usage": {"prompt_tokens": 7153, "total_tokens": 7153}}, "input": {"input": ["\nand retrain the model to keep it current with new data trends and patterns.\n Documentation and Reproducibility: Maintain thorough documentation of your training\nsetup, including the hardware configuration, software environment, and hyperparameters used.\nEnsure reproducibility by setting random seeds and providing detailed records of the training\nprocess. This practice not only aids in debugging and further development but also facilitates\ncollaboration and sharing of results with the broader research community.\n33\nChapter 6\nStage 4: Selection of Fine-Tuning\nTechniques and Appropriate Model\nConfigurations\nThis chapter focuses on selecting appropriate fine-tuning techniques and model configurations that suit\nthe specific requirements of various tasks. Fine-tuning is a crucial stage where pre-trained models are\nadapted to specific tasks or domains.\n6.1\nSteps Involved in Fine-Tuning\nThe following steps outline the fine-tuning process, integrating advanced techniques and best practices.\n1. Initialise the Pre-Trained Tokenizer and Model: Begin by loading the pre-trained tokenizer\nand model. The tokenizer ensures that the input text is converted into a format the model can\nprocess, while the pre-trained model serves as the foundation for further adaptation. Depending\non the task, select a model that has been pre-trained on relevant data to provide a strong starting\npoint.\n2. Modify the Models Output Layer: Adjust the models output layer to align with the specific\nrequirements of the target task. This may involve modifying existing layers or adding new layers.\nFor instance, tasks like classification may require a softmax layer with the appropriate number of\nclasses, while text generation tasks might involve changes in the decoding mechanism.\n3. Choose an Appropriate Fine-Tuning Strategy: Select the fine-tuning strategy that best fits\nthe task and the model architecture. Some Options include:\n Task-Specific Fine-Tuning: For tasks such as text summarisation, code generation, classi-\nfication, and question answering, adapt the model using relevant datasets.\n Domain-Specific Fine-Tuning: Tailor the model to comprehend and generate text relevant\nto specific domains, such as medical, financial, or legal fields.\n Parameter-Efficient Fine-Tuning (PEFT): Techniques like LoRA, QLoRA, and adapters\nallow for fine-tuning with reduced computational costs by updating a small subset of model\nparameters.\n Half Fine-Tuning (HFT): Balance between retaining pre-trained knowledge and learning\nnew tasks by updating only half of the models parameters during each fine-tuning round.\n4. Set Up the Training Loop: Establish the training loop, incorporating the selected fine-tuning\nstrategy. The loop should include data loading, loss computation, backpropagation, and parameter\nupdates.\nWhen using PEFT methods, ensure that only the relevant parameters are updated\nto maximise efficiency. Implement techniques like dynamic learning rates and early stopping to\nenhance the training process.\n34\n5. Incorporate Techniques for Handling Multiple Tasks: If fine-tuning for multiple tasks,\nconsider strategies like fine-tuning with multiple adapters or leveraging Mixture of Experts (MoE)\narchitectures. These methods allow a single model to handle various tasks by utilising specialised\nsub-networks or adapters for each task.\n6. Monitor Performance on a Validation Set: Regularly evaluate the models performance on\na validation set to ensure it generalises well to unseen data.\nAdjust hyperparameters such as\nlearning rate, batch size, and dropout rates based on the validation performance. Utilise advanced\nmonitoring tools to track metrics like accuracy, loss, and overfitting.\n7. Optimise Model Using Advanced Techniques: Employ techniques such as Proximal Policy\nOptimisation (PPO) for reinforcement learning scenarios, or Direct Preference Optimisation (DPO)\nfor aligning model outputs with human preferences. These techniques are particularly useful in\nfine-tuning models for tasks requiring nuanced decision-making or human-like responses.\n8. Prune and optimise the Model (if necessary): To deploy the model in resource-constrained\nenvironments, consider pruning techniques to reduce its size and complexity. This involves removing\nunnecessary parameters or components without significantly affecting performance. Utilise dynamic\npruning methods during inference to optimise the model on-the-fly for different scenarios.\n9. Continuous Evaluation and Iteration: Continuously evaluate the models performance across\nvarious tasks using appropriate benchmarks. Iterate on the fine-tuning process, making adjustments\nbased on performance metrics and real-world testing. This iterative approach helps in refining the\nmodel to meet specific performance criteria.\n6.2\nFine-Tuning Strategies for LLMs\n6.2.1\nTask-Specific Fine-Tuning\nTask-specific fine-tuning adapts large language models (LLMs) for particular downstream tasks using\nappropriately formatted and cleaned data. Below is a summary of key tasks suitable for fine-tuning\nLLMs, including examples of LLMs tailored to these tasks.\nTask\nDescription\nKey Models\nText Summarisation\nCondensing long texts into coherent sum-\nmaries while retaining key information. Ap-\nproaches include Extractive (selecting key\nsentences) and Abstractive summarisation\n(generating new sentences).\nBERTSUM, GPT-3, T5\nCode Generation\nAutomatically generating programming code\nbased on natural language descriptions, par-\ntial code snippets, or structured data inputs.\nCodex, GPT-3, CodeBERT\nClassification\nCategorising text into predefined labels such\nas Sentiment Analysis, Topic Classification,\nand Entity Classification.\nBERT, RoBERTa, GPT-4\nQ&A\nUnderstanding and generating accurate, con-\ntextually relevant answers to natural lan-\nguage questions.\nBERT, GPT-3, T5\nTable 6.1: Overview of tasks such as text summarisation, code generation,", " or structured data inputs.\nCodex, GPT-3, CodeBERT\nClassification\nCategorising text into predefined labels such\nas Sentiment Analysis, Topic Classification,\nand Entity Classification.\nBERT, RoBERTa, GPT-4\nQ&A\nUnderstanding and generating accurate, con-\ntextually relevant answers to natural lan-\nguage questions.\nBERT, GPT-3, T5\nTable 6.1: Overview of tasks such as text summarisation, code generation, classification, and Q&A, along\nwith their key LLMs and descriptions.\n6.2.2\nDomain-Specific Fine-Tuning\nDomain-specific fine-tuning focuses on tailoring the model to comprehend and produce text relevant to\na specific domain or industry. By fine-tuning the model on a dataset derived from the target domain,\nit enhances the models contextual understanding and expertise in domain-specific tasks. Below are\nexamples of domain-specific LLMs.\n35\nMedical Domain\nModel Description: Med-PaLM 2 is trained on meticulously curated medical datasets and is capable\nof accurately answering medical questions, achieving performance comparable to that of medical profes-\nsionals [55].\nBase Model: PaLM 2\nFine-tuned Model Parameters: Not Known\nFine-Tuning Techniques Used: Instruction fine-tuning\nDatasets Used:\n MedQA\n MedMCQA\n LiveQA\n MedicationQA\n HealthSearchQA\nResults: Med-PaLM 2 outperformed GPT-4 in several key medical benchmarks, demonstrating superior\nperformance in handling complex medical knowledge and reasoning tasks.\nFinance Domain\nModel Description: FinGPT, an open-source LLM tailored for the financial sector, enhances financial\nresearch and cooperation by promoting data accessibility and handling finance-specific issues like data\nacquisition and quality [56].\nBase Model: LlaMA, ChatGLM, and other Transformer Models\nFine-tuned Model Parameters: Not Known\nFine-Tuning Techniques Used: LoRA, Reinforcement Learning on Stock Prices (RLSP)\nDatasets Used:\n Financial News (Reuters, CNBC, Yahoo Finance)\n Social Media (Twitter, Facebook, Reddit, Weibo)\n Regulatory Filings (e.g., SEC filings)\n Trends (Seeking Alpha, Google Trends)\n Academic Datasets\nResults: Not Applicable\nLegal Domain\nModel Description: LAWGPT, the first open-source model specifically designed for Chinese legal\napplications, demonstrates superior capability in handling Chinese legal tasks [57].\nBase Model: Chinese Alpaca Plus 7B base model\nFine-tuned Model Parameters: Not Known\nFine-Tuning Techniques Used: LoRA with Alpaca template\nDatasets Used:\n Open-source dataset: 200,000 examples containing crime type prediction and crime consultation\ntasks.\n JEC-QA dataset: 20,000 examples containing legal question answering tasks.\n Constructed legal dataset: 80,000 examples, refined from open-source and JEC-QA datasets using\nChatGPT.\nResults: LAWGPT demonstrates notable performance improvements over the LLaMA 7B model in\nvarious legal tasks, but still trails behind proprietary models like GPT-3.5 Turbo and GPT-4.\n36\nPharmaceutical Domain\nModel Description: PharmaGPT, a suite of domain-specific large language models tailored to the\nbiopharmaceutical and chemical industries, sets a new benchmark for precision in these fields [58].\nBase Model: LlaMA series\nFine-tuned Model Parameters: 13B and 70B\nFine-Tuning Techniques Used: Instruction fine-tuning and RLHF\nDatasets Used:\n Specific-domain data from academic papers and clinical reports\n Text data from NLP dataset formats (e.g., question answering, summarisation, dialogue)\n Instruction fine-tuning dataset for multitask learning\n RLHF dataset with human preference expert-annotated instructions\nResults: PharmaGPT models demonstrated impressive performance on various pharmaceutical bench-\nmarks, consistently outperforming GPT-3.5 Turbo.\nFinance Domain\nModel Description: Palmyra-Fin-70B-32K, developed by Writer, is a leading large language model\nspecifically designed for the financial sector. [59]\nBase Model: LlaMA\nFine-tuned Model Parameters: 70B\nFine-Tuning Techniques Used: Not Known\nDatasets Used: Not Known\nResults: Palmyra-Fin-70B-32K exhibits state-of-the-art performance, achieving leading results across\nvarious financial datasets and excelling in financial document analysis, market trend prediction, and risk\nassessment.\n6.3\nParameter-Efficient Fine-Tuning (PEFT) Techniques\nParameter Efficient Fine Tuning (PEFT) is an impactful NLP technique that adeptly adapts pre-trained\nlanguage models to various applications with remarkable efficiency. PEFT methods fine-tune only a\nsmall subset of (additional) model parameters while keeping most of the pre-trained LLM parameters\nfrozen, thereby significantly reducing computational and storage costs. This approach mitigates the issue\nof catastrophic forgetting, a phenomenon where neural networks lose previously acquired knowledge and\nexperience a significant performance decline on previously learned tasks when trained on new datasets.\nPEFT methods have demonstrated superior performance compared to full fine-tuning, particularly in\nlow-data scenarios, and exhibit better generalisation to out-of-domain contexts. This technique is appli-\ncable to various modalities, such as financial sentiment classification and machine translation of medical\nterminologies. A taxonomy of PEFT-based fine-tuning approaches is provided in Figure6.1. We will\nfurther discuss a few key PEFT-based approaches in the following sections.\n6.3.1\nAdapters\nAdapter-based methods introduce additional trainable parameters after the attention and fully connected\nlayers of a", " and exhibit better generalisation to out-of-domain contexts. This technique is appli-\ncable to various modalities, such as financial sentiment classification and machine translation of medical\nterminologies. A taxonomy of PEFT-based fine-tuning approaches is provided in Figure6.1. We will\nfurther discuss a few key PEFT-based approaches in the following sections.\n6.3.1\nAdapters\nAdapter-based methods introduce additional trainable parameters after the attention and fully connected\nlayers of a frozen pre-trained model, aiming to reduce memory usage and accelerate training. The specific\napproach varies depending on the adapter; it might involve adding an extra layer or representing the\nweight updates delta (W) as a low-rank decomposition of the weight matrix. Regardless of the method,\nadapters are generally small yet achieve performance comparable to fully fine-tuned models, allowing for\nthe training of larger models with fewer resources.\nHuggingFace supports adapter configurations through the PEFT library. During fine-tuning, new adapters\nare integrated into the model using LoraConfig 1. HuggingFace uses PeftConfig to load existing pre-\ntrained models and apply PEFT techniques. Additionally, HuggingFace provides built-in support to\n1https://huggingface.co/docs/peft/en/package_reference/lora\n37\nFigure 6.1: Comprehensive Taxonomy of Parameter-Efficient Fine-Tuning (PEFT) Methods for Large\nLanguage Models (LLMs). This figure categorises various PEFT techniques, highlighting their distinct\napproaches, from additive and selective fine-tuning to reparameterised and hybrid methods. It details\nspecific strategies within each category, such as Adapter-Based Fine-Tuning, Soft Prompt-Based Fine-\nTuning, and their respective sub-techniques like LoRA and its derivatives, showcasing the diverse and\nevolving landscape of LLM fine-tuning. (adapted from [60])\nrun the fine-tuning process across any distributed configuration using Accelerate2, making large-scale\ntraining and inference simple, efficient, and adaptable.\n6.3.2\nLow-Rank Adaptation (LoRA)\nLow-Rank Adaptation (LoRA)[62] is a technique designed for fine-tuning large language models, which\nmodifies the fine-tuning process by freezing the original model weights and applying changes to a separate\nset of weights, added to the original parameters. LoRA transforms the model parameters into a lower-\nrank dimension, reducing the number of trainable parameters, speeding up the process, and lowering\ncosts. This method is particularly useful in scenarios where multiple clients require fine-tuned models\nfor different applications, allowing for the creation of specific weights for each use case without the\nneed for separate models. By employing low-rank approximation methods, LoRA effectively reduces\ncomputational and resource requirements while preserving the pre-trained models adaptability to specific\ntasks or domains.\nBenefits of Using LoRA\n1. Parameter Efficiency: LoRA significantly reduces the number of parameters that need to be\ntrained by focusing only on the low-rank matrices, resulting in lower memory and storage require-\nments compared to full fine-tuning.\n2. Efficient Storage: The storage of the trained model is more efficient as it only requires storing\nthe low-rank matrices instead of the full model weights.\n2https://huggingface.co/docs/accelerate/en/index\n38\nFigure 6.2: Schematic representation of the Adapter Architecture used in LLMs. The diagram showcases\nthe integration of adapters within the Transformer architecture, including the feed-forward up and down\nlayers and their role in enabling efficient model adaptation by inserting additional parameters while\nmaintaining the models core structure (adapted from [61])\n3. Reduced Computational Load: Training with low-rank matrices requires fewer computational\nresources, making it faster and more scalable.\n4. Lower Memory Footprint: Since fewer parameters are being updated, the memory footprint\nduring training is reduced, enabling the use of larger batch sizes or more complex models within\nthe same hardware constraints.\n5. Flexibility: LoRA can be easily integrated with existing pre-trained models without extensive\nmodifications to the model architecture.\n6. Compatibility: It can be used alongside other fine-tuning techniques, such as adapter layers or\nprompt-tuning, to further enhance performance.\n7. Comparable Results: Despite the reduction in the number of trainable parameters, LoRA has\nbeen shown to achieve performance comparable to full fine-tuning in many tasks.\n8. Task-Specific Adaptation: It effectively adapts the pre-trained model to specific tasks, leverag-\ning the knowledge already embedded in the original model.\n9. Avoiding Overfitting: By focusing on low-rank updates, LoRA can help in mitigating overfitting,\nespecially when dealing with smaller task-specific datasets.\nLimitations\nWhile LoRA demonstrates considerable power, it also presents challenges:\n Fine-tuning Scope: LoRA may face difficulties when applied to tasks demanding substantial\nalterations to the pre-trained models internal representations.\n Hyperparameter Optimisation: Tuning the rank parameter r requires meticulous adjustment\nfor optimal performance.\n Ongoing Research: Despite its promise, LoRA is still in active research stages, and its long-term\nimplications remain to be fully explored.\n39\nFigure 6.3: A comparison between weight updates in regular fine-tuning and LoRA fine-tuning.\nIn\nregular fine-tuning, the entire weight update matrix (W) is applied to the pre-trained weights. In\ncontrast, LoRA fine-tuning introduces two low-rank matrices (A and B) that approximate the weight\nupdate matrix (W), significantly reducing the number of trainable parameters by leveraging the inner\ndimension (r), which is a hyperparameter.\nThis method is more efficient in terms of memory and\ncomputation,", "-tuning and LoRA fine-tuning.\nIn\nregular fine-tuning, the entire weight update matrix (W) is applied to the pre-trained weights. In\ncontrast, LoRA fine-tuning introduces two low-rank matrices (A and B) that approximate the weight\nupdate matrix (W), significantly reducing the number of trainable parameters by leveraging the inner\ndimension (r), which is a hyperparameter.\nThis method is more efficient in terms of memory and\ncomputation, making it ideal for fine-tuning large models. (adapted from [63])\nDespite these challenges, LoRA stands as a pioneering technique with vast potential to democratise access\nto the capabilities of LLMs. Continued research and development offer the prospect of overcoming current\nlimitations and unlocking even greater efficiency and adaptability.\nTutorial for Fine-Tuning LLM Using LoRA\nAn open-source template for fine-tuning LLMs using the LoRA method with the Hugging Face library\ncan be found here. This template is designed specifically for adapting LLMs for instruction fine-tuning\nprocesses.\n6.3.3\nQLoRA\nQLoRA[64] is an extended version of LoRA designed for greater memory efficiency in large language mod-\nels (LLMs) by quantising weight parameters to 4-bit precision. Typically, LLM parameters are stored\nin a 32-bit format, but QLoRA compresses them to 4-bit, significantly reducing the memory footprint.\nThis allows fine-tuning on less powerful hardware, including consumer GPUs. QLoRA also quantises the\nweights of the LoRA adapters from 8-bit to 4-bit, further decreasing memory and storage requirements\n(see Figure 6.4). Despite the reduction in bit precision, QLoRA maintains performance levels comparable\nto traditional 16-bit fine-tuning.\nIt achieves this by backpropagating gradients through a frozen, 4-bit quantised pre-trained language\nmodel into Low-Rank Adapters, making the fine-tuning process efficient while preserving model effective-\nness. The QLoRA configuration is supported by HuggingFace via the PEFT library, utilising LoraConfig\nand BitsAndBytesConfig for quantisation. Innovations such as an optimal 4-bit data type, double quan-\ntisation of constants, and memory spike management enable QLoRA to reduce memory usage from 96\nbits per parameter in traditional fine-tuning to 5.2 bits per parameter, an 18-fold reduction.\nPerformance-wise, QLoRA outperforms naive 4-bit quantisation and matches 16-bit quantised models\non benchmarks. Additionally, QLoRA enabled the fine-tuning of a high-quality 4-bit chatbot using a\nsingle GPU in 24 hours, achieving quality comparable to ChatGPT.\nThis tutorial explains the end-to-end steps of fine-tuning QLoRA on a custom dataset for the Phi-2\nmodel.\n40\nFigure 6.4: Quantised Low-Rank Adaptation (QLoRA) Optimisation Workflow. This figure illustrates\nthe QLoRA optimisation process, showing how the optimisation states, adapters, and the model interact\nduring fine-tuning. It demonstrates the use of different bit-widths (32-bit, 16-bit, and 4-bit) to optimise\nthe memory and computational efficiency during the fine-tuning of large language models (adapted from\n[65]).\n6.3.4\nWeight-Decomposed Low-Rank Adaptation (DoRA)\nIn the context of optimising model fine-tuning, the pattern analysis of LoRA and Full Fine-Tuning\n(FT) reveals significant differences in learning behaviours and updates. LoRA, employing a strategy of\nincrementally updating pre-trained weights using the product of two low-rank matrices, maintains the\noriginal weights largely static during the fine-tuning process, which allows for efficient inference. Despite\nits computational efficiency, previous studies have suggested that LoRAs limited number of trainable\nparameters might contribute to its performance discrepancies when compared to FT.\nWeight-Decomposed Low-Rank Adaptation (DoRA) [66] is a novel fine-tuning methodology designed to\noptimise pre-trained models by decomposing their weights into magnitude and directional components.\nThis approach leverages the efficiency of Low-Rank Adaptation (LoRA) for directional updates, facili-\ntating substantial parameter updates without altering the entire model architecture. DoRA addresses\nthe computational challenges associated with traditional full fine-tuning (FT) by maintaining model\nsimplicity and inference efficiency, while simultaneously bridging the performance gap typically observed\nbetween LoRA and FT. Empirical and theoretical evaluations demonstrate that DoRA not only achieves\nlearning outcomes comparable to FT across diverse tasksincluding natural language processing and\nvision-language applicationsbut also consistently surpasses LoRA in performance, providing a robust\nsolution for enhancing the adaptability and efficiency of large-scale models.\nPython Library - DoRA is facilitated via the HuggingFace LoraConfig package. To incorporate DoRA\ninto the fine-tuning process, it is essential to specify the use dora = True parameter during the Lora\nconfiguration. Further information on initialisation can be found here.\nBenefits of DoRA\n1. Enhanced Learning Capacity: DoRA achieves a learning capacity closely resembling full fine-\ntuning (FT) by decomposing pre-trained weights into magnitude and directional components, al-\nlowing for more nuanced updates.\n2. Efficient Fine-Tuning: By utilising the structural advantages of Low-Rank Adaptation (LoRA)\nfor directional updates, DoRA enables efficient fine-tuning without altering the entire model archi-\ntecture.\n3. No Additional Inference Latency: Despite its improved learning capabilities, DoRA does not\nintroduce any additional inference latency over LoRA,", "uning (FT) by decomposing pre-trained weights into magnitude and directional components, al-\nlowing for more nuanced updates.\n2. Efficient Fine-Tuning: By utilising the structural advantages of Low-Rank Adaptation (LoRA)\nfor directional updates, DoRA enables efficient fine-tuning without altering the entire model archi-\ntecture.\n3. No Additional Inference Latency: Despite its improved learning capabilities, DoRA does not\nintroduce any additional inference latency over LoRA, maintaining model simplicity and efficiency.\n4. Superior Performance: Experimental results demonstrate that DoRA consistently outperforms\nLoRA across a wide range of tasks, including natural language processing (NLP), visual instruction\ntuning, and image/video-text understanding. For example, it shows significant improvements in\ncommonsense reasoning and visual instruction tuning benchmarks.\n5. Versatility Across Backbones: DoRA has been validated across various model backbones,\nincluding large language models (LLM) and vision-language models (LVLM), indicating its broad\n41\nFigure 6.5: An overview of DoRA (Decomposed Representations for Adaptation), which is a method for\nweight decomposed low-rank adaptation. The figure illustrates how pre-trained weights are decomposed\nand adapted for fine-tuning. In the left section, pre-trained weights are decomposed into a magnitude and\ndirection. The right section shows how these decomposed weights are merged with trainable parameters\nduring fine-tuning, resulting in updated weights that combine both frozen (blue) and trainable (green)\ncomponents. The process emphasises efficient adaptation by focusing on the most significant directions\nin the parameter space, facilitating effective fine-tuning while maintaining the integrity of the original\nmodel (adapted from [66]).\napplicability and robustness in different domains.\n6. Innovative Analysis: The introduction of a novel weight decomposition analysis helps uncover\nfundamental differences in the learning patterns of FT and various parameter-efficient fine-tuning\n(PEFT) methods, contributing to a deeper understanding of model fine-tuning dynamics.\nComparison between LoRA and DoRA\nLow-Rank Adaptation (LoRA) and Weight-Decomposed Low-Rank Adaptation (DoRA) are both ad-\nvanced techniques designed to improve the efficiency and effectiveness of fine-tuning large pre-trained\nmodels. While they share the common goal of reducing computational overhead, they employ different\nstrategies to achieve this (see Table6.2).\n42\nCriteria\nLoRA\n(Low-Rank\nAdapta-\ntion)\nDoRA\n(Weight-Decomposed\nLow-Rank Adaptation)\nObjective\nProvide an efficient method for\nfine-tuning pre-trained models by\nusing low-rank matrix products\nto update weights incrementally\nwithout increasing inference la-\ntency.\nImproves\nlearning\ncapacity\nby\nclosely mimicking the learning pat-\nterns of full fine-tuning, optimis-\ning magnitude and direction sep-\narately.\nApproach\nImplements a low-rank decompo-\nsition where the weight update is\nmodelled as the product of two\nlow-rank matrices (B and A), keep-\ning the original weights static.\nUses weight decomposition anal-\nysis to reparameterise the weight\nmatrix into separate magnitude\nand direction components for dis-\ntinct updates.\nModel Architecture\nKeeps the pre-trained weight ma-\ntrix (W0) unchanged and applies\nupdates using low-rank matrices\n(B and A). Matrix A is initialised\nwith a uniform Kaiming distribu-\ntion, while B is set to zero initially.\nRestructures\nthe\nweight\nmatrix\ninto\nmagnitude\nand\ndirectional\ncomponents, ensuring directional\nvectors are unit vectors for more\ndetailed adjustments.\nTable 6.2:\nA detailed comparison between LoRA (Low-Rank Adaptation) and DoRA (Weight-\nDecomposed Low-Rank Adaptation), highlighting their objectives, approaches, and the specific architec-\ntural strategies they employ for fine-tuning large language models.\nTutorial for Fine-Tuning LLM using DoRA\nThis tutorial offers an in-depth guide and detailed explanation of the steps involved in implementing\nDoRA from scratch, as well as insights into the fine-tuning process essential for optimising performance.\n6.3.5\nFine-Tuning with Multiple Adapters\nDuring fine-tuning, we have explored the method of freezing the parameters of the LLM and focusing\nsolely on fine-tuning a few million trainable parameters using LoRA. For example, fine-tuning an LLM\nfor translation involves training a translation adapter with relevant data. This approach allows us to\nfine-tune separate adapters for each specific task we want the LLM to perform. However, a key question\narises: can we consolidate multiple adapters into a unified multi-task adapter? For instance, if we have\nseparate adapters for translation and summarisation tasks, can we merge them so that the LLM can\nproficiently handle both tasks? (Illustrated via Figure6.6).\nThe PEFT library simplifies the process of merging adapters with its add weighted adapter function 3,\nwhich offers three distinct methods:\n1. Concatenation: This straightforward method concatenates the parameters of the adapters. For\ninstance, if two adapters each have a rank of 16, the resulting adapter will have a rank of 32. This\nmethod is highly efficient.\n2. Linear Combination: Although less documented, this method appears to perform a weighted\nsum of the adapters parameters.\n3. SVD: The default method employs singular value decomposition through torch.linalg.svd. While\nversatile, it is notably slower than the other methods, particularly for adapters with high ranks\n(greater than 100), which can take several hours.\nEach method allows for customising the", " the resulting adapter will have a rank of 32. This\nmethod is highly efficient.\n2. Linear Combination: Although less documented, this method appears to perform a weighted\nsum of the adapters parameters.\n3. SVD: The default method employs singular value decomposition through torch.linalg.svd. While\nversatile, it is notably slower than the other methods, particularly for adapters with high ranks\n(greater than 100), which can take several hours.\nEach method allows for customising the combination by adjusting weights. For instance, when merging\ntwo adapters, X and Y, assigning more weight to X ensures that the resulting adapter prioritises behaviour\nsimilar to X over Y.\nThis approach is particularly suited for consolidating a single LLM to handle multiple tasks rather than\ncreating separate models for each task domain. By adopting this method, there is no longer a need to\n3https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraModel.add_weighted_adapter\n43\nindividually fine-tune a model for each task. Instead, a single adapter layer can be fine-tuned for each\ntask, allowing queries to yield the desired responses efficiently.\nFigure 6.6: Overview of how multiple adapters can be used with a pre-trained LLM to fine-tune it for\nvarious specific tasks, such as summarisation, proofreading, sentiment analysis, and more. (adapted from\n[67])\nSteps for Fine-Tuning LLM with LoRA for Multiple Tasks and Adapters\n1. Adapter Creation: Create multiple adapters, each fine-tuned for specific tasks using different\nprompt formats or task-identifying tags (e.g., [translate fren], [chat]).\n2. LoRA Integration: Implement LoRA to efficiently integrate these adapters into the pre-trained\nLLM. Utilise LoRAs methods such as concatenation, linear combination, or singular value decom-\nposition (SVD) to combine adapters while minimising computational overhead and maintaining\nperformance.\n3. Task-Specific Adaptation: Fine-tune each adapter with task-specific data to enhance perfor-\nmance for individual tasks. Ensure adapters are trained with data relevant to their respective\ntasks, optimising their ability to generate accurate responses.\n4. Behaviour Adjustment: Monitor the behaviour of combined adapters to identify any undesired\ninherited behaviours from individual adapters (e.g., short response generation from a translation\n44\nadapter). Adjust the combination weights or types to modify adapter behaviour as needed, ensuring\neach adapter performs optimally for its intended task.\n5. Evaluation and Iteration: Evaluate the performance of the combined model across multiple\ntasks using validation datasets. Iterate on the fine-tuning process, making adjustments to adapter\ncombinations and training parameters based on performance metrics and user feedback.\nTherefore, for optimal performance, it is advisable to combine adapters that have been fine-tuned with\ndistinctly varied prompt formats. However, even when using adapters with different prompt formats, the\nresulting adapter may not exhibit desired behaviour. For example, a newly combined adapter designed for\nchatting may only generate short responses, inheriting this tendency from an adapter that was originally\ntrained to halt after producing a single sentence. To adjust the behaviour of the combined adapter,\none can prioritise the influence of a specific adapter during the combination process and/or modify the\nmethod of combination used.\nAn illustrative tutorial demonstrating the fine-tuning of large language models (LLMs) using multiple\nadapter layers for various tasks can be found here.\n6.4\nHalf Fine Tuning\nHalf Fine-Tuning (HFT)[68] is a technique designed to balance the retention of foundational knowledge\nwith the acquisition of new skills in large language models (LLMs). HFT involves freezing half of the\nmodels parameters during each fine-tuning round while updating the other half, allowing the model to\nretain pre-trained knowledge and enhance new task performance without altering the model architecture.\nEach repetitive transformer layer is divided into three blocks: self-attention, feed-forward, and layernorm,\nwith half of the parameters in each block updated and the other half frozen, varying with each round.\nThis strategic parameter update helps maintain knowledge parity across training rounds and enhances\nscalability in successive training sessions.\nResearch on models like LLAMA 2-7B demonstrated that HFT could significantly restore forgotten basic\nknowledge while preserving high general ability performance. This methods robustness and efficiency\nmake it applicable to various fine-tuning scenarios, including supervised fine-tuning, direct preference\noptimisation, and continual learning. Additionally, HFTs ability to maintain the model architecture\nsimplifies its implementation and ensures compatibility with existing systems, further promoting its\npractical adoption.\n6.4.1\nBenefits of using Half Fine tuning\n1. Recovery of Pre-Trained Knowledge: By rolling back half of the fine-tuned parameters to their\npre-trained state, HFT effectively recovers a portion of the original knowledge, thereby mitigating\ncatastrophic forgetting of previously acquired capabilities.\n2. Enhanced Performance: Research experiments shows that HFT maintains or even surpasses\nthe performance of full fine-tuning (FFT) on downstream tasks, demonstrating its effectiveness in\nbalancing knowledge retention with task-specific learning.\n3. Robustness: The method is robust to different selection strategies and the number of parameters\nchosen for updating, ensuring consistent performance across various configurations.\n4. Simplicity and Scalability: HFT does not alter the model architecture, which simplifies im-\nplementation and allows for scalable applications, particularly beneficial in successive fine-tuning\nscenarios.\n5. Versatility: The technique has proven effective across diverse fine-tuning scenarios, including\nsupervised fine-tuning, direct preference optimisation, and continual learning.\n45\nFigure 6.7: Schematic illustration of the Half Fine-T"], "parameters": {"model": "nomic-embed-text:v1.5"}}, "key": "embeddings_814d62671e01d33af4aeba88822735ff39717c321879fc026ba81b5dd97a596e_v2"}