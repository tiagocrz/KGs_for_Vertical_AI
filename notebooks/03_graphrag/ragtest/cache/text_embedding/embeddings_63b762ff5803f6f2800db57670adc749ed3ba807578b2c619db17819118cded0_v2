{"result": {"data": [{"embedding": [0.01448318, 0.06544305, -0.16566122, -0.037110284, 0.06675994, -0.022687972, 0.04827131, 0.0759387, 0.008976516, 0.008544837, -0.008642612, -0.0074996515, 0.08484771, 0.07165413, 0.010458483, 0.033901967, -0.030410776, -0.040306702, -0.023110608, -0.024442455, -0.030529013, 0.0067029097, -0.009421369, -0.010558326, 0.0160639, 0.013087075, -0.021268325, 0.0051317113, -0.01963787, 0.042137764, 0.046495136, -0.03514551, 0.008893132, -0.028683253, -0.022848055, -0.0003492601, 0.042056695, 0.004121505, 0.001472221, 0.039416216, 0.041889593, -0.002329751, 0.027443508, -0.062957615, 0.030903688, 0.06785359, 0.06555445, -0.07698114, -0.0100703165, -0.06350649, 0.056878567, -0.023691539, 0.053699378, 0.0023409587, 0.05704708, 0.03638577, -0.0166357, 0.009099638, 0.05084154, -0.06787122, 0.085687414, 0.049613807, -0.047777794, 0.061769515, -0.015264176, -0.007129757, -0.080113575, 0.10500428, 0.027810762, -0.009709811, 0.022534473, -0.009216152, -0.018728686, 0.017918026, -0.005619948, 0.018669244, -0.03445566, -0.020770213, -0.012047784, 0.08201735, 0.010803558, -0.048700977, 0.053912446, -0.032883357, 0.015463371, 0.007659331, -0.043307662, -0.009067335, -0.02682498, 0.080292866, -0.008805609, -0.0025680456, 0.015132158, 0.026510878, -0.021876527, 0.08280614, 0.0020886366, 0.015204256, -0.04919741, 0.020407962, -0.039662283, -0.045661334, -0.010029463, -0.043949798, 0.012588295, -0.0010120568, -0.043095145, 0.0008023219, 0.003090216, 0.0517942, -0.025592882, -0.03704518, -0.0142465765, 0.017284987, -0.05620791, -0.03853512, 0.051079962, -0.027633168, 0.0012465758, -0.020327503, -0.018525163, 0.0023840314, -0.0038070893, 0.004611357, 0.04001591, 0.00672635, -0.059523776, 0.012021817, 0.006572867, -0.0660609, -0.04332273, -0.029349515, -0.020442493, -0.0005662732, -0.025195014, 0.06973557, -0.029262675, 0.011131132, 0.05431778, -0.0051729553, 0.032846026, 0.021995222, 0.013893579, -0.015831651, 0.0066835736, -0.06390067, 0.0045040986, -0.011943904, -0.027528621, 0.03476349, 0.003706917, 0.036111046, -0.01212041, 0.003211976, 0.06963181, -0.02811381, -0.0014910768, 0.02148895, 0.039369717, 0.024210628, 0.050103024, -0.013033647, -0.031193493, 0.03319799, -0.012247303, -0.022460068, 0.036314722, 0.03796469, 0.028984504, 0.020645535, -0.07716851, -0.032363374, -0.011089296, 0.030779598, 0.019845178, 0.022989668, 0.05754573, -0.05836108, 0.017420735, -0.015217328, 0.058041535, -0.027535602, 0.04557836, 0.07543289, 0.018818727, -0.008601968, -0.0066615255, 0.023170957, 0.02489528, -0.015786922, 0.02183834, -0.028937316, -0.08593132, -0.02569405, -0.029904202, -0.046002902, 0.068280466, -0.018057765, -0.010177655, -0.0058579203, -0.03948311, -0.024784964, -0.037894703, 0.046440963, -0.027269274, 0.04631557, 0.053798582, 0.0040156743, -0.003639739, 0.00069907855, 0.066166766, 0.04992079, 0.04222221, 0.012794191, 0.0062832083, 0.00566678, 0.020089751, -0.023559816, 0.0009625665, 0.0072713685, 0.031914733, 0.020598339, -0.019422116, -0.00027841007, -0.009244155, -0.014512354, -0.031077908, -0.011147876, -0.036576826, 0.015659038, -0.009551866, -0.072700076, 0.033501055, 0.009246358, 0.0031148205, 0.054893218, 0.013030776, 0.0062522376, -0.0026697195, 0.053964794, -0.01584399, 0.05827486, -0.023070449, -0.063270174, -0.059282422, -0.022925468, -0.004259439, -0.03184372, -0.042939033, 0.011595704, -0.041895933, -0.0005609455, 0.019831134, -0.02306245, 0.0017988, -0.027284717, -0.017008351, -0.010637697, 0.07700972, 0.0148717025, 0.07829839, 0.011308197, 0.02574294, 0.006565164, -0.01612791, -0.019839041, -0.005417935, -0.015494095, 0.0011328186, -0.02139276, -0.00976839, 0.02869289, 0.014392193, 0.03175601, -0.00917828, 0.027763864, -0.00622176, 0.017234728, 0.016935037, 0.054805778, -0.032285865, -0.04443227, -0.045571566, -0.040017284, -0.0060350173, 0.017753644, 0.029976094, -0.0051943185, 0.026774654, -0.01261651, 0.034209944, 0.020600354, -0.0068592485, 0.0181122, -0.026623262, 0.025132649, 0.03509919, 0.0014433444, -0.02688059, -0.0037878086, 0.03632938, 0.019648634, 0.07382577, 0.05486055, -0.00027668884, 0.0018716969, -0.02117608, -0.050200064, 0.008536946, -0.0008068226, -0.028811974, -0.010038554, -0.05412372, 0.009154263, -0.0015544245, 0.027874408, 0.036708713, 0.039765052, 0.019391373, -0.017954307, 0.038533702, 0.014235814, -0.03215696, -0.06512113, 0.021178873, 0.04035848, -0.011532955, 0.02167555, -0.04565465, -0.0031183832, 0.10480546, 0.058620732, 0.0272412, -0.03808324, -0.053052004, 0.012997863, 0.042622495, -0.008580163, 0.0112902215, -0.021244608, 0.034126844, -0.06091713, 0.0029803317, -0.060597472, 0.010523255, 0.025262933, -0.022528553, -0.01536573, 0.029967586, -0.012109062, -0.03909407, 0.015536991, -0.058036137, 0.0028485565, 0.0033832446, -0.010753451, 0.03312701, 0.0011144114, 0.0051029883, -0.0066005844, -0.006835047, 0.0017769007, -0.046234567, -0.043640245, -0.022834882, -0.011289327, 0.0426159, 0.020370478, 0.02734681, -0.020081125, 0.008094474, -0.0048563634, -0.028470961, 0.02477637, -0.03516629, 0.027212385, -0.050844796, -0.033698052, -0.00013416182, -0.010595102, -0.032847155, -0.031018522, -0.013650646, -0.0072067087, 0.03502511, 0.026360337, 0.011283761, -0.051664542, 0.008504019, 0.03252656, 0.025327707, -0.053915694, -0.0618301, 0.0059209256, -0.0026222258, -0.013020747, 0.009973159, -0.011271508, -0.012951102, 0.028689416, -0.05138044, -0.058948107, 0.0048185014, -0.02186366, -0.007094778, 0.026823932, -0.0431739, 0.016291637, 0.076518014, 0.008715725, 0.051495574, 0.025595216, -0.00088235067, -0.03781036, -0.011865675, -0.008023427, 0.077046424, 0.02386413, -0.024153907, -0.024914278, 0.024157532, 0.0307119, -0.02439484, -0.015271196, -0.020333502, 0.040784996, 0.013032468, 0.0042120414, 0.005180799, -0.035378437, 0.076214954, -0.0054254243, 0.049492955, -0.02514509, 0.0057029603, 0.012411283, 0.027134785, -0.016658667, 0.019817654, 0.06561275, -0.004167284, -0.05763982, -0.024975246, -0.048307296, -0.021000694, 0.060937297, 0.023410222, -0.0132342195, -0.05072603, 0.024896177, -0.000973893, -0.014901988, 0.004616885, 0.020023638, 0.10187621, -0.011870579, -0.0145020345, 0.008203477, 0.035483547, 0.04807599, 0.060853194, 0.025184961, -0.1151368, 0.052226514, 0.018037409, 0.030081786, -0.005756475, -0.04556695, 0.02355748, 0.046883035, -0.0126795005, -0.010809795, 0.03266637, -0.015477132, -0.05527288, 0.010286609, -0.0022444879, -0.02419035, 0.032895356, 0.062295057, -0.016735092, -0.0063978056, -0.019648928, -0.02361863, 0.020641228, 0.037570994, 0.009289334, -0.008243722, -0.005488656, -0.026910404, 0.06263672, 0.03510056, 0.0049819243, -0.05321451, -0.0118286, -0.075527124, -0.020123689, 0.0551371, 0.011280597, -0.014163619, 0.07583387, 0.02862026, -0.03687556, -0.020141056, -0.021986002, -0.034601495, 0.058019124, -0.07083648, -0.029854447, 0.045536432, -0.0034228002, 0.015781969, 0.034820013, 0.010563907, 0.023913672, -0.046715304, 0.01925132, 0.013745808, 0.003920249, -0.0013906253, 0.002774732, 0.0370477, -0.029175036, -0.03295326, -0.13392779, -0.015131567, 0.009027617, -0.049193144, 0.015272184, -0.004340897, 0.03286132, 0.015421458, -0.054238833, -0.025582941, 0.0012587543, 0.0053462107, -0.011037998, -0.0050817057, 0.022939933, 0.028638, 0.03461487, 0.016428199, 0.043371607, 0.011734734, 0.027923547, -0.01004943, 0.004025015, 0.032936614, 0.049538266, -0.070410475, -0.0025541633, -0.07296758, 0.018209027, -0.021301333, 0.032546315, 0.013007393, -0.013346769, -0.057385, 0.009734916, 0.012468659, -0.016568342, -0.044813354, 0.04518055, 0.022367364, -0.042479552, 0.005409943, -0.028138122, 0.009035026, 0.016406864, 0.018113164, 0.028190289, -0.09753005, -0.013715875, -0.0021113802, 0.046808675, -0.0026900182, 0.03168461, -0.021811323, -0.04484, -0.060622636, -0.0026118378, -0.03168451, 0.058008473, -0.044135932, 0.01325656, -0.007508948, -0.043463707, -0.019435516, -0.019573623, -0.051218774, -0.005381723, -0.0016505102, 0.015804594, -0.04347178, 7.669331e-06, -0.041098002, -0.007038904, -0.06555865, -0.056787264, -0.02470475, -0.030004626, -0.032018963, 0.027776746, -0.019025885, -0.0530207, 0.11425568, -0.02629774, 0.037774872, -0.020067314, -0.021756733, 0.022688033, -0.04464679, -0.016639005, 0.018266814, 0.0013968062, -0.07832806, 0.056048643, 0.038404282, -0.001489127, 0.00050041196, -0.036858186, -0.06691236, 0.032221656, -0.031073917, 0.059856154, -0.014530547, -0.010817879, -0.057817087, 0.058832683, 0.039709553, -0.01565497, -0.0035573123, -0.016220324, 0.017065836, -0.03312364, 0.034087744, -0.04245653, 0.0068679955, -0.029856052, 0.0135088395, -0.00016524145, -0.0023549718, -0.0061035017, 0.03870112, 0.048288792, -0.04681298, 0.052928917, 0.070076816, 0.042240676, -0.051237416, 0.0718933, 0.06324954, 0.025392564, -0.009983759, -0.057752024, -0.012358818, 0.029483503, -0.04857484, -0.020389251, -0.032395046, -0.00025616522, -0.0124232145, -0.034087524, -0.032274798, 0.005697136, -0.016702352, -0.028201494, -0.032234874, -0.03385344, -0.01734134, 0.02834187, -0.0047740955, -0.062521145, -0.040142577, -0.003861577, -0.0032837803, 0.036880575, 0.016446868, -0.008219517, -0.004239921, -0.0044991793, -0.017694766, 0.018223869, -0.061208043, -0.004701721, -0.0039161104, 0.018706877, 0.030338021, -0.0023663829, -0.045081217, -0.012314189, -0.065260306, 0.018632112, 0.0002558246, -0.01906463, 0.07699736, 0.032816533, -0.040247485, 0.008661915, 0.020885408, -0.025143314, 0.04247462, 0.040679887, 0.060818687, -0.041647844, -0.008606584, 0.009212775, 0.011658155, 0.002689429, 0.0006300755, -0.029347174, 0.012565384, -0.05542476, 0.05209034, 0.014286328, 0.005856182, -0.044057526, -0.009624148, 0.0037561161, 0.02944752, 0.03097845, 0.0011690313, -0.07079241, -0.0009859637, 0.022152763, 0.011385201, 0.03628296, -0.011311758, -0.022961142, 0.026183864, 0.0032815933, -0.012045014, -0.045397587, 0.014925916, 0.024265401, -0.023600789, -0.104077294, -0.039546214, -0.06327332, -0.008366551, -0.020418702, -0.006671101, 0.0061117033, -0.041233767, 0.023477748, -0.0010114758, -0.0028354414, 0.030942801, 0.027175171, -0.0519206, -0.006493601, -0.046600502, -0.03317274, 0.0020510792, -0.037270278, 0.058491755, 0.013716374, 0.015316641, 0.1055008, 0.041465588, 0.016428828, -0.05594445, 0.051859908, 0.021081688, -0.009017201, 0.0029924042, 0.014760468, -0.028642325], "index": 0, "object": "embedding"}, {"embedding": [-0.02735385, 0.027121378, -0.16959304, -0.009254718, 0.04229991, -0.034475457, 0.056642566, -0.013007106, 0.018764801, 0.054936204, -0.0020677324, 0.025614968, 0.0948792, 0.093211725, 0.023316795, -0.023252714, -0.030810999, -0.023318218, 0.038378593, 0.0070670554, -0.0034919407, -0.022669617, -0.004447466, -0.00092355977, 0.042449158, 0.05653599, -0.07004716, -0.022971082, -0.03285373, -0.008165469, 0.039522268, -0.055469435, 0.052637644, -0.06225981, -0.07454321, -0.0025349078, 0.012728778, 0.05100331, 0.045493726, 0.029179405, 0.02300518, 0.018827528, -0.0420919, 0.008318936, 0.009539559, 0.019850047, 0.0009189226, -0.016561272, 0.039151296, -0.06666377, 0.03969336, 0.053162824, 0.017435128, 0.020721538, 0.036801305, 0.034944486, -0.03825119, -0.0337133, 0.08901279, 0.007819639, -0.011841163, 0.028696565, -0.08231764, 0.058763668, 0.044726845, 0.04401297, -0.0233211, -0.004814009, -0.0022531978, -0.04764447, 0.02841229, -0.029042248, 0.0107778525, -0.020520667, -0.03029301, 0.02011633, -0.0027988076, 0.030063014, -0.03145236, 0.03267295, 0.033695202, 0.048965063, 0.083714776, 0.016203843, 0.026111498, 0.005681143, 0.009854699, 0.03040305, -0.05773487, 0.090177245, -0.010420758, 0.004827409, 0.028109953, -0.0037244044, -0.094752066, 0.018590583, -0.03145002, 0.018251693, -0.06862158, -0.008186946, -0.0074403803, 0.006365675, -0.0039934283, -0.042233657, 0.03034367, 0.08040417, 0.0060565723, 0.014569892, -0.005250285, 0.01239764, -0.04056005, -0.000652197, -0.02242391, 0.017206306, -0.034664087, -0.042821616, 0.0064031184, 0.0049040583, -0.009545678, 0.004757369, 0.0122991605, -0.0052999374, -0.035109688, -0.018167097, 0.00076401385, 0.040667012, -0.0514081, 0.023150634, 0.027708946, -0.0029527678, -0.013950285, -0.03343603, -0.030208472, 0.027773105, -0.033961132, 0.063880466, -0.07005418, -0.03512435, 0.024925927, 0.0068469206, 0.022528138, -0.010044814, -0.0171245, -0.022865528, -0.009945207, -0.033102244, 0.037962634, 0.0017458522, -0.029016985, 0.026954647, 0.021052344, 0.07511877, -0.015117384, 0.054014593, 0.027286476, -0.046617158, -0.053311236, 0.023991212, 0.0057446347, 0.011707026, 0.046158556, -0.041770212, -0.0023732048, 0.0036031948, -0.05031737, -0.02731807, 0.016601034, 0.04223315, -0.0013464254, 0.026177712, -0.003186338, -0.055749606, -0.012296116, -0.023829216, 0.032278474, -0.035532366, 0.07638375, -0.033790857, 0.0010228134, 0.011639194, 0.08565121, -0.022537883, 0.041216794, -0.028504815, 0.0021043199, -0.023782173, 0.016087756, 0.013443617, -0.06063865, -0.017782442, 0.005088227, 0.014622985, -0.06566543, 0.0045687677, 0.0053934334, 0.028086895, 0.0809317, -0.04393271, 0.0048355516, -0.070321605, -0.032350317, -0.01522734, -0.024600701, 0.029011672, -0.03602926, 0.07669999, -0.004510522, 0.032861967, 0.009433318, -0.045003388, 0.02538364, -0.01663852, 0.017773272, -0.0007882766, -0.015373461, -0.003610102, 0.014315365, -0.01181825, -0.025429897, -0.006831174, 0.020597372, -0.0015327822, -0.016935105, -0.013873435, 0.011615192, -0.0338352, -0.0018276777, -0.014215377, -0.04190088, 0.022898644, 0.017026473, -0.01939938, 0.021438729, -0.0043839794, -0.019648917, 0.004490963, 0.066201136, 0.051012784, -0.009857097, 0.050915718, 0.012095729, -0.010391282, -0.024221823, -0.01905852, -0.10230893, -0.018240832, 0.008945361, 0.0049970895, 0.05131355, 0.065896474, -0.010093267, 0.012539833, 0.03691115, 0.0053702784, 0.04339053, -0.067630924, -0.03952622, 0.0040596663, 0.044006515, 0.043753736, 0.023050357, -0.039159283, 0.030194404, -0.0018401545, -0.03992284, -0.05229078, 0.026444932, -0.045263126, -0.03603749, -0.009281586, -0.03090212, 0.037326694, -0.019688318, 0.020963, 0.009334998, 0.026868084, 0.031703766, -0.008581628, -0.011514, -0.013529152, -0.043750234, -0.01081788, -0.041151404, -0.0041198777, 0.00068787154, -0.010038142, 0.031406194, -0.026018383, 0.0024766668, 0.037047353, 0.02015799, -0.0076068384, -0.0025693986, 0.04181119, -0.026209265, 0.031354666, 0.07021601, -0.031812366, 0.02216039, 0.0030027332, -0.027643215, 0.039628386, 0.04781991, 0.009917099, 0.057433106, -0.005252269, -0.00027435616, -0.017512131, 0.006562404, -0.02675647, -0.0011558853, 0.016515484, -0.028697269, -0.023155395, -0.036576066, -0.0046693804, 0.008377604, 0.006928919, 0.03339387, -0.03493181, -0.00952869, 0.004778633, 0.024390329, -0.022912586, -0.01987723, 0.05668203, -0.017674869, 0.065982744, -0.0512255, 0.015879115, 0.094594, -0.014587667, 0.04014914, -0.02711769, -0.079568364, -0.009640622, 0.016803253, -0.04146773, 0.004136043, 0.0497289, 0.0119360695, -0.0010451161, 0.03356077, -0.019763468, -0.036226783, -0.027939131, -0.039314095, 0.009547583, 0.027983842, 0.008011671, -0.016397338, 0.0025392997, 0.0026782479, -0.0026514402, 0.04630924, -0.027784944, 0.059309322, 0.005370562, 0.022962337, 0.042069156, 0.07481231, -0.012730835, -0.0026230821, -0.024051538, 0.0035331224, 0.05501529, 0.027341101, 0.022616003, 0.01258481, -0.010233229, -0.001650259, 5.8809976e-05, -0.025225686, 0.03967926, -0.02805683, -0.008241092, -0.0557347, -0.03842058, -0.039100718, 0.017041033, 0.041411426, 0.013902528, -0.043787576, -0.00356997, 0.03880053, 0.0060370234, -0.034790672, -0.009766717, 0.022584178, -0.020950329, -0.010352417, 0.00943774, -0.016617224, 0.038949683, 0.04124633, -0.035163548, 0.0027977675, 0.009344601, -0.017097574, 0.0060005193, -0.006713744, -0.0486432, 0.028737461, -0.0043194853, 0.02193278, -0.015683651, 0.017193101, -0.0042259125, 0.028270118, 0.00038566845, -0.023032382, 0.024319988, -0.021361234, -0.050818738, 0.024137355, 0.029243683, 0.06616309, -0.0006826236, -0.041642178, -0.0020480927, 0.0060023265, 0.027244594, -0.018989716, -0.00767284, 0.03354563, 0.016556768, 0.053241115, 0.069368206, -0.01372016, -0.07590031, 0.013490788, 0.009330337, 0.029496456, -0.030498657, -0.04222785, 0.040783547, -0.0149169685, 0.08204185, -0.015491355, 0.032561984, 0.031190958, -0.04235426, -0.032725036, -0.022191023, 0.047461927, 0.033988755, 0.03999245, -0.025992658, -0.12919456, -0.0053422847, -0.061716557, -0.0018655289, 0.006518351, 0.031849135, 0.07064459, -0.011376587, 0.023095526, 0.01306035, 0.072452724, 0.0613579, 0.017091293, 0.05298797, -0.026858905, 0.00030847845, 0.0007654739, -0.016136572, 0.0016896544, 0.029143546, 0.023101244, 0.05933838, -0.0122001255, -0.010857461, 0.0039829863, -0.0018325517, -0.013651487, -0.02860732, -0.07665935, -0.044813883, 0.002878196, 0.020866256, -0.018368902, -0.06294166, -0.0032806054, -0.008338266, -0.017423319, 0.057807203, -0.011054793, -0.013727508, -0.010507711, -0.015671192, 0.045802843, -0.007606709, 0.009455452, -0.0038408753, -0.04135492, -0.00401626, -0.024230618, 0.055156693, 0.0123070115, -0.006414988, 0.028729552, 0.029567724, -0.031595517, 0.008134426, 0.01059378, -0.0356914, 0.012229538, -0.05823433, -0.004850111, 0.042948544, 0.029754689, -0.031496547, -0.0010386374, 0.042581957, 0.053958554, -0.036792215, 0.031173574, -0.002229952, -0.06574388, -0.010222986, 0.05418733, -0.012616233, -0.016854065, -0.028673006, -0.08556545, 0.013077998, -0.014241015, -0.092681415, 0.038338736, -0.05456721, -0.002597738, 0.0126609625, -0.03190658, 0.0030593704, 0.033314206, 0.020721085, -0.0069799884, 0.01322832, 0.060913283, 0.052723102, -0.0124237, 0.023419281, 0.0298406, -0.00021277604, 0.04880089, 0.030892152, -0.009112445, 0.062094238, 0.04654809, -0.039328545, -0.025943508, -0.05363617, -0.006592465, -0.03446361, 0.046002373, -0.023562692, -0.009004872, -0.07108802, 0.0019043091, 0.041294646, -0.035644818, 0.022622954, 0.0700223, 0.04492397, -0.018692117, -0.01330831, -0.0036929136, -0.032668747, -0.008127152, 0.035503417, 0.039237075, -0.06944675, 0.022516625, 0.01103611, -0.00026091954, 0.026614487, 0.00754486, -0.00038102744, -0.042151947, -0.01564767, 0.035111316, -0.075502835, -0.032690547, 0.0016295094, 0.012794029, -0.0017886277, -0.053112622, -0.032624632, -0.00903088, -0.020885095, -0.06190924, -0.00055206753, 0.06037815, -0.07361892, 0.012877584, -0.025995072, -0.023041356, 0.0027391643, -0.04069804, -0.066812515, 0.01035649, -0.0009890819, 0.07458639, -0.03775439, 0.06375059, 0.069700025, -0.062330954, 0.03337047, 0.0027562266, -0.0028702794, -0.022980988, 0.05388353, -0.018026352, -0.01824598, 0.036068514, -0.02512331, 0.016361827, -0.005911198, -0.017080588, -0.025364073, -0.06113873, -0.06567431, 0.053886045, -0.0806311, -0.0006745117, 0.0018418089, -0.023597304, -0.021356247, -0.004334943, 0.035380166, -0.07146092, -0.0055595157, -0.026482495, 0.00010495506, -0.024577891, -0.007948438, -0.064044885, 0.007946153, 0.00042526927, 0.07849361, 0.017798431, 0.002933783, -0.033728123, 0.06886096, 0.022879431, -0.05517041, 0.0054534217, 0.07512855, 0.013642407, -0.015969625, 0.035563488, 0.05971382, -0.0032133881, -0.029764395, -0.024881423, 0.022874575, 0.014797308, -0.0153037915, -0.011264209, 0.029240835, 0.017914142, -0.01939915, -0.0064681317, 0.007594245, -0.009454707, -0.06294974, -0.006700143, 0.00034582094, -0.013862135, -0.0059138145, 0.011659894, -0.010811958, -0.019544987, 0.030446878, 0.0074392227, 0.004476819, -0.0012763558, 0.03772418, 0.004804314, 0.009747948, 0.04099582, -0.010899835, 0.04268044, -0.015189849, -0.0018688774, -0.074608855, -0.01379009, -0.027695892, -0.070869885, -0.013801017, -0.026810471, -0.023526879, 0.08389247, -0.024593556, -0.06516328, 0.117450215, -0.0028487889, -0.023394555, -0.0048749256, 0.037328348, 0.006708573, 0.02424336, -0.012319733, -0.009287974, -0.0025003336, -0.003257384, 0.013915393, -0.014735191, 0.036570463, -0.052395392, -0.01293311, 0.05316951, -0.013273576, 0.014292886, -0.051259816, -0.0022963868, -0.04461463, -0.073749445, -0.040291462, 0.01491961, 0.017202774, -0.031236699, -0.03632749, 0.01450841, 0.022131909, -0.055001557, 0.010434163, -0.0499364, 0.01825508, -0.012522676, -0.00612736, -0.032486755, -0.03665267, 0.077321365, -0.01736663, -0.015930768, 0.011223454, 0.019199671, -0.085937954, -0.007096308, -0.018895546, 0.00018208388, 0.0024587486, 0.009166484, -0.021805441, 0.03513653, -0.037024565, 0.0702766, -0.010227611, -0.017115545, 0.027999528, 0.035707116, -0.09408068, 0.0069300267, 0.032276906, 0.019175831, 0.0041308883, 0.040555764, 0.09624584, 0.023233553, -0.022614315, -0.046373315, -0.0031343938, -0.022572286, -0.026445957, -0.06570704, -0.029706355, -0.004994384], "index": 1, "object": "embedding"}, {"embedding": [0.021068022, 0.09270623, -0.14668784, -0.016555397, 0.07946177, -0.044589132, 0.03630418, 0.029741183, 0.021612993, -0.037697364, -0.029223116, -0.0014711657, 0.108584076, 0.016305292, 0.012279864, 0.03316144, 0.014916799, -0.039969094, -0.012165695, -0.0028504413, 0.015247242, -0.03763287, -0.008659903, 0.0077481945, 0.037874945, 0.008962808, -0.040150143, -0.04288211, -0.03273423, 0.015707495, 0.020877803, 0.012106733, -0.0067616254, -0.031637892, -0.05073187, -0.041734956, 0.058080174, 0.0025306374, -0.0014302043, 0.021015214, 0.04704941, 0.00044672526, 0.059530225, 0.016818931, -0.00905488, -0.06262495, 0.10057117, -0.01876965, 0.015954647, -0.07215882, 0.014778289, 0.016719094, 0.0071148835, 0.00023634595, 0.05981781, 0.017598374, 0.014233156, -0.02884333, 0.026805613, -0.07382728, 0.07786287, 0.06725326, -0.038380202, 0.05365357, 0.007621369, -0.010742622, -0.029872436, 0.038131222, -0.026192086, 0.0007909653, -0.002950659, 0.027192362, -0.021523397, 0.0054012304, -0.02891215, -0.018832702, -0.05596191, -0.02435866, -0.024862623, 0.0038622818, 0.053762224, -0.015424663, 0.03655162, -0.016432391, 0.0050202385, -0.0037726155, -0.015099348, -0.008048832, -0.0369432, 0.06763697, -0.035309434, 0.014493863, 0.043782733, 0.010752282, -0.073488936, 0.0810012, -0.04109267, 0.0025242264, -0.017603027, -0.0016716712, -0.046618156, -0.018416386, -0.009259389, 0.0005063416, 0.012687766, 0.044101756, 0.0415956, 0.02142595, 0.0017216328, 0.0039891605, -0.033134934, 0.055385664, -0.031186568, -0.0010802832, -0.04753721, -0.031233385, 0.062028732, -0.010052796, 0.012903987, 0.07478291, -0.040436663, 0.0014083611, -0.005662504, 0.008241196, -0.026945028, 0.04456378, -0.07375279, -0.044155676, -0.017356263, -0.0034164982, -0.033935834, -0.009038323, -0.06447398, 0.028839279, 0.013555879, 0.06514067, -0.028754205, 0.006414419, 0.069933765, -0.007091852, 0.015079384, 0.013840427, 0.0003645572, 0.025741046, -0.033260886, -0.023191068, 0.0043313946, 0.002357575, -0.025407216, 0.035045803, -0.04119214, -0.012744724, 0.020185653, -0.0025659446, 0.03750181, -0.058110878, 0.03408432, -0.01362671, 0.0147688845, 0.025344329, 0.070622124, -0.0021852695, -0.055711467, 0.008611966, -0.033380914, -0.054920543, 0.05003595, 0.008416865, -0.003976201, 0.024576917, -0.0074074967, -0.069006436, 0.027785834, -0.018249448, 0.06673006, -0.010195202, 0.054305047, 0.005182244, 0.04179872, -0.00020448971, 0.07679118, -0.037558388, 0.005538509, 0.026011331, 0.0035145956, 0.013642547, -0.0009850365, 0.029715957, -0.020839639, -0.036469784, 0.040705062, 0.007967452, -0.07403273, -0.010457738, -0.03835663, -0.045652103, 0.083777934, -0.05329707, 0.0061576786, -0.046696573, -0.049640767, -0.029449213, -0.01733664, 0.019680278, -0.011491006, 0.03572492, 0.003280423, 0.029862978, -0.014603433, 0.0009423247, 0.05019454, -0.014814769, 0.0401234, 0.051563874, -0.020241044, -0.007839051, -0.027750185, 0.02416639, -0.008730031, 0.0014398857, 0.008304177, -0.0022792423, -0.043799356, 0.046973895, -0.011179091, -0.016634785, -0.009315913, 0.032029744, -0.055071656, 0.025345806, -0.010126916, -0.10216392, -0.005417165, -0.024076365, 0.014719266, -0.01324069, 0.017963937, 0.051298186, -0.0222639, 0.022887722, -0.004963877, 0.029274028, -0.062773354, 0.01275305, -0.030850833, -0.007898847, 0.0055036168, -0.018943695, -0.01073827, 0.03928148, 0.019571217, 0.03757928, 0.06840741, 0.0069244476, 0.025631486, -0.06973006, -0.019960035, 0.045878105, 0.0391957, 0.008617446, 0.00815676, -0.019311346, 0.018401861, -0.034747757, -0.008117363, -0.0029767526, -0.012303974, -0.06604132, -0.05367689, -0.00030145198, -0.00027090014, 0.057362527, -0.00499149, 0.060996056, -0.005277015, 0.02116473, 0.00051385164, 0.014270728, -0.027480925, -0.0022291092, -0.00045695333, -0.08836827, -0.063787006, 0.027556252, -0.038406067, 0.004597485, 0.055319395, -0.0015026171, 0.020930815, -0.0025174352, 0.03634591, 0.0013491, 0.00026646236, -0.020012885, -0.025744304, -0.013296056, 0.065093085, -0.0075594564, 0.014880874, -0.008524923, 0.013649983, -0.0038063556, 0.037920613, 0.041520633, -0.011963779, -0.0343047, -0.022391131, -0.076600246, 0.030097276, 0.016525034, -0.0030658846, 0.004643791, -0.07975521, -0.010738452, -0.019017644, -0.00061993895, 0.011743633, 0.028103694, 0.04888096, -0.067197815, 0.0050505535, -0.031892575, -0.044931155, -0.059066504, -0.015269586, 0.031489093, -0.007686395, -0.0062121255, -0.0097795455, 0.008986547, 0.092060104, 0.05199498, 0.025094615, -0.078180976, -0.05459624, -0.015140367, 0.027591156, 0.056108993, 0.021456176, 0.03243311, 0.06309463, -0.0450113, 0.027767597, -0.08374314, -0.028432194, 0.020629209, -0.009947401, -0.043711353, 0.022319522, 0.015209475, 0.0024334872, -0.0021093397, -0.0029409258, -0.02733305, 0.03688609, -0.01291217, 0.032456394, -0.011517817, -0.013201968, 0.023594832, 0.03568182, 0.025934942, -0.025881642, -0.019164743, 0.016789328, 0.029395172, 0.039889526, 0.011001417, -0.022564229, -0.017738875, -0.023714788, -0.020547427, -0.0365738, 0.024115864, -0.048647713, -0.009996959, -0.058024794, -0.01978699, 0.01419542, -0.017657487, 0.027099917, 0.0010188408, -0.0052531213, -0.026944345, -0.014953849, 0.034304727, 0.017778678, -0.041176185, 0.020216327, 0.034417257, 0.002625762, -0.0040224874, -0.037602402, 0.026786538, 0.039613765, -0.005795461, -0.029763136, 0.0026672066, 0.01121589, 0.034093946, -0.04339474, -0.06248146, -0.01849809, -0.01635554, -0.007558535, -0.046098355, -0.036062155, 0.00156404, 0.00080717227, 0.0559891, -0.0021857133, 0.023049777, -0.02724556, -0.037742063, 0.02769131, 0.016611133, 0.056202877, -0.021045232, 0.015666574, -0.017792894, 0.045824073, 0.03631317, -0.012745074, -0.0045006596, -0.0023671451, 0.010481552, 0.0246195, 0.051854633, 0.015183862, -0.04983827, 0.021892251, -0.0015307469, 0.06953313, -0.028501295, -0.038196832, 0.0025442608, -0.008171101, -0.02608952, -0.011585702, 0.030793743, 0.028337171, -0.05595071, 0.02215101, -0.080498435, -0.00070264516, 0.04959004, 0.055410024, -0.017020723, -0.013584228, -0.012399414, 0.015171316, 0.01980457, 0.062433433, -0.0050321636, 0.07471138, -0.023624921, 0.007942396, -0.0003415879, 0.06072825, 0.06902264, 0.036254447, -0.022597894, -0.06057341, 0.018206647, 0.031983506, -0.036719818, 0.0036893717, -0.011713681, 0.059451353, 0.06784081, -0.053121306, -0.039887924, 0.045721818, -0.032382518, -0.037720688, -0.022487167, -0.019411342, -0.012690957, 0.07732115, 0.016058227, 0.016973676, 0.007825474, 0.00303408, -0.013565727, -0.020934407, 0.007463514, 0.0069625606, -0.033944085, 0.0069433353, -0.055104, -0.010765331, 0.01777622, 0.011012144, -0.038888466, -0.02919365, -0.020685948, 0.0017673385, 0.05583543, 0.033564787, -0.00033682788, 0.009943033, 0.03734904, 0.015162696, -0.015228478, 0.017611839, -0.038101587, 0.05592901, -0.050377876, -0.038209576, 0.0036948863, 0.05613754, 0.0355993, 0.010304383, 0.056338314, -0.011221431, -0.07976273, 0.025386926, -0.01038876, -0.046637163, -0.028413229, 0.031607088, -0.016898612, -0.02589729, -0.04558393, -0.06504486, 0.036193196, 0.0057631913, -0.032659374, 0.07156273, -0.036345754, 0.03822403, 0.059071664, -0.048706915, 0.0069836644, -0.03336513, 0.030356552, -0.035410035, 0.006826441, 0.030317094, -0.022197582, -0.0001758793, 0.0007436889, 0.075029, -0.021593591, 0.06107597, -0.055093504, -0.03689929, 0.036749117, -0.038104698, -0.055366643, -0.0044069416, -0.04683696, -0.0008881423, -0.04426681, 0.062603556, -0.004085961, 0.006449314, -0.05702076, 0.013784677, -0.033341784, -0.0109646255, 0.020623349, 0.059220698, 0.027093705, 0.030673277, 0.042128783, -0.0026749275, 0.018118722, -0.011764937, 0.03815561, 0.03768331, -0.07584886, 0.0018440245, 0.036013216, -0.0035082563, -0.03699222, 0.006689463, 0.015935035, -0.031384513, -0.04591338, -0.007574131, -0.030652504, 0.011068897, 0.053247154, 0.030083045, 0.015137463, -0.06737531, -0.016794309, 0.024780227, -0.028147735, -0.00698095, -0.024340995, 0.021893615, -0.089380376, -0.005343086, -0.007563869, 0.009757731, -0.038268104, -0.080252096, -0.035290137, -0.027915264, 0.02164089, 0.06428835, -0.018117804, 0.01439832, 0.07262931, 0.014631187, 0.0444692, 0.024836265, -0.0014134037, -0.025274076, -0.02214655, -0.035014093, -0.0007519559, 0.020465173, -0.0075252256, 0.08105616, 0.015284029, 0.026829923, -0.021330623, -0.037816167, -0.04134219, 0.060311172, -0.08291418, 0.010280241, -0.04040577, -0.023897454, -0.06287646, 0.023972888, 0.033474788, -0.032636303, 0.017769048, -0.07254043, 0.037434142, -0.0730824, 0.030803194, -0.040147524, 0.013578516, -0.055463623, 0.033052675, 0.036271416, -0.0038615128, -0.005089518, 0.018197186, 0.0061522387, 0.0018504473, 0.033974078, 0.069137655, 0.02561342, -0.03930196, 0.07595391, 0.070196755, 0.0041980143, -0.041338917, -0.0038632548, -0.023127569, 0.03527761, -0.0014341625, 0.03174181, -0.02162617, 0.024037223, -0.008351026, -0.032292984, 0.025221271, 0.03895199, -0.03255629, -0.018883, 0.020857897, -0.020456154, -0.026925007, 0.053954147, 0.060467176, -0.06990648, -5.061131e-05, 0.044382397, 0.03139933, 0.032185875, 0.012797741, 0.0197955, -0.0322497, 0.028202217, -0.02351184, 0.008318088, -0.0640414, 0.008970954, -0.008520702, -0.008535172, 0.007132575, -0.03563378, 0.0102039585, -0.023738697, 0.00482907, 0.03199153, 0.0010208507, -0.0019575544, 0.041917447, -0.040168297, -0.020849463, 0.03536143, 0.034928016, 0.028578946, 0.041446384, -0.0072381515, -0.0013742584, -0.019699255, -0.044984262, 0.003571432, -0.008819854, -0.013272078, -0.045771282, -1.1452637e-05, 0.01490957, -0.058330953, 0.028805276, -0.020348223, 0.033819817, -0.03069263, 0.014716559, -0.013539613, 0.05719475, 0.02054927, 0.016052317, -0.049813885, -0.018824877, 0.00045688148, -0.012614609, 0.03587161, -0.06509493, 0.027163459, 0.021235354, 0.011129363, -0.03155385, -0.06896774, 0.06690545, 0.0350503, 0.015010717, 0.013190483, -0.031329222, -0.05903297, -0.012852569, 0.0065314737, -0.028940586, -0.009471493, -0.0034510202, 0.010269765, -0.018067978, -0.023437003, 0.061954383, 0.027253918, -0.042016946, 0.0070002517, -0.020962404, -0.079626516, 0.052227583, -0.03842733, 0.026610442, 0.029725026, 0.0005856019, 0.094454005, 0.0020676332, 0.04073174, -0.010499193, 0.055156726, -0.033148043, -0.025559949, -0.028696505, 0.018913135, -0.03485513], "index": 2, "object": "embedding"}, {"embedding": [-0.0139816655, 0.06649977, -0.12119187, -0.071933836, 0.053552385, 0.005469679, 0.027773703, 0.024263673, -0.01981622, 0.001940681, -0.0012546415, -0.025027286, 0.07764841, 0.015395989, 0.018521411, 0.0026029295, 0.040293224, -0.07427882, -0.03854185, 0.026537541, -0.010060391, -0.056023426, 0.009043456, -0.04717797, 0.023721797, 0.02628813, -0.024440156, -0.0051901764, -0.00086520525, 0.03498088, 0.027112985, -0.03157996, -0.0028352353, -0.00065601745, -0.049943663, -0.0562692, 0.0087821735, 0.030736804, -0.02135246, 0.02245561, 0.08771027, 0.020306362, 0.047901582, 0.018289823, 0.04125449, -0.057784215, 0.008498442, -0.019962873, 0.02181252, -0.05974929, 0.00035638022, -0.016247083, 0.005637738, 0.008497881, 0.09825287, 0.04669303, -0.058076385, 0.002993783, 0.009394323, -0.047290206, 0.089722686, 0.07288036, -0.06302793, 0.050338347, -0.028653208, 0.014424483, -0.0501276, 0.003242199, 0.0022863704, -0.05756998, -0.033206332, 0.0051129903, -0.04766166, 0.013022415, -0.060323868, 0.017527485, -0.022782926, -0.04027237, 0.0070390836, 0.050129533, 0.0002750433, 0.0035265428, 0.036483802, -0.023137087, 0.0020197812, 0.003663783, 0.0052920366, -0.013904319, -0.04795331, 0.04985228, -0.025108725, 0.003997334, 0.039158083, 0.043816164, -0.08462668, 0.049530927, -0.038824394, -0.0049485653, -0.005078419, -0.016042791, -0.01939492, 0.035971794, -0.007888251, 0.03179713, 0.007443851, 0.054143008, 0.033072222, 0.035425693, -0.053656664, -8.9317626e-05, -0.033191245, 0.04128914, -0.0027286503, -0.023869153, -0.008680148, -0.045108415, 0.055068467, -0.032400213, 0.029287336, 0.1012328, 0.03688959, -0.052396104, -0.0044176104, -0.026367113, 0.03565003, -0.016624382, -0.088109575, -0.022794709, -0.017693553, -0.019846868, 0.0071988627, 0.014454987, -0.024844408, 0.0029963055, 0.005388684, 0.066974014, -0.024585307, 0.0024150994, 0.0582826, 0.033987742, 0.07818965, 0.0284239, 0.009289111, 0.021861728, -0.022968657, -0.050757293, -0.009641003, 0.0067544323, 0.0060307975, 0.052362148, -0.0153964255, 0.014454127, 0.048315387, 0.059945457, 0.014440241, -0.022670725, -0.013961088, -0.017573688, -0.011052932, -0.0075874077, 0.028576137, 0.029150233, -0.0432132, 0.025111223, 0.014014422, -0.043785155, 0.03763172, -0.010189582, 0.013286092, 0.024546623, -0.05468252, -0.027786324, 0.039252285, -0.018748606, 0.014259386, -0.030871933, 0.050371118, -0.039765943, 0.014337559, -0.0202558, 0.039829973, -0.09795695, 0.042896524, 0.001016699, -0.036938023, 0.013842315, 0.025070144, -0.018689899, 0.023237318, -0.03244433, 0.0055605057, 0.0061794417, -0.04035516, -0.03418615, -0.04932217, -0.037676726, 0.048588675, -0.072961554, -0.0013057566, 0.008424295, -0.042953197, -0.055950247, -0.015275567, -0.002504532, -0.06667584, 0.026359463, 0.032172803, 0.036965124, -0.01963462, -0.006911857, 0.041497067, 0.016192053, -0.014501453, 0.014709137, -0.032517962, 0.017859038, -0.04618298, 0.036219563, 0.0016932327, 0.021581922, 0.0054919254, -0.059326846, 0.01615438, 0.050051507, 0.004323082, 0.00031289662, -0.05459107, 0.015907407, -0.082446575, 0.03448799, 0.008405732, -0.048359368, 0.06565105, 0.017692722, 0.018319963, 0.03824958, 0.013006014, 0.022304393, -0.0037853685, 0.016367113, -0.026620703, -0.0034005626, -0.036259577, -0.012399831, -0.08336444, -0.014096482, 0.0038059603, 0.01873, -0.012859655, 0.09319203, -0.027512735, 0.039050184, 0.044665847, -0.0416283, 0.055169143, -0.035258494, -0.041017454, 0.0570985, 0.037417572, 0.01755376, 0.022666318, -0.045047585, 0.010102466, -0.052176803, 0.006434935, -0.012751133, 0.00036173564, -0.023817535, 0.019143108, -0.004148682, 0.059710838, 0.06548727, 0.012205958, 0.04151036, -0.014707633, 0.02705208, -0.03336906, -0.017277647, 0.014603135, 0.027577203, 0.011585143, -0.059990503, -0.015269941, -0.020439241, 0.023387508, 0.030484647, 0.0072095725, 0.00041498474, 0.017796168, -0.029340502, 0.035610918, 0.022902085, -0.03628051, -0.024595214, -0.01918753, 0.030819537, 0.0667058, -0.0180544, 0.014641459, -0.042833157, -0.005327921, 0.012245445, 0.051454566, 0.064625375, -0.037055183, 0.00028953137, -0.022266634, 0.015916593, 0.04265863, -0.006925878, -0.006982124, -0.019161083, -0.08791999, -0.005519432, -0.014860621, 0.025396563, 0.011205606, 0.03158293, 0.013829192, -0.059867617, 0.022218037, -0.05375863, -0.006377224, -0.0548951, -0.022228884, 0.05849718, 0.017706024, -0.0026560202, -0.013078299, 0.015642626, 0.027277244, 0.037496187, 0.043808166, -0.050278272, -0.06004113, 0.02332847, -0.010738311, -0.0047447393, 0.03423839, 0.024191888, 0.030729337, -0.009628989, 0.062684916, -0.027988225, -0.013044633, 0.01156132, -0.064642884, -0.06693108, 0.07604698, 0.04002219, -0.014409887, 0.0050671473, -0.049793713, -0.015608233, 0.069037706, -0.037747547, 0.049520265, 0.067876704, -0.011300701, 0.020124631, 0.031709146, 0.03913846, -0.016125124, -0.042655822, 0.008988647, 0.04727965, 0.050540876, -0.0107269, 0.008671872, -0.00040412153, 0.013457534, 0.008208464, -0.03909434, 0.028419444, -0.0129081635, 0.014675665, -0.058258872, -0.00502495, -0.027928656, 0.0060184314, 0.024373785, 0.036356546, -0.065749064, 0.024133123, -0.0178436, -0.042040087, -0.025095643, -0.051450115, 0.020931972, 0.019829702, -0.03244439, -0.04385692, -0.03324132, 0.071705066, -0.009429003, -0.027090767, 0.043379944, -0.0136981895, -0.012452804, -0.0048973984, -0.063616864, -0.06409707, -0.0008366236, 0.0078061842, -0.02245706, -0.041077435, -0.0632223, -0.016612556, -0.017571071, 0.0074873483, -0.016833067, 0.060014192, -0.013803111, -0.013325812, 0.010508544, -0.0037764004, 0.010968007, -0.008805224, 0.032206688, -0.012229212, 0.034772877, 0.024738373, -0.005186857, 0.014733601, 0.036842484, 0.012400095, 0.03437538, 0.039856378, -0.00785175, -0.054771658, 0.030707732, 0.015699012, 0.056946278, 0.0010119203, -0.022591908, 0.021639217, 0.0151952235, 0.018991433, 0.023319399, 0.068065576, -0.0117299715, -0.08813544, 0.0047383746, -0.07252336, 0.013449748, 0.022442093, 0.022378512, -0.00030094356, -0.073941424, 0.022735326, -0.006013422, -0.0062135253, 0.025424035, 0.028794825, 0.080106154, 0.0026702196, 0.022428077, 0.05434949, 0.018035412, 0.027617475, 0.02078037, 0.0100909285, -0.041731182, 0.008351397, 0.0112755615, -0.009654986, 0.00226049, -0.006626333, -0.0042556506, 0.06232886, -0.06866284, 0.010145172, 0.054747973, 0.0029819645, -0.028186074, 0.001157281, -0.034376733, -0.0025728326, 0.029412169, 0.005690802, -0.021520624, -0.008760179, -0.0073512867, -0.01632451, -0.0065614777, 0.008173483, -0.0061489264, -0.023237266, -0.0016068958, 0.014289363, 0.03101211, 0.013190579, -0.01011895, -0.017402226, -0.0092910025, 0.034295823, -0.017353559, 0.0555771, 0.053651888, 0.017215434, 0.05037748, 0.05224412, -0.021142863, -0.014365406, -0.032373764, -0.00474513, 0.080765344, -0.06453213, -0.056366567, 0.015501455, 0.037547898, 0.018404284, -0.0036438233, 0.06832849, 0.011129607, -0.052710257, 0.04091485, 0.04182554, -0.031445805, 0.015088141, 0.03449755, -0.05580458, 0.004115778, -0.047527503, -0.05894667, 0.0021982098, 0.017889412, -0.05173923, 0.047534678, -0.006490967, 0.026147537, 0.037245322, -0.020552134, -0.0037797969, 0.007412507, -0.0129449675, -0.067005865, 0.0041246843, 0.008137646, 0.012128387, -0.0014233006, 0.016024634, 0.035130378, -0.026315274, 0.024913942, -0.0038353754, 0.0034411172, 0.0484022, -0.050540783, -0.039585743, -0.0044579674, -0.07518324, 0.04125372, -0.014067575, 0.051065303, -0.06458728, -0.017719531, -0.02684841, 0.01858974, 0.0013767405, 0.018646257, 0.011389166, 0.067508824, 0.014683311, -0.010918122, 0.008617375, -0.0069176494, -0.018949075, -0.035101935, 0.0063255597, 0.043279085, -0.06034728, 0.004964589, 0.025473299, -0.020188829, -0.043356515, -0.008952097, 0.0017179038, -0.044982597, -0.044093277, -0.0406313, -0.040466286, 0.048030358, 0.0041167666, -0.008412801, -0.005408335, -0.063816845, -0.019533785, 0.006486237, -0.004778366, 0.0153792715, -0.053422786, 0.03676416, -0.055753984, 0.023164755, -0.0036221773, 0.0133757405, -0.03413844, -0.05929881, -0.05974034, 0.013162117, 0.033932324, 0.021155974, -0.071899325, 0.04392928, 0.0121198855, -0.02380242, 0.03740504, 0.00077101367, -0.037197627, 0.03182648, 0.024278417, -0.025821565, -0.009211649, 0.019140054, 0.006306511, 0.052833937, -0.010679807, -0.014823803, 0.021769311, -0.017998867, -0.058524653, 0.065077685, -0.089551695, -0.01577152, -0.021064816, -0.047667816, -0.047037758, -0.023702705, 0.06904892, -0.059217073, 0.009935713, -0.055637397, -0.012218688, -0.046377257, 0.03145351, -0.034322824, -0.029655028, -0.0037494234, 0.058327913, 0.043527793, 0.010138424, -0.045722567, 0.033925973, 0.01804181, -0.028410003, 0.022403624, 0.08126009, 0.0061504445, -0.042584233, 0.07601232, 0.0773493, 0.012928826, -0.04378881, 0.014853515, 0.024142867, 0.036884785, -0.008526831, -0.028892897, 0.024162762, 0.0272824, -0.038257305, -0.023309493, 0.003723742, 0.023695871, -0.018926207, -0.015618695, 0.031393506, -0.03960213, 0.00277926, 0.06685427, 0.037495166, -0.028659906, -0.027676359, 0.04230446, 0.014423734, 0.008748461, -0.022175083, 0.01935391, 0.0040921294, 0.027418086, 0.0074576093, 0.006453532, -0.053812493, -0.048511356, -0.07964272, 0.022423238, -0.019940525, -0.0604216, 0.027949672, -0.056001276, -0.029773038, -0.004176996, 0.020204086, -0.024810204, 0.032450825, -0.026769234, 0.0074924836, 0.03273938, 0.087514326, 0.0056857783, 0.038230006, -0.019201543, 0.018997207, 0.0098184645, -0.004149262, -0.0063852156, -0.013363299, 0.0067418246, -0.032295913, -0.013246096, 0.00612608, -0.024383305, 0.024094556, 0.0276567, 0.052341815, -0.06486566, -0.0037638645, 0.0024282963, 0.0040988713, 0.062282924, -0.012446328, -0.027558887, 0.015956748, 0.036566358, -0.037631746, 0.016175423, -0.028870074, 0.0008587425, 0.06659428, 0.007598552, -0.04989694, -0.05047698, 0.041006867, -0.01477354, 0.01024331, 0.023586813, 0.019255498, -0.05160551, 0.008253529, -0.007329181, 0.015256937, 0.0025488671, 0.0475613, 0.0045438726, -0.025588237, -0.015061801, 0.059843626, 0.018330812, -0.04594275, -0.0042532496, -0.01751682, -0.08968572, -0.0041904896, -0.026985863, 0.022518035, -0.001295697, 0.03380717, 0.07807567, -0.026077243, 0.028342932, -0.05090938, 0.08827374, -0.012612905, -0.039824806, -0.0038507837, -0.03467922, -0.017385568], "index": 3, "object": "embedding"}, {"embedding": [-0.010607744, 0.05135354, -0.119323835, -0.09601037, 0.046889998, -0.02853313, 0.059692357, 0.00043260472, 0.021460332, 0.022521766, -0.008552724, 0.028750556, 0.092477456, 0.0023945123, 0.040346038, 0.031441215, 0.03841013, -0.023093069, 0.034138653, 0.057811864, -0.021393804, -0.023110406, -0.018186389, -0.005291553, 0.011484901, 0.06974869, -0.025576556, -0.07451779, -0.066547506, 0.028892126, 0.036222693, 0.0034748951, 0.04102457, -0.03903658, -0.06740375, -0.048286993, 0.07646995, 0.0061816215, 0.024393557, 0.001457153, 0.0058945464, 0.039654173, 0.081876196, 0.022865467, 0.041689135, -0.0070995907, 0.00896717, 0.008443066, 0.030654114, -0.046422545, 0.006608926, 0.0044002435, 0.01744731, -0.012235155, 0.04226301, 0.026675679, 0.013802161, 0.0014685732, 0.043549463, -0.025322823, 0.03584245, 0.03398009, -0.057172544, 0.013446128, 0.03564272, 0.013672514, -0.033755377, 0.041496668, -0.020350145, -0.06608873, 0.02256766, 0.059590977, -0.030107869, -0.028091544, -0.06643196, 0.031434227, 0.0037487894, -0.038592402, -0.014534734, 0.059687924, 0.004007559, 0.008886006, 0.030473514, 0.057929616, -0.0007564913, -0.010574822, 0.02225313, -0.008579475, -0.03046538, 0.045172382, -0.07716614, 0.028951786, 0.009114816, 0.009127208, -0.089171566, 0.03182285, -0.022637283, 0.0047773304, -0.017258618, 0.01729373, -0.0172477, 0.0130743245, -0.033209678, 0.01558447, -0.01841117, 0.021528093, 0.069903605, 0.018463124, -0.0053152745, -0.040443372, -0.055177145, 0.048467077, -0.021762047, -0.047081508, 0.022490785, -0.035210438, 0.036486838, -0.053170655, 0.04993122, 0.0676321, 0.016501775, -0.0347818, -0.02972739, 0.051231846, 0.0076332283, 0.010212592, -0.04332573, 0.015390898, 0.015226353, -0.030685773, -0.005483139, -0.018290324, -0.024748532, -0.0025433204, -0.03806312, 0.046120077, -0.0153450705, -0.0024310001, 0.048984863, 0.033309568, 0.03957602, 0.008578265, 0.017731532, -0.01629611, -0.012975066, -0.018476209, 0.008034105, -0.028742459, -0.021243861, 0.030971752, 0.015355509, -0.011703993, -0.0059931134, 0.036754597, -0.0044897525, -0.08047349, 0.026317941, 0.019561235, 0.0018739973, 0.02579988, 0.05754131, 0.026878675, -0.057959672, 0.03244974, -0.061512835, -0.05435719, 0.062007695, -0.03565362, 0.0014472993, -0.00044339505, -0.028135598, -0.035216648, 0.041532636, -0.020883707, 0.08522793, 0.00640184, 0.041231714, -0.023048518, 0.04426528, -0.0042969906, 0.059640016, -0.021407317, 0.0029197948, 0.03421253, -0.044519838, 0.004403062, 0.011823437, 0.012404485, -0.0044687847, -0.013592691, 0.027729662, 0.007816994, -0.0803803, -0.010144549, -0.049320295, 0.016026404, 0.03483034, -0.025251163, -0.0009704761, -0.018138846, -0.01948511, -0.048422623, -0.02241503, 0.019670885, -0.058219556, -0.008691299, 0.03740799, 0.040172976, -0.03651944, -0.002565713, 0.033505492, 0.027206887, -0.006793001, 0.027433977, -0.020266522, 0.006364586, -0.02119123, 0.017240394, -0.06309022, -0.021414757, 0.04224523, 0.006329267, -0.0140624335, 0.0056392443, 0.003197569, -0.0052288817, -0.051788226, -0.00042617737, -0.07854998, 0.0026714583, -0.0026588973, -0.049061745, 0.08505717, 0.023834163, 0.029868072, 0.03872155, 0.032260083, 0.042678244, 0.0023991307, -0.00034885612, -0.011389475, -0.008833121, -0.028172886, 0.02264182, -0.09859475, -0.0147255715, 0.017014503, -0.022654606, -0.008416257, 0.11003469, -0.035245854, 0.09243285, 0.039026797, -0.0027525986, 0.04531177, 0.008662551, -0.045891788, -0.033149775, 0.005959446, 0.03145526, 0.02333238, 0.008812989, 0.033027794, -0.060412005, -0.004938053, -0.061302923, -0.03964215, -0.05455163, -0.015689213, 0.01902442, 0.041758798, 0.05259442, 0.0028195158, 0.038028445, 0.03870929, 0.047724675, -0.04220096, -0.059172902, -0.019004371, 0.009891799, 0.021553215, -0.023501297, -0.065519564, -0.02971598, -0.024581747, 0.056213003, 0.02906536, -0.0051750285, 0.0086998185, -0.011076298, 0.028981552, 0.041458, -0.018230004, -0.024616523, 0.044021092, 0.012148234, 0.06858394, -0.022011576, -0.0055111456, -0.02574871, 0.0049073356, 0.0753602, 0.03175755, 0.02252192, -0.0072844927, -0.0011993039, -0.05695324, 0.00992667, 0.041276716, 0.0010905807, 0.032952834, 0.03321366, -0.08615279, -0.0005025875, -0.028668806, 0.038331434, -0.0069099087, 0.0515371, 0.008948866, -0.040011168, 0.0033479629, 0.006174525, -0.03421804, -0.035358112, -0.009037648, 0.01709003, -0.020892873, 0.022439236, -0.015364632, -0.008391988, 0.03019844, 0.039873485, 0.07602772, -0.021746507, -0.068754986, 0.031615827, 0.033778537, -0.0037226614, -0.0025559203, 0.07348817, 0.05706292, -0.026987059, 0.041071936, -0.06295257, 0.014588253, 0.013852337, 0.0045438004, -0.044873737, 0.07111804, 0.021342387, -0.015757168, -0.027665364, -0.0201869, -0.007118267, 0.07361717, -0.008495626, 0.06950432, 0.03753836, 0.025436668, 0.02454726, 0.01199628, -0.011185148, 0.0008551781, -0.038473014, 0.011861848, 0.0003290925, 0.013088157, 0.021301305, -0.027025627, 0.0028866928, -0.0008797499, -0.012903267, -0.019803233, 0.08430774, 0.010466332, 0.0074412697, -0.047315784, 0.050505996, -0.011012311, 0.0037548442, 0.020653179, -0.017854134, -0.04217658, -0.030200265, 0.012352523, -0.030673958, 0.0013821278, -0.07948324, 0.011079041, 0.03997635, -0.021549832, -0.0010062527, -0.041180097, 0.018935507, 0.033781886, -0.013652749, 0.054029252, -0.021207161, -0.027435256, 0.023248743, -0.06855842, -0.0400863, -0.007540682, -0.038394503, 0.040651176, 0.013779197, -0.04898372, -0.01982367, -0.030915394, -0.0048519005, 0.0065146135, 0.066985354, -0.012293631, -0.026886137, 0.0066919043, -0.013074132, 0.068547376, 0.0052288994, 0.04095471, -0.04853367, 0.022895372, 0.046936795, -0.006930657, 0.0027087186, -0.027799817, 0.00082514796, 0.025649818, 0.062134296, 0.00335975, -0.07323748, 0.03479094, 0.036491748, 0.07667566, -0.03382123, -0.0069570765, 0.0065635103, 0.0013803947, -0.0013881973, -0.01832133, 0.064586855, -0.0067512556, -0.07495126, -0.017734976, -0.04457691, 0.0257081, 0.014647304, 0.027492223, -0.026950033, -0.061134037, -0.0028712475, 0.015342391, 0.021254238, 0.05669208, 0.008765434, 0.061863072, 0.009175873, 0.0076402295, 0.046410464, 0.022960247, 0.050148383, 0.015335579, 0.0123514775, -0.029459082, -0.015567047, 0.01945381, -0.010690732, 0.039569095, 0.012755956, 0.019234017, 0.04455764, -0.049111135, 0.023713717, 0.02673393, -0.010095747, -0.063501164, 0.029492684, -0.037018765, -0.042744588, 0.027425831, 0.041843213, -0.07436035, -0.05629149, -0.0002784143, -0.04845662, -0.025571216, 0.027499028, -0.010959558, -0.042922415, -0.015168462, 0.013105365, 0.009626445, 0.018968415, 0.02140363, -0.035874665, 0.009924872, -0.0007622467, 0.010670595, 0.05587318, 0.032728713, -0.016586889, 0.021281859, 0.063780114, -0.03167858, -0.010981312, 0.006917681, -0.03231146, 0.020566424, -0.030176232, -0.06213557, 0.004896949, 0.012764782, 0.034359686, 0.03701358, 0.016820021, 0.010524846, -0.07941007, 0.023536239, 0.027093468, -0.014244321, -0.012570319, 0.023161236, -0.025586314, -0.022430798, -0.056265716, -0.050893765, 0.005025146, -0.026607683, -0.068507925, 0.036826007, -0.050563265, -0.013313138, 0.009796136, -0.016009355, 0.012821436, 0.012935818, -0.021778466, -0.04731322, 0.023598008, 0.035080202, 0.0089986725, -0.005858444, 0.0224204, 0.00086149055, 0.012435077, 0.009143793, -0.0027396185, -0.035010207, 0.011727904, -0.043581437, -0.0565908, -0.0005336587, 0.0021683443, 0.017815793, -0.038593657, 0.039947774, -0.0661926, -0.000113279944, -0.06376108, -0.015018439, 0.024114404, -0.010967076, -0.026477663, 0.043497134, 0.04734017, 0.004352948, -0.0047635194, -0.012885886, -0.048728347, -0.045252983, 0.022936989, 0.06277567, -0.06997031, -0.0140001485, 0.025506973, -0.014041154, -0.018964633, -0.047819216, -0.035959106, 0.0014337691, -0.075273424, -0.015952637, -0.03838813, -0.026970396, 0.007425272, -0.04019485, -0.007987064, -0.03037978, -0.027663192, -0.020708138, -0.020492062, 0.021366263, -0.05467035, 0.016218944, -0.061118484, 0.0076664397, 0.030819414, -0.005595735, -0.013090323, -0.051986456, -0.042739093, -0.023186645, 0.03278168, 0.08575725, -0.041266043, 0.018386869, 0.015197869, -0.028973186, 0.06586081, 0.009995264, -0.06320449, 0.0062753297, 0.046177857, 0.0007801142, -0.0030161582, 0.027474985, 0.035081457, 0.032262046, -0.007028353, -0.042350743, 0.018275151, 0.009073115, -0.049358666, 0.043808967, -0.05952334, 0.009110167, -3.2505242e-05, -0.019737022, -0.044721637, -0.03568488, 0.0413604, -0.061447784, 0.013868185, -0.018519448, -0.015238675, -0.07014549, 0.021505428, -0.06440279, -0.0010540491, -0.024825474, 0.041255802, 0.030342532, -0.003065983, -0.006805459, 0.055192996, -0.003715935, 0.04432304, 0.013352113, 0.07230958, -0.008455892, -0.035499062, 0.045789916, 0.06569607, 0.03136132, -0.021815207, 0.011582427, 0.051323757, 0.04402548, -0.017308243, -0.011587364, -0.00816654, 0.015605418, -0.068453446, -0.052287605, -0.0023483455, 0.0045336084, -0.040343262, -0.012314396, -0.013385059, -0.04369275, 0.004478119, 0.044077855, 0.009113566, -0.06457946, -0.013459591, 0.02131701, -0.004671628, 0.02331745, -0.005403304, 0.026738109, 0.022599129, 0.009921051, 0.025597721, 0.003486218, -0.014321434, -0.03644111, -0.02956088, 0.022904593, -0.011405327, -0.046963003, 0.023369033, -0.06373559, -0.058344997, -0.006175141, -0.0075763064, -0.03879154, 0.008616699, -0.047436707, -0.008489294, -0.009112284, 0.0427385, 0.04429731, 0.028766982, -0.018186916, -0.023942173, 0.015409406, -0.017777931, 0.034737293, -0.044900697, 0.015199239, -0.0029676342, 0.052268274, 0.012188927, 0.022752855, 0.020027349, 0.026015153, 0.029849013, -0.06031323, -0.02424879, -0.01866594, -0.0013112032, 0.025132615, -0.0056540854, -0.022361422, 0.04069758, 0.053774636, 0.019806882, 0.024106082, -0.05124784, -0.0122158555, 0.027869891, -0.0012432649, -0.06995101, -0.04714161, 0.036423735, 0.0047060824, 0.04379695, -0.021570968, 0.016217308, -0.07802464, 0.012496259, -0.020599015, 0.009758463, -0.019748736, -0.021641634, -0.0041487874, -0.032163676, -0.032586772, 0.10464958, 0.0046653394, -0.03419432, -0.011675102, 0.017274402, -0.030230718, 0.0075150095, 0.026128601, 0.05406413, 0.008308351, -0.02371861, 0.07521183, 0.04344521, 0.052367065, -0.074573025, 0.051306695, -0.0319219, -0.040929332, 0.0029823487, -0.055946566, 0.006900852], "index": 4, "object": "embedding"}, {"embedding": [0.009305379, 0.0034888708, -0.17477445, -0.07373383, 0.052953314, 0.0054439977, 0.053990588, 0.048117127, -0.046655048, 0.023901155, 0.012513128, 0.039531425, 0.115269, -0.014241126, 0.001469012, 0.0075528338, -0.0073917415, -0.07023515, 0.0017745115, 0.025461793, -0.04191032, -0.0076130177, 0.012649249, -0.042217035, 0.0012865327, 0.010698673, -0.023925606, -0.016061636, -0.016150605, 0.026646819, 0.049777407, -0.025365293, -0.0016442246, -0.08875174, -0.10487395, 0.00089182047, 0.05157359, -0.012017323, 0.01493558, 0.023691623, 0.04481338, 0.004057902, 0.022833815, 0.00024742284, 0.058887705, 0.01627069, 0.04886571, 0.002794643, 0.048255105, -0.046066467, 0.013031168, 0.00044365865, 0.029430747, 0.035242997, 0.113369085, 0.026315035, -0.020685578, 0.03512951, 0.05689023, -0.068773195, 0.06857387, -0.007321212, -0.09212156, 0.003703807, -0.0007322848, -0.034399506, -0.029838387, 0.034323372, -0.010661844, -0.025497684, -0.011318572, 0.018562056, -0.004078597, 0.041081592, -0.031183574, 0.050822347, -0.0049731266, 0.007375048, 0.021333672, -0.0143104205, -0.0018901933, -0.00506233, 0.06389518, 0.0100592915, 0.022981545, 0.017241405, -0.062392276, 0.011163627, -0.07978257, 0.10570582, -0.0064030234, 0.014090336, 0.027443368, 0.0051277373, -0.03131116, 0.04845588, -0.06849074, -0.014612857, 0.023013016, 0.027351111, -0.027729206, -0.01126889, 0.010269542, -0.0051965397, -0.011380004, 0.015165802, -0.0108433915, -0.025322277, -0.03334818, 0.025172466, -0.03861286, 0.036511235, -0.0074007423, 0.015308963, -0.0151847005, -0.03448374, 0.10915765, -0.017073372, -0.02066855, 0.024677131, 0.0038786037, -0.014815338, -0.027060363, 0.00231878, 0.0050381804, 0.08264391, -0.025285652, 0.016003322, -0.03321559, -0.0489171, 0.0069331536, -0.026047481, -0.03112238, 0.002342223, 0.024389353, 0.068901405, -0.038472213, 0.015044597, 0.033713013, 0.011381087, 0.0030526055, 0.009343639, 0.007056359, -0.0051843263, -0.028899686, -0.04618363, -0.030733956, -0.039098643, -0.047384325, -0.0035478498, 0.0055633252, 0.047876623, 0.03279747, 0.017214242, 0.0057850457, -0.01133038, -0.030913997, -0.039278958, 0.010264793, 0.0027378977, 0.03958161, 0.0019911325, 0.01829269, 0.03605728, -0.01633196, -0.0011624633, 0.045282103, -0.01447793, 0.050303828, -0.0033797524, -0.037608888, -0.026117997, 0.021658871, -0.019131545, -0.011520115, -0.0032038104, 0.06553721, -0.039527573, 0.05007223, -0.021540914, 0.036812354, -0.03549932, 0.071284465, 0.020549823, 0.0025561417, -0.04304037, 0.027069239, 0.022005398, -0.0527813, -0.06010907, 0.04752475, 0.04153763, -0.08704657, -0.038171872, 0.006986427, -0.04968398, 0.056628503, -0.03120363, 0.0022183838, -0.0038363857, -0.0011028233, -0.0035568473, -0.038012605, 0.051002376, -0.06454802, 0.10899831, 0.018174661, 0.010493224, -0.025633952, 0.023523908, 0.020157399, -0.026179014, 0.030910844, 0.027519112, -0.00043143608, 0.012860251, -0.014910385, -0.0024184713, -0.015597507, 0.001982547, 0.014477877, -0.008537788, 0.019727135, -0.0236218, 0.026187392, 0.026618008, -0.06879633, 0.0034740053, -0.0071955263, 0.034174908, -0.011810549, -0.068099454, 0.04699039, 0.009757788, -0.06770811, 0.0072908527, 0.022374261, 0.03797525, 0.024302699, 0.013235263, 0.007230375, 0.02209201, -0.035335094, 0.0062476057, -0.047353346, -0.019002633, 0.0068848548, -0.050611284, -0.022891285, -0.0020877705, -0.012317634, 0.045533147, 0.0045452276, -0.07522402, 0.0007223348, -0.0025499559, -0.02037267, -0.017534193, 0.0024731748, 0.036537606, 0.025567804, -0.009884685, 0.026709413, -0.015574845, -0.013149872, -0.080837704, -0.0022158094, -0.006932047, -0.0037977453, -0.051331915, 0.015604188, 0.036285084, 0.064280905, 0.058526002, 0.011813968, 0.03716377, -0.026003085, -0.007987883, -0.0052283523, 0.0069134394, -0.041480113, -0.056960154, -0.015813012, -0.0015379966, -0.0067169913, -0.003154167, 0.038316704, 0.013157022, 0.062047765, 0.011918529, 0.021715254, 0.033395175, -0.022896996, -0.001762905, -0.012956838, 0.034017067, 0.08217729, -0.030059282, 0.03281668, -0.03674316, -0.011654595, 0.04091494, 0.063970275, 0.033276033, -0.012827335, -0.0038350117, -0.054344483, -0.035056565, 0.050834134, 0.025147114, -0.036136493, -0.018470792, -0.052138478, -0.013505627, -0.032986954, -0.025412332, 0.029139962, 0.04051361, 0.027975645, -0.008336652, 0.07401483, -0.048684943, 0.00641236, -0.042977046, -0.020707548, -0.015763013, -0.0065484587, -0.01595874, -0.043594744, -0.018919418, 0.05021357, -0.01222591, 0.044296302, -0.029443419, -0.028139241, -0.016537193, 0.0041424497, -0.022990689, 0.023110894, 0.024895007, 0.04600759, -0.0022066538, 0.02719765, -0.018054293, -0.04652242, 0.0016045085, -0.06513785, -0.027250456, -0.010065798, 0.0019231675, -0.017400624, -0.0019570794, -0.017018834, -0.011351262, 0.03297543, 0.019379834, 0.058074944, 0.0050254785, 0.02706262, 0.011165891, -0.0013932133, -0.029977635, -0.005768714, -0.05859908, -0.0037564572, -0.01690707, 0.02058441, 0.019384284, 0.045752715, 0.026243912, -0.012844018, -0.021398911, -0.045694597, 0.059789088, -0.017644325, 0.008448502, 0.0047770683, -0.01125548, -0.05531011, -0.015498526, 0.025118167, 0.021828584, -0.042681918, -0.0063840635, -0.0001776486, -0.0062539093, 0.024890985, -0.022255616, -0.011215333, 0.013603339, 0.029300394, -0.010979837, -0.07475008, 0.039195493, 0.0029460299, -0.015831167, 0.114965186, -0.0021242283, 0.022118688, 0.04339587, -0.030516239, -0.04915905, 0.0184291, -0.006147799, -0.016598104, -0.022876335, -0.039320063, 0.018808674, -0.011789643, 0.022264611, 0.028447207, 0.015261828, -0.035513844, -0.042432267, -0.0035252795, -0.0019864729, 0.00998704, -0.015076627, -0.022611585, 0.004861842, 0.05550382, 0.013665108, -0.00020077074, 0.01202519, -0.015662344, 0.02949699, 0.100999564, 0.03472158, -0.005909344, -0.09781573, 0.044246614, 0.027580429, 0.00023768937, 0.023014044, 0.048313092, 0.035147764, -0.01795077, 0.010575498, 0.0036766694, 0.057263363, 0.024072034, -0.051544156, -0.024980528, -0.049028467, -0.0166519, 0.046206985, 0.09225714, -0.026687859, -0.09565709, -0.01092392, -0.051578738, -0.0019064564, 0.03733648, -0.021545665, 0.07685358, -0.011226857, 0.03957016, 0.03635405, 0.01137809, 0.03800552, 0.031076914, -0.006163769, -0.050101873, -0.0064905444, -0.00198112, -0.019735705, -0.032464, 0.013865689, 0.006974813, 0.064573154, -0.020628724, 0.022190422, 0.029154025, 0.031653464, -0.03964269, -0.00015910262, -0.028940266, 0.0071878657, 0.04046803, 0.020271784, 0.021446433, -0.03465849, -0.014492559, -0.04606493, -0.010005635, 0.041171666, 0.03784303, -0.013115627, 0.010349092, 0.026762618, 0.04655497, 0.059077423, 0.03220661, -0.004376384, -0.058054198, -0.014458918, 0.024778597, 0.008533859, -0.00060505787, -0.00558874, -0.013195211, 0.069737636, -0.022304269, 0.003992307, -0.0010179501, -0.021332568, 0.023104161, -0.07357799, -0.054406784, 0.024729999, -0.0002412381, 0.042442996, 0.009470682, 0.04014965, 0.03768664, -0.08853845, 0.044801105, 0.03388568, -0.00034205706, 0.014075126, 0.046370864, -0.012864252, 0.015565276, -0.03191579, -0.075330414, -0.028366279, -0.0018814412, -0.023062931, 0.025833447, 0.013907201, 0.041245714, 0.033206835, -0.05564555, -0.009141837, 0.0009557411, -0.014535032, -0.019516993, 0.05931808, -0.016627675, 0.02407624, -0.013588235, -0.0003117244, -0.04067807, -0.016421447, 0.0023830668, -0.024943907, -0.018599894, -0.019493666, -0.0035706833, -0.07275076, 0.014212081, -0.08379349, 0.041566335, -0.045669347, 0.045005444, -0.034286916, -0.05034985, -0.036053747, 0.0016121326, -0.0011907639, 0.011320808, 0.0055506476, 0.113768704, 0.037672084, 0.029155914, -0.016067464, -0.029976243, 0.01928001, -0.003959093, 0.087591805, 0.031614073, -0.04997899, 0.0054788566, 0.0066445093, -0.018586013, -0.026582817, -0.0024460997, -0.029663298, -0.009132044, -0.06382956, 0.011579985, -0.018093726, 0.032151006, -0.002397482, 0.0047114524, 0.02756775, -0.07267412, -0.03830467, 0.03601875, -0.008702004, -0.04379676, -0.030957391, 0.008678022, -0.025789263, -0.036947865, -0.017296284, -0.040027916, -0.0715003, -0.03862749, -0.042998098, -0.053117987, 0.031085584, 0.045164555, -0.092569396, 0.05029271, 0.09010343, -0.01873359, 0.010106194, 0.032364313, -0.003209355, -0.030561108, -0.023824671, -0.0019172109, -0.0025203167, -0.01828785, -0.020305498, 0.017977254, -0.009122291, -0.036909267, 0.022327887, -0.035997443, -0.035959546, 0.03208741, -0.052382853, 0.0077517508, 0.034273252, -0.013097528, -0.0058482196, 0.02513236, 0.052073628, -0.075165965, 0.022779984, -0.01503077, 0.012581777, -0.088059165, 0.028973764, -0.049712475, 0.00032156537, 0.045307107, 0.05755685, 0.04852097, -0.036980655, -0.016940108, -0.020084495, 0.0091641955, 0.0082925055, 0.036368042, 0.054744653, 0.01712087, -0.02562616, 0.019819057, 0.07650724, 0.020069815, -0.050534807, -0.03560006, 0.06796077, 0.034423336, -0.0071066963, 0.022575645, 0.020798588, 0.03813074, -0.0356883, 0.0152805885, 0.026887974, 0.050921816, -0.016968777, -0.019136498, -0.041439362, 0.0016253729, -0.032116544, 0.055550486, -0.025061008, -0.010863953, -0.024920667, 0.056291983, -0.0057998784, -0.015117242, 0.023331182, 0.011403934, 0.03508717, -0.008893669, -0.017355569, -0.0021151379, -0.02504504, -0.0443841, 0.02298797, -0.013681369, -0.0021481884, 0.0263868, -0.038731858, -0.04441738, -0.028601244, -0.017438434, -0.010340099, -0.035910785, 0.055600446, 0.045542467, 0.0075990413, 0.012655111, 0.04132314, 0.019177845, 0.044172898, -0.010103596, 0.034960438, 0.01608674, 0.012839038, -0.0020954139, -0.008776081, 0.017304204, -0.024609365, -0.05319763, -0.018042546, -0.056274947, 0.044309903, -0.01368522, 0.015939346, 0.008072664, -0.061300434, -0.031658623, -0.017677316, 0.03618585, 0.011192629, -0.046973083, 0.025414677, 0.044412814, -0.0437484, -0.03199921, -0.019751908, -0.009840525, 0.019162368, -0.008040571, -0.079588376, -0.01471176, 0.03301756, -0.0035611002, 0.024518881, 0.014577995, -0.00392516, -0.016183814, -0.004977405, -0.029359365, 0.03804074, 0.022625403, 0.020008007, -0.036938265, -0.067486905, -0.01804421, 0.05029699, 0.018890565, -0.037913993, 0.0029196502, -0.039843503, 0.012616865, 0.0049696523, 0.0075712902, 0.006696057, -0.00036596914, -0.014477331, 0.09723743, -0.0061366265, 0.0056426194, -0.015722804, 0.044873223, -0.0018592074, -0.033734202, -0.014077022, -0.025472043, -0.006736755], "index": 5, "object": "embedding"}], "model": "nomic-embed-text:v1.5", "object": "list", "usage": {"prompt_tokens": 7097, "total_tokens": 7097}}, "input": {"input": [" updating, ensuring consistent performance across various configurations.\n4. Simplicity and Scalability: HFT does not alter the model architecture, which simplifies im-\nplementation and allows for scalable applications, particularly beneficial in successive fine-tuning\nscenarios.\n5. Versatility: The technique has proven effective across diverse fine-tuning scenarios, including\nsupervised fine-tuning, direct preference optimisation, and continual learning.\n45\nFigure 6.7: Schematic illustration of the Half Fine-Tuning (HFT) method as applied to LLAMA 2s\narchitecture. The diagram shows multiple stages of fine-tuning, where specific model parameters are\nselectively activated (orange) while others remain frozen (blue). This approach optimises training by\nreducing computational requirements while still effectively adapting the model to new tasks or data.\n(adapted from [68])\n6.4.2\nComparison between HFT and LoRA\nCriteria\nHFT\nLoRA\nObjective\nThe goal is to retain the foun-\ndational knowledge acquired dur-\ning pre-training while learning new\ntask-specific skills, thus balancing\nbetween maintaining existing ca-\npabilities and acquiring new ones.\nLoRA aims to reduce computa-\ntional and memory requirements\nduring fine-tuning, making it more\nefficient and feasible to train large\nmodels on limited hardware re-\nsources.\nApproach\nHFT involves freezing half of the\nmodels parameters during each\nfine-tuning round and updating\nonly the other half.\nLoRA reduces the number of train-\nable parameters by\nintroducing\nlow-rank decomposition into the\nweight matrices of the neural net-\nwork. This involves injecting low-\nrank matrices into the models lay-\ners during fine-tuning.\nModel Architecture\nHFT does not alter the models ar-\nchitecture or introduce new param-\neters, making it straightforward\nto apply without additional struc-\ntural changes.\nLoRA\nmodifies\nthe\nmodel\nby\nadding low-rank matrices, which\nchanges the training dynamics and\nrequires additional computations\nfor the low-rank updates.\nPerformance\nResearch has shown that HFT\ncan restore forgotten basic knowl-\nedge while maintaining high per-\nformance in general abilities.\nLoRA is designed to achieve com-\npetitive performance with full fine-\ntuning but with significantly fewer\ntrainable\nparameters\nand\nlower\ncomputational costs.\nTable 6.3: Comparative Analysis of Half Fine-Tuning (HFT) and Low-Rank Adaptation (LoRA).\n46\n6.5\nLamini Memory Tuning\nLamini [69] was introduced as a specialised approach to fine-tuning Large Language Models (LLMs),\ntargeting the reduction of hallucinations. This development was motivated by the need to enhance the\nreliability and precision of LLMs in domains requiring accurate information retrieval. Traditional training\nmethods typically consist of running stochastic gradient descent on vast datasets, which, despite fitting\nthe training data well, often produce models that fail to generalise effectively and are prone to such errors.\nFoundation models often follow a training regimen similar to the Chinchilla recipe, which prescribes\ntraining for a single epoch on a massive corpus, such as training Llama 2 7B on about one trillion\ntokens. This approach results in substantial loss and is geared more towards enhancing generalisation\nand creativity where a degree of randomness in token selection is permissible. However, it falls short for\ntasks demanding high factual precision. In contrast, Lamini Memory Tuning delves deeper by analysing\nthe loss of individual facts, significantly improving the accuracy of factual recall.\nBy augmenting a\nmodel with additional parameters specifically for memory (e.g., an 8B parameter model with an extra 2B\nparameters for weights), Lamini enables the model to memorise and accurately recall a significant number\nof facts, closely aligning performance with LLM scaling laws without compromising on generalisation.\n6.5.1\nLamini-1 - A model architecture based on Lamini\nDeparting from traditional transformer-based designs, the Lamini-1 model architecture (Figure 6.8) em-\nploys a massive mixture of memory experts (MoME). This system features a pre-trained transformer\nbackbone augmented by adapters that are dynamically selected from an index using cross-attention\nmechanisms. These adapters function similarly to experts in MoE architectures, and the network is\ntrained end-to-end while freezing the backbone. This setup allows for specific facts to be stored exactly\nin the selected experts.\nFigure 6.8: Diagram of the Lamini-1 Model Architecture, featuring a Massive Array of Memory Experts\n(MoME). This architecture integrates a pre-trained transformer backbone with dynamically selected\nadapters via cross-attention mechanisms. Each adapter, functioning as a memory expert, is capable of\nstoring specific factual data. (adopted from [69])\nAt inference time, only the relevant experts are retrieved from the index, enabling the LLM to store a\nlarge number of facts while maintaining low inference latency. Specialised GPU kernels written in Triton\nare used to accelerate the lookup of experts, optimising the system for quick access to stored knowledge.\nSystems Optimisations for Banishing Hallucinations\nThe MoME architecture is designed to minimise the computational demand required to memorise facts.\nDuring training, a subset of experts, such as 32 out of a million, is selected for each fact. The weights of\nthe backbone network and the cross attention used to select the expert are frozen, and gradient descent\nsteps are taken until the loss is sufficiently reduced to memorise the fact. This approach prevents the\nsame expert from being selected multiple times for different facts by first training the cross attention\n47\nselection mechanism during a generalisation training phase, then freezing", " memorise facts.\nDuring training, a subset of experts, such as 32 out of a million, is selected for each fact. The weights of\nthe backbone network and the cross attention used to select the expert are frozen, and gradient descent\nsteps are taken until the loss is sufficiently reduced to memorise the fact. This approach prevents the\nsame expert from being selected multiple times for different facts by first training the cross attention\n47\nselection mechanism during a generalisation training phase, then freezing its weights.\nThis method ensures that computation scales with the number of training examples, not the total\nnumber of parameters, thereby significantly reducing the computation required for memory tuning.\nThis optimised approach allows Lamini-1 to achieve near-zero loss in memory tuning on real and random\nanswers efficiently, demonstrating its efficacy in eliminating hallucinations while improving factual recall.\n6.6\nMixture of Experts\nA mixture of experts (MoE) is an architectural design for neural networks that divides the computation\nof a layer or operation (e.g., linear layers, MLPs, or attention projection) into several specialised subnet-\nworks, referred to as experts. Each expert independently carries out its computation, and the results\nare aggregated to produce the final output of the MoE layer. MoE architectures can be categorised as\neither dense, where every expert is engaged for each input, or sparse, where only a subset of experts is\nutilised for each input.\n6.6.1\nMixtral 8x7B Architecture and Performance\nMixtral [70] 8x7B employs a Sparse Mixture of Experts (SMoE) architecture (Figure 6.9), mirroring the\nstructure of Mistral 7B but incorporating eight feedforward blocks (experts) in each layer. For every\ntoken at each layer, a router network selects two experts to process the current state and combine their\noutputs. Although each token interacts with only two experts at a time, the selected experts can vary at\neach timestep. Consequently, each token has access to 47 billion parameters but utilises only 13 billion\nactive parameters during inference. Mixtral 8x7B not only matches but often surpasses Llama 2 70B\nand GPT-3.5 across all evaluated benchmarks. Its performance is notably superior to Llama 2 70B in\nmathematics, code generation, and multilingual tasks.\nFigure 6.9: Diagram of the Mixtral 8x7B Mixture of Experts (MoE) model architecture. The model is\ncomposed of a router network that dynamically selects the most relevant experts from a pool of eight\ntransformer-based experts, each with 7 billion parameters. The experts are organised into transformer\nblocks, where the router directs data to the appropriate expert based on the input, optimising com-\nputational efficiency and model performance. This architecture allows for scalability and specialised\nprocessing within large language models. (adapted from [71])\n48\n6.7\nMixture of Agents\nDespite the numerous LLMs and their notable accomplishments, they continue to encounter fundamental\nlimitations regarding model size and training data. Scaling these models further is prohibitively expen-\nsive, often necessitating extensive retraining on multiple trillion tokens. Simultaneously, different LLMs\nexhibit distinct strengths and specialise in various aspects of tasks. A recent study has investigated\nleveraging the collective expertise of multiple LLMs to develop a more capable and robust model, a\nmethod known as Mixture of Agents (MoA) [72].\nMoA functions using a layered architecture, where each layer comprises multiple LLM agents (Figure\n6.10). This structure reveals a phenomenon known as the collaborativeness of LLMs. The innova-\ntive MoA framework utilises the combined capabilities of several LLMs to enhance both reasoning and\nlanguage generation proficiency. Research indicates that LLMs naturally collaborate, demonstrating im-\nproved response quality when incorporating outputs from other models, even if those outputs are not\nideal.\nFigure 6.10: Illustration for Mixture of Agents (MoA) LLM configuration. The model consists of multiple\nlayers, each incorporating several agents that process the input independently before concatenating their\noutputs to form an intermediate result. The process continues across layers, refining the output at each\nstage to generate the final output based on the given prompt (adapted from [72]).\n6.7.1\nMethodology\nTo enhance collaboration among multiple LLMs, it is essential to understand their individual strengths\nand classify them accordingly. The classification includes:\n1. Proposers: These models excel at generating valuable reference responses for other models. While\nthey may not perform exceptionally on their own, they provide useful context and varied perspec-\ntives that improve the final output when utilised by an aggregator.\n49\n2. Aggregators: These models are adept at merging responses from various models into a single\nhigh-quality result. An effective aggregator should maintain or even enhance the quality of the\nfinal response, regardless of the quality of the individual inputs.\nThe careful selection of LLMs for each MoA layer is crucial Performance metrics, such as average win\nrates in a given layer, help assess the suitability of models for subsequent layers, ensuring the production\nof higher-quality outputs. Diversity in model outputs is vital, as varied responses from different models\ncontribute significantly more than homogeneous outputs from a single model. In MoA, given an input\nprompt, the output of the ith MoA layer yi is calculated as follows:\nyi =\nn\nM\nj=1\n[Ai,j(xi)] + x1, xi+1 = yi\n(6.1)\n6.7.2\nAnalogy with MoE", " ensuring the production\nof higher-quality outputs. Diversity in model outputs is vital, as varied responses from different models\ncontribute significantly more than homogeneous outputs from a single model. In MoA, given an input\nprompt, the output of the ith MoA layer yi is calculated as follows:\nyi =\nn\nM\nj=1\n[Ai,j(xi)] + x1, xi+1 = yi\n(6.1)\n6.7.2\nAnalogy with MoE\nMixture-of-Experts (MoE) is a well-established machine learning technique where multiple expert net-\nworks, each with specialised skills, collaborate to address complex problems. This approach has demon-\nstrated significant success across various applications and serves as the inspiration for the Mixture-of-\nAgents (MoA) method. In a typical MoE design, a stack of layers, known as MoE layers, consists of\nmultiple expert networks, a gating network, and residual connections to improve gradient flow. The\noutput for layer yi is calculated as follows:\nyi =\nn\nX\nj=1\nGi,j(xi)Ei,j(xi) + xi\n(6.2)\nThe MoA framework advances the MoE concept by operating at the model level through prompt-based\ninteractions rather than altering internal activations or weights. Instead of relying on specialised sub-\nnetworks within a single model, MoA utilises multiple full-fledged LLMs across different layers. In this\napproach, the gating and expert networks functions are integrated within an LLM, leveraging its ability\nto interpret prompts and generate coherent outputs without additional coordination mechanisms.\n6.7.3\nWhat makes MoA works well?\n1. MoAs Superior Performance: MoA significantly outperforms LLM-based rankers, which select\none answer from the proposals rather than generating new responses. This suggests that MoAs\napproach of aggregating all generated responses provides more effective results than simply choosing\nfrom pre-existing options.\n2. Effective Incorporation of Proposals: The aggregator in MoA demonstrates a tendency to\nintegrate the best proposed answers. This is supported by positive correlations between aggregator\nresponses and various similarity metrics, such as BLEU scores, which measure n-gram overlaps. The\nuse of alternative similarity measures also shows a consistent positive correlation with preference\nscores, indicating that the aggregator effectively utilises the proposed responses.\n3. Influence of Model Diversity and Proposer Count: Increasing the number of proposers\nimproves output quality, highlighting the benefits of additional auxiliary information. Additionally,\nusing a diverse set of LLMs as proposers consistently yields better results compared to using a single\nLLM. This suggests that both the number and diversity of LLM agents in each MoA layer contribute\nto enhanced performance, with potential for further improvement through scaling.\n4. Model Specialisation: Analysis of model roles within the MoA ecosystem reveals that GPT-4o,\nQwen, and LLaMA-3 are effective in both assisting and aggregating tasks. In contrast, WizardLM\nexcels as a proposer but struggles with aggregating responses from other models.\n6.8\nProximal Policy Optimisation (PPO)\nPPO [73] is a widely recognised reinforcement learning algorithm used for training agents to perform tasks\nin diverse environments. This algorithm leverages policy gradient methods, where policiesrepresented\n50\nby neural networksdetermine the actions taken by the agent based on the current state. PPO ef-\nfectively handles the dynamic nature of training data generated through continuous agent-environment\ninteractions, a feature that differentiates it from static datasets used in supervised learning.\nThe innovation of PPO lies in its surrogate objective function, optimised via stochastic gradient ascent.\nThis approach allows for multiple updates from the same batch of data, enhancing both training efficiency\nand stability over traditional policy gradient methods. Developed by OpenAI, PPO was designed to\nbalance ease of implementation with the robust performance characteristics of more complex algorithms\nlike Trust Region Policy Optimisation (TRPO), but without the associated computational complexity.\nPPO operates by maximising expected cumulative rewards through iterative policy adjustments that\nincrease the likelihood of actions leading to higher rewards. A key feature of PPO is its use of a clipping\nmechanism in the objective function, which limits the extent of policy updates, thus preventing drastic\nchanges and maintaining stability during training.\nFigure 6.11: Schematic of Proximal Policy Optimisation (PPO) applied in the context of Reinforcement\nLearning from Human Feedback (RLHF) for fine-tuning a Large Language Model (LLM). The process\ninvolves using a prompt dataset to train the LLM. The PPO algorithm adjusts the LLMs policy based\non rewards provided by the reward model, which is fine-tuned through human feedback. (adapted from\n[73])\nPython Library - HuggingFace Transformer Reinforcement Learning (TRL4) package supports the\nPPO Trainer5 for training language models from the preference data.\nThe PPOTrainer expects to align a generated response with a query given the rewards obtained from the\nReward model. During each step of the PPO algorithm we sample a batch of prompts from the dataset,\nwe then use these prompts to generate the a responses from the SFT model. Next, the Reward model\nis used to compute the rewards for the generated response. Finally, these rewards are used to optimise\nthe SFT model using the PPO algorithm. Therefore the dataset should contain a text column which we\ncan rename to query. Each of the other data-points required to optimise the SFT model are obtained\nduring the training loop.\n6.8.1\nBenefits of PPO\n1. Stability: Proximal Policy Optimisation (PPO) is designed to", " Reward model\nis used to compute the rewards for the generated response. Finally, these rewards are used to optimise\nthe SFT model using the PPO algorithm. Therefore the dataset should contain a text column which we\ncan rename to query. Each of the other data-points required to optimise the SFT model are obtained\nduring the training loop.\n6.8.1\nBenefits of PPO\n1. Stability: Proximal Policy Optimisation (PPO) is designed to ensure stable and reliable policy\nupdates. The clipped surrogate objective function is central to this stability, as it limits policy\nupdates to prevent large, potentially destabilising changes. This results in smoother and more\nconsistent learning.\n2. Ease of Implementation: Compared to advanced algorithms TRPO, PPO is relatively straight-\nforward to implement. It avoids the need for second-order optimisation techniques, making it more\n4https://huggingface.co/docs/trl/en/index\n5https://huggingface.co/docs/trl/main/en/ppo_trainer\n51\naccessible to less experienced practitioners.\n3. Sample Efficiency: PPO achieves data efficiency through its use of the clipped surrogate objec-\ntive. This mechanism regulates policy updates, ensuring stability while effectively reusing training\ndata.\nConsequently, PPO tends to be more sample-efficient than other reinforcement learning\nalgorithms, performing well with fewer samples, which is advantageous in scenarios where data\ncollection is costly or time-consuming.\n6.8.2\nLimitations of PPO\n1. Complexity and Computational Cost: Proximal Policy Optimisation (PPO) involves intricate\npolicy and value networks, necessitating substantial computational resources for training. This\ncomplexity often results in extended training durations and increased operational expenses.\n2. Hyperparameter Sensitivity: PPOs performance is highly dependent on several hyperparame-\nters, such as the clipping range, learning rate, and discount factor. Achieving optimal performance\nrequires meticulous tuning of these parameters. Incorrect settings can lead to suboptimal policy\noutcomes or instability during the learning process.\n3. Stability and Convergence Issues: Although PPO is designed to enhance stability compared\nto earlier methods, it can still encounter convergence issues, particularly in highly dynamic or\ncomplex environments. Maintaining stable policy updates remains a significant challenge.\n4. Reward Signal Dependence: PPOs effectiveness is heavily reliant on a well-defined reward\nsignal to guide the learning process. In scenarios where designing an appropriate reward function\nis challenging or impractical, PPO may struggle to attain the desired results.\n6.8.3\nTutorial for training models using PPO technique\nThe tutorial for tuning GPT2 to generate positive movie reviews based on the IMDB dataset using PPO\ntechnique can be found here.\n6.9\nDirect Preference Optimisation (DPO)\nDirect Preference Optimisation (DPO) [74] offers a streamlined approach to aligning language models\n(LMs) with human preferences, bypassing the complexity of reinforcement learning from human feedback\n(RLHF). Large-scale unsupervised LMs typically lack precise behavioural control, necessitating meth-\nods like RLHF that fine-tune models using human feedback. However, RLHF is intricate, involving the\ncreation of reward models and the fine-tuning of LMs to maximise estimated rewards, which can be\nunstable and computationally demanding. DPO addresses these challenges by directly optimising LMs\nwith a simple classification objective that aligns responses with human preferences. This approach elim-\ninates the need for explicit reward modelling and extensive hyperparameter tuning, enhancing stability\nand efficiency. DPO optimises the desired behaviours by increasing the relative likelihood of preferred\nresponses while incorporating dynamic importance weights to prevent model degeneration. Thus, DPO\nsimplifies the preference learning pipeline, making it an effective method for training LMs to adhere to\nhuman preferences.\nPython Library - HuggingFace TRL package supports the DPO Trainer6 for training language models\nfrom the preference data. The DPO training process requires a dataset formatted in a very specific\nmanner. If you are utilising the default DPODataCollatorWithPadding data collator, your final dataset\nobject must include three specific entries, which should be labelled as follows:\n Prompt\n Chosen\n Rejected\nHuggingFace offers datasets compatible with DPO and can be accessed here.\n6https://huggingface.co/docs/trl/main/en/dpo_trainer\n52\nFigure 6.12: Direct Preference Optimisation (DPO) Process Flow. This figure illustrates the Direct\nPreference Optimisation (DPO) technique used in fine-tuning large language models. The process begins\nwith preference data (Yw > Yl), where Yw represents preferred outputs, and Yl represents less preferred\noutputs. Through a maximum likelihood estimation process, this preference data is used to optimise\nthe models parameters, resulting in the final large language model (LLM). The method is designed to\nimprove the alignment of model outputs with desired user preferences, enhancing the models effectiveness\nin specific tasks. (adapted from [74])\n6.9.1\nBenefits of DPO\n1. Direct Alignment with Human Preferences: DPO directly optimises models to generate\nresponses that align with human preferences, thereby producing more favourable outputs.\n2. Minimised Dependence on Proxy Objectives: In contrast to methods that rely on next-\nword prediction, DPO leverages explicit human preferences, resulting in responses that are more\nreflective of human behaviour.\n3. Enhanced Performance on Subjective Tasks: For tasks requiring subjective judgement, such\nas dialogue generation or creative writing, DPO excels in aligning the model with human prefer-\nences.\n6.9.2\nBest Practices for DPO\n1. High-Quality", ". Minimised Dependence on Proxy Objectives: In contrast to methods that rely on next-\nword prediction, DPO leverages explicit human preferences, resulting in responses that are more\nreflective of human behaviour.\n3. Enhanced Performance on Subjective Tasks: For tasks requiring subjective judgement, such\nas dialogue generation or creative writing, DPO excels in aligning the model with human prefer-\nences.\n6.9.2\nBest Practices for DPO\n1. High-Quality Preference Data: The performance of the model is heavily influenced by the\nquality of preference data. Ensure the dataset includes clear and consistent human preferences.\n2. Optimal Beta Value: Experiment with various beta values to manage the influence of the\nreference model. Higher beta values prioritise the reference models preferences more strongly.\n3. Hyperparameter Tuning: optimise hyperparameters such as learning rate, batch size, and LoRA\nconfiguration to determine the best settings for your dataset and task.\n4. Evaluation on Target Tasks: Continuously assess the models performance on the target task\nusing appropriate metrics to monitor progress and ensure the achievement of desired results.\n5. Ethical Considerations: Pay attention to potential biases in the preference data and take steps\nto mitigate them, preventing the model from adopting and amplifying these biases.\n6.9.3\nTutorial for training models using DPO technique\nThe tutorial for DPO training, including the full source code of the training scripts for SFT and DPO,\nis available here.\n6.9.4\nIs DPO Superior to PPO for LLM Alignment?\nThe recent study on DPO superior to PPO for LLM Alignment[75] investigates the efficacy of reward-\nbased and reward-free methods within RLHF. Reward-based methods, such as those developed by Ope-\nnAI, utilise a reward model constructed from preference data and apply actor-critic algorithms like\nProximal Policy Optimisation (PPO) to optimise the reward signal. Conversely, reward-free methods,\nincluding Direct Preference Optimisation (DPO), RRHF, and PRO, forego an explicit reward function,\n53\nwith DPO focusing exclusively on policy optimisation through a logarithmic representation of the reward\nfunction.\nOne of the objectives of this study is to determine whether DPO is genuinely superior to PPO in the\nRLHF domain. The study combines theoretical and empirical analyses to uncover the inherent limita-\ntions of DPO and identify critical factors that enhance PPOs practical performance in RLHF.\nTheoretical findings suggest that DPO may yield biased solutions by exploiting out-of-distribution re-\nsponses. Empirical results indicate that DPOs performance is notably affected by shifts in the distri-\nbution between model outputs and the preference dataset. Furthermore, the study highlights that while\niterative DPO may offer improvements over static data training, it still fails to enhance performance\nin challenging tasks such as code generation. Ablation studies on PPO reveal essential components for\noptimal performance, including advantage normalisation, large batch sizes, and exponential moving av-\nerage updates for the reference models parameters. These findings form the basis of practical tuning\nguidelines, demonstrating PPOs robust effectiveness across diverse tasks and its ability to achieve state-\nof-the-art results in challenging code competition tasks. Specifically, on the CodeContest dataset, the\nPPO model with 34 billion parameters surpasses AlphaCode-41B, showing a significant improvement in\nperformance metrics.\n6.10\nOdds-Ratio Preference Optimization (ORPO)\nOdds-Ratio Preference Optimization (ORPO) is a novel approach designed to align the output of lan-\nguage models with desired responses by introducing a penalisation mechanism for undesirable outputs.\nUnlike traditional supervised fine-tuning (SFT) approaches, which focus solely on maximising the likeli-\nhood of correct responses, ORPO adds a specific odds-ratio based loss to penalise unwanted generations.\nThis technique provides a refined method for improving preference alignment without relying on a ref-\nerence model, making it efficient for large-scale implementations.\nGiven an input sequence x, the log-likelihood of generating an output sequence y of length m is\ncomputed as:\nlog P(y|x) = 1\nm\nm\nX\ni=1\nlog P(yi|x)\nThe odds of generating the output sequence y given input x is expressed as:\nodds(y|x) =\nP(y|x)\n1 P(y|x)\nORPO introduces an odds-ratio that contrasts the likelihood of generating a preferred (chosen) re-\nsponse yw with a less preferred (rejected) response yl, defined as:\nOR(yw, yl|x) = odds(yw|x)\nodds(yl|x)\nThe ORPO loss function incorporates two components:\n Supervised Fine-tuning Loss (SFT):\nLSF T = 1\nM\nM\nX\nk=1\n|V |\nX\ni=1\nyk\ni log pk\ni\nwhere yk\ni is a binary indicator for the i-th token in the vocabulary, and pk\ni is its predicted probability.\n Odds-Ratio Loss:\nLOR = log \n\u0012\nlog odds(yw|x)\nodds(yl|x)\n\u0013\nwhere  is the sigmoid function applied to stabilise the log odds ratio.\n54\nThus, the total ORPO objective is:\nLORP O = LSF T + LOR\nwhere  controls the strength of preference alignment.\nThis loss function effectively guides the\nmodel towards generating the chosen response while discouraging the rejected one, facilitating efficient\nalignment without the need for additional reference models [76].\nAdvantages of ORPO: ORPOs strength lies in its ability to perform preference alignment in a", " is the sigmoid function applied to stabilise the log odds ratio.\n54\nThus, the total ORPO objective is:\nLORP O = LSF T + LOR\nwhere  controls the strength of preference alignment.\nThis loss function effectively guides the\nmodel towards generating the chosen response while discouraging the rejected one, facilitating efficient\nalignment without the need for additional reference models [76].\nAdvantages of ORPO: ORPOs strength lies in its ability to perform preference alignment in a\nmonolithic manner, bypassing the need for separate phases of fine-tuning and preference optimisation.\nThis reduces computational overhead and provides state-of-the-art performance across various models,\nincluding LLaMA and Mistral, when evaluated on benchmark tasks such as AlpacaEval and MT-Bench\n[77].\n6.11\nPruning LLMs\nPruning LLMs involves eliminating unnecessary or redundant components from a neural network to\nreduce its size and complexity, thereby enhancing its efficiency and performance. This process assists AI\ndevelopers and engineers in addressing the challenges associated with deploying AI models in resource-\nlimited environments, such as mobile devices, edge computing, or embedded systems. Pruning AI models\ncan be achieved through various techniques, each suited to the type and structure of the neural network,\nthe pruning objective, and the pruning criterion. The following are common approaches:\n1. Weight Pruning: Involves removing weights or connections with minimal magnitude or impact on\nthe output. This method reduces the number of parameters and operations in the model, although\nit may not necessarily decrease memory footprint or latency.\n2. Unit Pruning: Eliminates entire units or neurons with the lowest activation or contribution to\nthe output. This technique can reduce the models memory footprint and latency but may require\nretraining or fine-tuning to maintain performance.\n3. Filter Pruning: Involves removing entire filters or channels in convolutional neural networks that\nhave the least importance or relevance to the output. This strategy also reduces memory footprint\nand latency, though it may necessitate retraining or fine-tuning to preserve performance [78].\n6.11.1\nWhen to Prune AI Models?\nPruning AI models can be conducted at various stages of the model development and deployment cycle,\ncontingent on the chosen technique and objective.\n1. Pre-Training Pruning: Leverages prior knowledge or heuristics to determine the optimal network\nstructure before training begins. This approach can save time and resources during training but\nmay necessitate careful design and experimentation to identify the best configuration.\n2. Post-Training Pruning: Involves using metrics or criteria to assess the importance or impact of\neach network component after training. This method helps maintain model performance but may\nrequire additional validation and testing to ensure quality and robustness.\n3. Dynamic Pruning: Adjusts the network structure during inference or runtime based on feedback\nor signals. This approach can optimise the model for different scenarios or tasks but may involve\nhigher computational overhead and complexity to implement and execute.\n6.11.2\nBenefits of Pruning\n1. Reduced Size and Complexity: Pruning decreases the size and complexity of AI models, making\nthem easier to store, transmit, and update.\n2. Improved Efficiency and Performance: Pruned models are faster, more energy-efficient, and\nmore reliable.\n3. Enhanced generalisation and Accuracy: Pruning can make models more robust, less prone\nto overfitting, and more adaptable to new data or tasks.\n55\n6.11.3\nChallenges of Pruning\n1. Balance Between Size Reduction and Performance: Achieving the optimal balance between\nreducing size and complexity and maintaining performance is challenging; excessive or insufficient\npruning can degrade model quality and functionality.\n2. Choosing Appropriate Techniques: Selecting the right pruning technique, criterion, and objec-\ntive for the specific neural network type and structure is crucial, as different methods can produce\nvarying effects and outcomes.\n3. Evaluation and Validation: Pruned models need thorough evaluation and validation to ensure\npruning has not introduced errors, biases, or vulnerabilities that could impact performance and\nrobustness.\n56\nChapter 7\nStage 5: Evaluation and Validation\n7.1\nSteps Involved in Evaluating and Validating Fine-Tuned\nModels\n1. Set Up Evaluation Metrics: Choose appropriate evaluation metrics, such as cross-entropy, to\nmeasure the difference between the predicted and actual distributions of the data.\n2. Interpret Training Loss Curve: Monitor and analyse the training loss curve to ensure the\nmodel is learning effectively, avoiding patterns of underfitting or overfitting.\n3. Run Validation Loops: After each training epoch, evaluate the model on the validation set to\ncompute relevant performance metrics and track the models generalisation ability.\n4. Monitor and Interpret Results: Consistently observe the relationship between training and\nvalidation metrics to ensure stable and effective model performance.\n5. Hyperparameter Tuning and Adjustments: Adjust key hyperparameters such as learning\nrate, batch size, and number of training epochs to optimise model performance and prevent over-\nfitting.\n7.2\nSetting Up Evaluation Metrics\nCross-entropy is a key metric for evaluating LLMs during training or fine-tuning.\nOriginating from\ninformation theory, it quantifies the difference between two probability distributions.\n7.2.1\nImportance of Cross-Entropy for LLM Training and Evaluation\nCross-entropy is crucial for training and fine-tuning LLMs. It serves as a loss function, guiding the model\nto produce high-quality predictions by minimising discrepancies between the predicted and actual data.\nIn LLMs, each potential word functions as a separate class, and the models task is to predict the next\nword given the context. This task is inherently complex, requiring"], "parameters": {"model": "nomic-embed-text:v1.5"}}, "key": "embeddings_63b762ff5803f6f2800db57670adc749ed3ba807578b2c619db17819118cded0_v2"}