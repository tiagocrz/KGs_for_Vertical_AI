{"result": {"data": [{"embedding": [0.021134362, 0.022644514, -0.15719339, -0.07771939, 0.028360061, -0.07178034, 0.036872294, -0.01279557, -0.0016900128, -0.036120977, 0.00040848023, 0.0035963184, 0.11096377, 0.032337435, 0.009320466, 0.019066, -0.011671057, -0.056912333, -0.031483997, 0.045545314, -0.024902515, -0.01479113, -0.005371926, -0.020688303, 0.0248819, 0.004827904, 0.026203262, 0.036228452, -0.01835709, -0.0031342278, 0.047148295, -0.069283724, -0.005104248, -0.051490456, 0.020474385, -0.051857207, 0.09966771, 0.01599617, -0.016147582, 0.09932524, 0.054026015, -0.03939011, 0.014125119, 0.021020865, -0.04195035, -0.0047693015, 0.038033865, -0.046530776, 0.0004438491, 0.01169621, 0.011194148, -0.009861009, -0.012404042, 0.013931029, 0.086873, 0.0029904288, -0.015557852, 0.010975476, 0.025267337, -0.05061181, 0.060997505, 0.024632592, -0.060558442, 0.06471034, 0.011250684, -0.022854278, -0.08735317, 0.030470265, 0.02060741, -0.006369859, 0.011469349, -0.008360776, 0.0580418, 0.03018362, -0.018051893, -0.04397729, -0.052176155, -0.046593778, -0.05345065, 0.02422489, 0.04520596, 0.009850814, 0.019687306, -0.007247232, 0.006027293, -0.012848284, -0.036422525, -0.01814305, -0.04827846, 0.069057375, 0.048970375, 0.0302187, 0.019308126, 0.013253642, -0.026167773, 0.008002944, -0.019079393, 0.038381588, -0.030779518, 0.017223038, -0.064146645, -0.020331696, 0.004537829, -0.028659167, -0.01576881, 0.07788406, 0.05971517, -0.01136813, -0.013994559, -0.015260456, -0.032515354, 0.019952554, -0.02691592, -0.0061645596, -0.022676159, -0.033613373, 0.08848059, -0.009776955, -0.008795221, 0.033348586, -0.040822387, -0.013298054, -0.0003187504, -0.0017224203, -0.012279241, -0.00954728, -0.057873365, -0.014816669, -0.020091103, 0.0004490944, 0.007861919, -0.03501025, -0.026671117, 0.029807465, 0.034123994, 0.07116904, 0.0026902407, -0.031537294, 0.039626986, -0.016683033, -0.00204819, 0.031515807, -0.037284292, -0.020918913, 0.009967094, -0.045128338, 0.027672864, 0.0005730788, -0.01924133, -0.013115373, 0.0010992768, 0.012114081, 0.021054655, 0.030315148, 0.030468263, -0.031429518, 0.015954554, 0.008430101, 0.011002902, 0.024139905, 0.053844035, -0.0013512941, -0.012982189, 0.022140345, 0.010255831, -0.010412898, 0.054999746, 0.03271374, 0.02996823, 0.039025147, -0.0019080462, -0.07052926, 0.011259767, -0.016733417, 0.070119634, 0.012675831, 0.05707269, -0.03222957, 0.022064105, -0.012017352, 0.069931455, -0.055482756, 0.0052613243, -0.017033849, -0.025442285, -0.027651718, -0.0039217905, 0.0029524479, -0.016654918, -0.035457496, 0.0029377728, 0.03465346, -0.067672275, -0.020530324, -0.057117682, -0.019333206, 0.071752, -0.06325502, 0.0053330003, -0.030728418, 7.115308e-06, -0.033692375, -0.034920767, 0.0626177, -0.07364638, 0.016487397, 0.022460092, 0.0005321126, -0.032551836, 0.0055261855, 0.03803268, -0.0016800255, -0.0010986483, 0.014668799, 0.0040225717, 0.015183869, -0.022899212, -0.01638875, -0.024418935, -0.0311118, 0.041926432, -0.018371338, 0.0053201765, -0.031835407, 0.03702855, -0.027932273, -0.047416274, -0.007868841, -0.048295856, 0.039649896, 0.025778482, -0.007201686, 0.025498826, -0.0118028065, -0.0038211206, -0.0011336333, -0.012257577, 0.09670086, -0.030468017, 0.06970224, 0.023795415, -0.0048876437, -0.019458707, -0.024975292, -0.07115214, -0.05074012, -0.02457325, -0.050957732, 0.030163625, 0.07459353, -0.011548194, -0.004992718, 0.018167736, 0.008567798, 0.008595523, -0.016063994, -0.023316042, -0.012887111, -0.00061445584, 0.027038567, 0.07371465, -0.0034736327, -0.028961383, 0.0074877315, -0.01581462, -0.05628796, 0.017752908, 0.020058062, 0.0022958876, 0.010437073, 0.0026760125, 0.026825784, -0.024028739, 0.009063061, -0.026640287, -0.005288087, -0.0024948248, -0.0047617117, -0.041775864, -0.0051419185, 0.010600649, -0.06499895, -0.03582168, 0.020912964, 0.0058073355, 0.02250321, -0.0022766641, -0.018679185, -0.029574867, -0.01078714, 0.03687381, 0.009349013, -0.0059167477, -0.02617576, -0.039893385, 0.0024068176, 0.02318843, 0.049968485, 0.022951381, 0.002853992, 0.038768005, -0.019239565, 0.07463694, 0.04884847, -0.009232159, -0.046692997, -0.0015726094, -0.03807231, 0.035021245, 0.03600927, -0.042052273, -0.00990019, -0.091090694, 0.055119067, 0.008997663, -0.037454758, 0.01586508, 0.008243925, 0.028099122, -0.0007245891, 0.06017594, 0.0006794773, -0.005057039, -0.020987729, 0.008076606, 0.057240933, -0.012714793, 0.00032035998, 0.009736055, 0.0064486032, 0.058036666, 0.03018481, -0.016081963, 0.0033162197, -0.0010757397, 0.004169172, 0.026369097, 0.02549998, 0.031284776, 0.0009146523, 0.078318566, -0.0621739, 0.012278803, -0.068786286, 0.006134919, -0.024094474, -0.027508508, -0.031237554, 0.024736775, 0.019742778, -0.040814042, -0.0121530155, -0.0250397, -0.03812265, 0.03778842, 0.00070626475, 0.030160867, 0.03544267, 0.007133344, -0.03492318, 0.044510018, -0.0022529855, -0.028532816, -0.0092020435, -0.00095202686, 0.010026935, 0.05172714, -0.007457652, 0.013490786, -0.013245055, -0.0073148063, -0.0033233168, -0.06418854, -0.007609868, -0.045977425, 0.031412818, -0.018303625, 0.010674853, -0.0020703922, -0.01678952, -0.009979498, -0.006792827, 0.042594027, 0.014543646, -0.006147436, 0.01091969, 0.06903211, -0.020947462, 0.016918965, 0.003438725, 0.024430541, -0.055121515, -0.0056707067, 0.017615281, 0.02287604, -0.010216271, 0.037413236, -0.014950192, -0.006422073, 0.051361196, -0.067775115, -0.06108336, 0.01163525, 0.0036401467, -0.0011576824, -0.0031928986, -0.06289999, -0.003890889, 0.0415868, 0.004331245, 0.04005899, 0.022317449, -0.010306015, -0.046807915, 0.023330012, 0.033160377, 0.02431683, 0.057118688, -0.0073347497, 0.03703072, 0.018599529, 0.072040215, -0.0044565783, -0.008508398, 0.03558157, 0.051686127, 0.0134573765, 0.06341506, 0.0117175, -0.091120206, 0.056897726, 0.01418003, 0.065553, 0.012998622, -0.05032245, 0.0022521087, -0.0322006, 0.0025653986, 0.026126558, 0.0036639632, 0.03874934, -0.049583554, -0.02225239, -0.030588023, -0.0049507525, 0.07407283, 0.019851426, -0.020037126, -0.027112106, 0.05440776, -0.01670752, -0.0018979885, 0.033255927, 0.042259343, 0.10415287, -0.013192572, 0.018382784, 0.012722296, 0.055211764, 0.046862166, 0.040591702, 0.009433915, -0.05566364, 0.041670207, 0.028260456, -0.046017665, -0.013643878, -0.026816659, 0.0522941, 0.040130015, -0.065355115, -0.007169508, 0.0040924037, -0.03574552, -0.034901522, 0.0024815334, -0.012999317, -0.00013637671, 0.042119946, 0.048151616, 0.003043096, -0.07115793, -0.06488717, -0.06833398, 0.006719375, 0.02399026, 0.037820973, -0.00045612027, 0.017569555, 0.019473758, 0.037212614, 2.9572555e-05, 0.022667237, -0.014565895, -0.002446518, 0.004019613, -0.008973205, 0.038339004, 0.048553567, -0.006731133, 0.022226209, 0.04073691, -0.04438662, -0.030269923, -0.006981249, -0.015430936, 0.009278914, -0.071002506, -0.062368784, 0.04217961, 0.026900627, -0.025884587, -0.03898987, 0.006037153, 0.021793183, -0.09069371, 0.009013183, 0.051986348, 0.0216137, -0.005024101, 0.016121319, -0.04352131, -0.017680623, -0.02113085, -0.061660264, 0.017964551, 0.067957245, -0.055228002, 0.06166123, 0.015143148, 0.035750393, 0.05146158, -0.04466682, 0.010137589, -0.045394067, 0.06459377, -0.0525196, 0.02271938, 0.06406852, 0.02133754, 0.015384026, -0.009585142, 0.03343675, -0.028628636, 0.0403383, -0.026550217, 0.0028797023, 0.006684782, -0.024189355, -0.019234927, 0.019121552, -0.050699636, 0.00029420413, -0.0040096818, 0.012486341, -0.035870135, -0.0028848576, -0.0052730283, -0.026840176, -0.04048934, -0.013147527, -0.007436335, 0.0474393, 0.049450565, -0.022354785, -0.02637567, -0.00019074281, -0.00745931, 0.013884141, -0.0025926451, 0.023269257, -0.06619209, 0.05101062, -0.017082974, -0.023876218, 0.021429995, 0.03238712, 0.059531175, -0.07164774, -0.034579754, 0.008839358, -0.0375065, 0.0069183325, -0.016398834, 0.054884538, 0.025059963, -0.058507018, -0.030072486, -0.037440717, -0.002892476, -0.014207429, -0.019644005, 0.009281226, -0.05851314, 0.05997426, -0.022700327, 0.012208805, -0.015535119, -0.060042407, -0.034136113, -0.00919548, 0.004727015, 0.04995406, -0.035254966, -0.014805952, 0.045608025, -0.005648189, 0.021835053, 0.034756508, -0.02223461, 0.008088632, 0.0027535926, -0.029785065, -0.0050704926, 0.02779586, -0.016445236, 0.03592791, 0.011301323, -0.0029868898, -0.01983388, -0.065277934, -0.053371467, 0.030444873, -0.07426942, -0.013185106, -0.07532302, -0.008578728, -0.0054760664, 0.048623104, 0.036474798, -0.03721912, -0.07213152, -0.064903036, 0.010567526, -0.049726225, 0.012706342, -0.025245719, -0.005998214, 0.05813321, 0.024776952, 0.012891164, -0.035410807, -0.011084924, -0.017808639, 0.045453787, -0.011432356, 0.023475127, 0.052202474, 0.046249624, -0.042021494, 0.11359318, 0.08050839, -0.0043575857, -0.029370595, -0.03169641, 0.010894967, -0.005559258, -0.046336126, -0.022832911, -0.0338101, -0.018532854, 0.021800265, -0.04725112, -0.00959242, 0.034070984, -0.0062174588, 0.003404279, 0.00929909, -0.03108355, -0.021744588, 0.019611817, -0.0040513664, -0.041368756, 0.03700084, 0.044318423, -0.012950005, 0.054864086, 0.033730492, 0.04991388, 0.0009274516, 0.05112066, 0.012895128, 0.036586054, -0.010862523, 0.01786693, -0.055725943, 0.011782012, -0.0077397483, -0.040939786, -0.026620017, 0.0017448162, -0.018260928, -0.014282307, 0.039334767, -0.034785524, 0.0102224955, 0.0027720341, -0.017596325, 0.0045874724, 0.06991811, -0.027260344, 0.017680217, -0.030204322, 0.0070057395, -0.038408257, 0.0022194404, 0.054966513, -0.0013991236, 0.013870446, -0.09338026, -0.055185013, 0.02083584, -0.03909027, 0.012650782, 0.013270782, 0.032550603, -0.02681251, 0.026101144, -0.02278999, 0.014951069, 0.030087398, 0.021394582, -0.10315064, 0.02602373, 0.05076159, -0.009193565, 0.012641179, -0.07588091, -0.033439543, -0.022326175, -0.05046113, -0.008236259, -0.053993724, 0.036954403, 0.035534058, 0.00021285021, 0.0072055305, -0.054441698, 0.005558922, -0.036821723, 0.012425784, 0.008352208, 0.01992026, 0.017142344, -0.020631216, 0.052159563, -0.016374663, 0.041354414, 0.028710041, -0.053586, -0.035402723, -0.0016203473, -0.009295523, -0.0030957698, -0.00026071852, 0.028570188, -0.0077360924, 0.0013596852, 0.1290982, 0.014400447, -0.021754127, -0.022208326, 0.075066015, -0.043458465, -0.020592887, -0.07481394, -0.025361758, 0.001169758], "index": 0, "object": "embedding"}, {"embedding": [0.0115274135, 0.022263372, -0.1429217, -0.082143225, 0.055425525, -0.039455593, -0.022134004, 0.025911387, -0.043404765, -0.010805306, -0.04172163, 0.014293597, 0.02291677, 0.068510324, 0.021396205, -0.025655998, -0.013485667, 0.002069764, -0.04475559, 0.035847124, -0.008260975, -0.009230691, -0.008890086, -0.083357714, 0.06616811, 0.021401186, -0.023139196, 0.049038395, -0.070894785, -0.02092793, 0.07464076, -0.080847956, 0.011313684, 0.030469334, -0.058921658, -0.053685114, 0.023386244, 0.012464633, -0.06726467, 0.053790282, 0.08319286, -0.026544306, 0.0055702683, -0.004823321, 0.03270883, -0.055607498, 0.069461204, -0.041555285, 0.029529974, -0.08015209, 0.03124937, -0.054009855, -0.017092658, 0.0310619, 0.059727278, 0.024041872, -0.049263332, -0.019017188, 0.0034983063, -0.026259875, 0.06533982, 0.0643038, -0.08124975, 0.056722824, 0.054346055, 0.0059423856, -0.03574408, 0.021570617, -0.006975574, -0.02849529, 0.075268686, 0.008433988, -0.026043413, 0.031443324, -0.010403319, -0.012397841, -0.03748548, -0.034808226, -0.021705382, 0.06781584, 0.030891217, -0.031656913, 0.07031979, -0.003941275, 0.015803546, -0.034712173, -0.06652173, -0.0045813937, -0.056309875, 0.073973276, 0.0014865461, 0.029027928, 0.00089217786, 0.012645925, -0.08012313, 0.027005121, -0.05515782, 0.021027999, -0.018394535, -0.031862296, -0.072669335, -0.022628225, 0.030201884, -0.007849922, 0.029836496, 0.059148148, 0.021512121, -0.021192634, -0.031406768, 0.007875156, -0.029363077, 0.027870996, -0.020793281, -0.0059202476, -0.049951416, -0.045058977, 0.059088238, 0.02579101, -0.0057126293, 0.0626405, 0.06043585, -0.022416053, 0.015352506, -0.018132713, -0.017322054, 0.012488857, -0.032775003, -0.0031159276, -0.025103394, -0.050488297, 0.02979427, 0.015288785, -0.0032675492, -0.016543906, 0.04302587, 0.09536765, -0.02633048, -0.01702881, 0.031491075, -0.011164154, 0.031500015, -0.008753097, -0.043300938, 0.0051838956, -0.020362426, -0.06178743, 0.0016943646, -0.016985185, -0.024251042, -0.019535666, 0.018403796, 0.020539556, 0.031831212, 0.058006056, 0.002087431, -0.0039676707, 0.025226707, 0.018833984, -0.019017035, 0.018840855, 0.058094777, 0.041525908, -0.002033681, 0.03838241, -0.011297956, -0.04463157, -0.0018669076, 0.009309088, 0.03651285, 0.04012186, -0.053860042, 0.0047631576, -0.032628395, 0.044017807, 0.046500936, 0.017234223, 0.0309665, -0.07027029, -0.02177621, -0.023306845, 0.055883955, -0.048625346, 0.014288574, -0.030307945, -0.0270403, -0.040295977, -0.016599271, -0.05004934, 0.022680094, -0.012804903, -0.057398323, 0.041574206, -0.05270634, -0.012350768, -0.043133132, -0.024980346, 0.028151482, -0.03609739, 0.047145877, -0.03725551, -0.024488062, -0.004170356, -0.048124027, 0.003290399, -0.04124857, 0.054439694, -0.013726299, 0.027103527, -0.03743255, 0.026970947, 0.082493365, -0.03287179, 0.011541707, -0.011998131, 0.005234384, 0.028781448, -0.03145658, -0.03781733, 0.019221501, -0.017712543, 0.04448781, -0.0015077959, -0.005077818, 0.02234338, -0.021596275, -0.025944838, -0.03279921, 0.018002823, -0.0304835, -0.0010936072, 0.03785897, -0.03195262, 0.022079337, -0.017182762, -0.0044308957, 0.018375268, -0.022957737, 0.08479023, -0.031695135, 0.026132956, 0.04611311, 0.027419295, -0.05070204, -0.009373683, -0.059608333, -0.003248214, -0.014287881, -0.05268344, -0.0050458685, 0.07763116, -0.03183053, -0.007783679, 0.033729117, -0.025153782, 0.054101422, -0.01474334, -0.003064355, 0.027297739, 0.021641692, 0.011357569, 0.0384537, -0.049927898, -0.011658656, -0.024698317, -0.023705876, -0.030641254, 0.026252994, 0.031827692, 0.01080603, 0.003138699, -0.01396443, 0.047576968, 0.0013044586, 0.022122927, -0.0030938499, 0.031679004, -0.0012323902, 0.01316578, -0.009121368, 0.0032247605, -0.044811174, -0.066658616, 0.017145813, -0.047943972, -0.005518924, 0.042651806, 0.00051186205, -0.015186685, 0.010466492, 0.0052237725, 0.013089363, -0.015807219, -0.032336306, -0.020317469, -0.019359542, 0.039148882, 0.020682143, 0.024705766, 0.014357337, -0.00765645, 0.01750741, -0.011048666, 0.08092608, 0.040571764, -0.022439867, -0.027664633, -0.01150889, 0.0019779657, 0.056822438, -0.008294934, -0.049643278, -0.0113472575, -0.037834466, 0.0046340115, -0.034651365, 0.005398414, 0.04675393, 0.016826889, 0.063189566, -0.0031330504, 0.023048691, -0.052121274, 0.034986414, -0.06402861, -0.011748824, 0.04849533, 0.024105329, 0.01838851, -0.022849176, -0.018313782, 0.034882765, -0.029186765, 0.03965689, -0.02248882, -0.0074039665, 0.0077040186, -0.022298997, 0.036415268, 0.03544552, -0.0077263187, 0.06896899, -0.02138842, 0.023933413, -0.017420657, -0.04773647, -0.012755327, -0.06408162, -0.015642982, 0.05545801, 0.0016881947, -0.030640645, 0.01673495, 0.003192135, -0.039801206, -0.020834696, 0.0074307914, 0.034973506, 0.039000534, -0.006623649, -0.025740951, 0.046356335, -0.0298701, 0.03194429, -0.022711188, 0.011211517, -0.014789227, 0.08805062, 0.0049129673, 0.0028033303, 0.0070455293, 0.046362903, -0.025614534, -0.064450696, -0.009561944, -0.025982881, -0.0370309, -0.08969219, -0.016294131, -0.01717057, 0.018689938, 0.014778011, -0.00042746303, 0.01586376, -0.0064059948, 0.023666179, -0.05993995, 0.011407059, -0.017751725, -0.015936565, -0.05254945, -0.013985694, -0.045903973, -0.03941727, -0.02752058, 0.0040118005, -0.045149997, 0.047428582, -0.017708182, -0.029500222, 0.06506364, -0.029070994, -0.0104008075, -0.0065834, 0.023364084, 0.0407729, 0.009328423, -0.018710764, -0.036458142, 0.009194674, -0.027562728, -0.013800908, 0.044141263, -0.013598502, -0.02783631, -0.016424878, -0.014478142, 0.05530339, 0.0050027496, 0.02520307, 0.03676927, 0.0067847134, 0.042252414, -0.0057914336, 0.0094746575, 0.012993732, 0.02183552, 0.053564955, 0.038043406, -0.03819463, -0.07582138, 0.023007726, -0.019574292, 0.06966359, 0.020433327, -0.022100413, 0.027101712, -0.043180346, 0.042447437, 0.027524961, 0.013779898, 0.016941663, -0.105800845, -0.014837826, -0.046780705, 0.0470293, 0.08521858, 0.044347078, -0.038359795, -0.0409174, 0.011877561, -0.020538388, 0.027649775, -0.00858, 0.045950033, 0.12531206, 0.0049041077, 0.04570187, -0.009059958, 0.042727605, 0.021741807, 0.027393399, 0.009711637, -0.028007412, 0.013849348, 0.0077668363, -0.031246461, 0.020523993, -0.037882376, 0.04822327, 0.01266962, -0.035452094, -0.031660724, -0.023680849, 0.008285818, -0.011489399, 0.00500934, -0.04614957, 0.0064463415, 0.0431029, 0.025187874, 0.011723856, -0.043167684, -0.02501339, -0.03153361, -0.038563736, 0.052060645, 0.008522177, 0.00068110414, -0.019025782, 0.020185199, 0.024219308, -0.012547819, 0.005985763, -0.024020439, -0.04027741, 0.0075805862, -0.03181714, -0.018699253, 0.016397085, 0.035811894, 0.039806746, 0.018514559, -0.00575156, -0.037168425, 0.015257335, -0.026900513, 0.00440196, -0.029163582, -0.057007745, 0.05473942, 0.040404838, -0.007149381, -0.03642396, 0.023345374, 0.024572218, -0.0639761, 0.04454274, -0.0038978327, -0.051425733, 0.043326687, -0.0015855609, -0.07628591, 0.013402148, -0.021152431, -0.07706633, 0.028479552, 0.07800862, -0.06904216, 0.028523939, 0.0041300836, 0.02908798, 0.04683274, -0.009937036, 0.0040269373, 0.033012506, 0.018450005, -0.04043725, 0.05597401, 0.027017301, 0.051077, -0.013708767, -0.017379208, -0.021158528, 0.03731945, 0.056250125, 0.009560753, 0.00035399018, 0.037534673, -0.03822342, -0.06618625, 0.0144989565, -0.05504812, 0.014627596, -0.015865352, 0.05065708, -0.05973293, 0.0062392754, -0.006859759, -0.026608191, -0.022789514, -0.0030220777, 0.015759222, 0.08614319, 0.021718424, 0.0023133198, -0.025473867, -0.029041136, -0.0150843905, 0.005205261, 0.0646105, 0.0663675, -0.02197483, 0.018298225, -0.0025004763, -0.02164912, -0.021126604, -0.002267135, -0.018795285, -0.03344106, -0.025220796, 0.00929318, -0.043340597, 0.0050190734, 0.016391842, -0.004984032, 0.0073113744, -0.07860592, -0.018880496, -0.007868161, -0.027256142, 0.009404391, -0.034462847, 0.007276322, -0.03673333, 0.017410846, -0.018738305, 0.01593143, -0.023828752, -0.024905855, -0.08611005, -0.007961454, 0.049116645, 0.033826888, -0.024139332, 0.021417143, 0.05622577, -0.029728768, 0.0036123088, 0.018054789, -0.02605127, -0.007625411, -0.012187591, 0.013964553, -0.014621187, 0.033998262, 0.0021808639, 0.035053, -0.035826765, -0.026355803, -0.013699244, -0.015595803, -0.04210932, 0.051460743, -0.030243978, -0.008275958, 0.012653685, -0.041388176, -0.007597844, 0.003724503, 0.04433463, -0.041454814, -0.03262132, -0.012636095, -0.016734673, -0.060976896, 0.037405696, 0.00964748, -0.017204458, 0.028115835, 0.056743447, 0.018868072, 0.005046838, 0.021622343, 0.05655467, 0.046449378, -0.004146737, 0.03923903, 0.053510644, 0.023032092, -0.03474228, 0.055400662, 0.07383791, 0.013612033, -0.0049771867, 0.012736461, 0.012311836, 0.014791878, -0.009712795, -0.0014981278, 0.005471533, 0.024501113, -0.04427427, -0.00933753, 0.026423022, 0.050286923, -0.0041124015, 0.030916246, -0.0015840331, 0.014575677, -0.021038504, 0.052311387, 0.039837863, -0.0047449316, -0.024770562, -0.008793277, 3.6109302e-06, 0.021422202, 0.027903132, 0.027537424, -0.0014676052, 0.0481934, 0.006210288, -0.0010660557, -0.033249203, 0.042071816, -0.04964715, 0.048099402, 0.024552964, -0.06280917, -0.011790308, -0.0047485796, -0.014261449, -0.021384275, 0.019562272, -0.008388286, -0.0030824263, -0.009819991, 0.022793543, -0.02954358, 0.116009206, -0.014379388, 0.038172726, -0.03013254, 0.05379587, -0.003758008, -0.018723775, 0.06203455, -0.014730155, 0.019217728, -0.07836169, -0.05186474, 0.011748761, -0.052464742, 0.07089616, 0.019819552, 0.032063853, -0.013661242, 0.008652702, -0.031154154, 0.009474831, 0.036085013, -0.013737386, -0.031203866, 0.010609418, 0.03363627, -0.0018664392, 0.023749318, -0.042140584, -0.0024214487, -0.008939399, -0.043067057, -0.041417748, 0.007026063, 0.04454048, 0.0059358836, 0.026071288, 0.017791893, 0.035604816, -0.04752007, -0.0114288, -0.031873822, -0.0010622267, -0.026748639, 0.036216985, -0.012161335, -0.0342642, -0.034685925, 0.081405826, 0.028249547, -0.055155236, -0.0054448936, -0.007609611, 0.004656964, 0.0024779057, 0.007865452, -0.014348429, 0.011981309, 0.04956943, 0.1335866, -0.017551119, 0.0032021515, -0.0119691845, 0.034134243, -0.025905201, -0.08929674, -0.0015299398, -0.028108297, 0.0018089669], "index": 1, "object": "embedding"}, {"embedding": [0.022537598, 0.0038335912, -0.17972548, -0.058966607, 0.090753384, -0.011215598, 0.0355028, 0.0552522, -0.03286999, -0.015188692, -0.03561741, 0.018784633, 0.07108566, 0.05158581, -0.0009299376, -0.0154887615, 0.031626277, -0.085241154, -0.033295877, 0.03476475, -0.024308857, -0.018105457, 0.016500847, -0.006085411, 0.035984956, 0.021882938, -0.032971147, 0.029899755, -0.020566696, 0.005717481, 0.046016723, -0.021173421, -0.005099244, 0.014394204, -0.045169458, -0.03698102, -0.0002527004, 0.012671674, -0.054615106, 0.07244261, 0.06242519, -0.03630641, 0.006991823, -0.023661101, 0.034976196, -0.019227538, 0.05057149, -0.048131045, 0.014142889, -0.06944241, 0.038688485, -0.006196809, -0.034159992, 0.019583495, 0.087758265, 0.03938405, -0.03571234, 0.009384064, 0.026576439, -0.095602155, 0.0777938, 0.025818178, -0.10977931, -0.0022767105, 0.006348993, 0.010349983, -0.050175678, 0.035176378, 0.026844613, -0.011853974, 0.041738205, 0.02692495, 0.008747311, 0.0155809, -0.057539906, -0.022868862, -0.040979795, -0.04433675, -0.03270454, 0.047213253, 0.01627822, 0.0013762128, 0.02921531, -0.007696218, 0.044374257, -0.0019552128, -0.080819786, 0.033184003, -0.019553628, 0.088513955, -0.0021482303, 0.025738744, 0.03939092, 0.02806218, -0.00601569, 0.044346754, -0.027203416, 0.054436836, -0.015438534, -0.024433311, -0.023882821, -0.00097571546, 0.016269058, -0.042496204, -0.020088492, 0.051197793, 0.019396732, -0.005386975, -0.0016600934, -0.006572187, -0.017331203, 0.03534539, -0.02169654, -0.020158717, -0.033346605, -0.070358366, 0.054928757, 0.016801672, -0.027146349, 0.025611164, 0.01575653, -0.04012399, -0.023500413, -0.022489712, -0.00957434, 0.039686177, -0.04189036, -0.0038874985, -0.0440136, -0.020245481, 0.025868688, -0.014572506, -0.044135906, 0.009355723, 0.054777846, 0.10642637, -0.02009393, -0.009864262, 0.050036237, -0.0153802885, 0.0021136592, 0.021236496, 0.013703605, -0.014728292, -0.021715593, -0.032837447, -0.0018248137, -0.012160909, -0.051801994, 0.020153789, 0.019212296, 0.02775206, 0.029744651, 0.014103615, 0.020248843, -0.012173446, 0.0042766333, 0.014298808, 0.014196131, 0.027391061, 0.0403142, -0.0035387138, -0.012823892, 0.08063644, -0.007831871, -0.039994657, -0.01524026, 0.04422392, 0.023434948, 0.010142181, -0.051471926, -0.039080188, 0.036461398, 0.027582126, 0.017854761, 0.0014467784, 0.05814919, -0.045756117, 0.0025728967, -0.033701137, 0.06889329, -0.061549205, 0.026345398, -0.023420298, -0.032680076, -0.030378513, 0.016855247, -0.05052092, 0.01616051, -0.026178353, -0.018289855, 0.01882231, -0.08356887, -0.03248308, -0.032962102, -0.043732792, 0.050621495, -0.048301224, 0.043731626, 0.0076589705, -0.024422893, 0.0028646311, -0.030272543, 0.003545805, -0.041152418, 0.05644112, 0.008869548, -0.008191316, -0.020626295, 0.031847417, 0.067713976, -0.03203955, 0.043848567, 0.015101265, 0.001003476, 0.01641991, -0.002598227, -0.012628118, -0.01751165, -0.0023819853, 0.023381967, -0.005076202, 0.0049366583, 0.013600006, -0.002164535, -0.05769821, -0.03533583, 0.016731335, -0.008793571, -0.01054572, 0.038454913, -0.062966265, 0.027021589, -0.023177192, -0.0073640165, 0.00053561473, -0.0054083946, 0.02409651, -0.03243618, 0.01784506, 0.015485827, 0.04182662, -0.022890782, -0.035895593, -0.08095527, -0.043602217, -0.019411862, -0.0394764, 0.00993854, 0.0059127337, -0.062771834, 0.021651931, 0.07596678, -0.029963423, 0.026842529, -0.029698128, -0.071912006, 0.045746397, 0.051286247, 0.016862018, 0.036564972, 0.016431337, -0.0022918477, -0.026977342, -0.07297476, -0.054330055, -0.008676609, -0.009287919, 0.018654868, -0.027157767, 0.034254268, 0.04205776, 0.020250358, 0.020642418, -0.027092554, 0.052140333, 0.027010558, -0.010515456, -0.008262626, -0.0054137446, -0.06537408, -0.05219365, -0.037369173, -0.028090691, 0.0023909677, 0.027924024, 0.02172736, -0.017085178, -0.005827527, -0.027448062, 0.003214677, 0.010274229, -0.0081464425, 0.021696622, -0.02091206, 0.09351187, 0.028382836, -0.01137756, 0.040760532, -0.000646336, 0.02105428, 0.00020569163, 0.0708023, 0.040562276, -0.027846577, -0.032756176, -0.022698682, -0.0050219954, 0.058516145, 0.04071481, -0.063083574, -0.022534877, -0.060870066, 0.009496984, -0.035056897, -0.028917985, 0.021451082, 0.018905891, 0.03538051, 3.7428403e-05, 0.015555954, -0.024340939, -0.01416582, -0.050032504, 0.008274968, 0.034516573, 0.028626334, 0.021802437, -0.04609059, -0.02508391, 0.08131821, 0.013896736, -0.006241746, -0.041083172, -0.03414577, 0.011123637, 0.0061268588, 0.011577238, -0.017490417, 0.013536673, 0.04964164, -0.046421804, 0.019676233, -0.009393567, -0.022339744, -0.011267686, -0.017041687, -0.03127094, 0.020290593, 0.026522258, -0.024615906, 0.009797963, -0.042283695, -0.04873934, 0.045243178, -0.026465422, 0.029503731, 0.013760095, -0.03941098, -0.046813592, 0.069230214, -0.037958127, 0.00072541955, -0.027185926, -0.010491414, -0.04935999, 0.07290931, 0.008240371, 0.023137758, 0.04955649, 0.0058332966, -0.008061883, -0.052180283, 0.0067051025, -0.053998776, -0.018259263, -0.056220576, 0.010773925, -0.002404872, 0.021908797, -0.0071651246, -0.02292351, -0.0027913705, -0.055882256, 0.010975792, -0.042215288, 0.04151755, -0.03472686, 0.006080212, 0.021819867, 0.0036069711, -0.06807573, -0.02950175, 0.01229887, 0.03677568, -0.03465161, 0.02209825, -0.0021829137, -0.01816342, 0.034703106, -0.010345565, -0.02725785, 0.032192856, 0.015823107, -0.012320757, 0.013586861, -0.05463723, 0.012342004, 0.0016369285, 0.02575384, 0.04709088, 0.05540561, -0.024847068, -0.055963688, 0.003999154, -0.005688973, 0.03941899, 0.0103666, -0.012719048, 0.0004748223, -0.010232306, 0.033052664, -0.010936495, -0.011748818, 0.015194539, 0.049660612, 0.044026256, 0.023347143, -0.0013074416, -0.059716545, 0.0709612, -0.02151356, 0.07067896, 0.0067856354, 0.0076763295, 0.041055165, 0.01908698, -0.02881667, 0.005964603, 0.056141756, 0.0045339144, -0.03745533, -0.034412857, -0.011864428, -0.011577668, 0.036847383, 0.0690805, -0.02644681, -0.047329094, 0.051310427, -0.004250006, 0.01949493, 0.04665982, 0.016295101, 0.11611488, 0.03469904, 0.06148417, 0.036329962, 0.0048749372, 0.015864747, 0.060742546, 0.040514078, -0.028550114, -0.0066601913, 0.012311174, -0.03213475, -0.014009944, -0.03377116, 0.059571978, 0.031785697, -0.07459305, -0.025088761, 0.022719214, -0.0019542638, -0.020085776, 0.020910757, -0.048464127, -0.0040903077, -0.006855068, 0.033704836, 0.052110344, -0.040053178, -0.04354835, -0.02113747, -0.016752927, 0.02194654, 0.032145567, -0.0017787705, -0.0036646265, -0.0077039558, 0.06204615, 0.042803593, -0.005691688, -0.015061992, -0.03750975, -0.034490228, -0.013956365, 0.011229253, 0.014221962, -0.014389831, 0.035786957, 0.017804032, 0.007074238, -0.019860543, -0.00311607, -0.030902952, 0.02959713, -0.029271193, -0.07228294, 0.07583786, 0.0055674165, 0.016220957, -0.003096823, 0.03371235, 0.0461006, -0.053589456, -0.00736216, 0.04788925, -0.019704161, 0.018437942, 0.016288403, -0.03940781, -0.0247495, 0.009350755, -0.054997824, 0.014461322, 0.046853527, -0.042253967, 0.0077119335, -0.03343462, 0.03733603, 0.039587293, -0.07794586, 0.014937771, 0.035517655, -0.00038384713, -0.041476186, -0.0013183904, -0.016839046, -0.0045468984, -0.04381675, 0.04315318, 0.0059459913, -0.007399124, 0.046103206, 0.017471343, 0.015542077, 0.03218501, -0.0131557295, -0.06399711, -0.014139883, -0.046143025, -0.014429607, -0.0075004864, 0.080336705, -0.022289956, 0.0070273355, -0.054542497, -0.013736272, -0.030976582, -0.04301531, 0.0005412069, 0.08150719, 0.005205324, 0.01583451, -0.03603176, -0.027496792, -0.015086628, 0.014730239, 0.04007558, 0.013713353, -0.080728136, -0.013557695, -0.0026449636, -0.016487278, 0.013295451, 0.022498706, -0.0023569562, -0.035326142, -0.031100888, 0.0052605006, 0.007938723, 0.03590132, 0.01871421, -0.011447631, 0.03443065, -0.078145795, -0.031897463, -0.009919247, -0.018512232, -0.0145517355, -0.054044243, 0.0045693335, -0.017434442, -0.0019455923, -0.0047926335, 0.020333841, -0.026500482, -0.056717925, -0.05021188, -0.010589773, 0.025552152, 0.069094576, -0.04829952, -0.0061861835, 0.018710002, 0.0014925745, 0.060840998, 0.008424801, 0.011480959, 0.016549291, -0.021326948, -0.0026960631, -0.024791708, 0.014665051, -0.025524223, 0.09395837, -0.00048672056, -0.0029114103, 0.01377725, -0.030756302, -0.03716621, 0.07052947, -0.05537273, 0.009431829, -0.036010258, -0.046232425, -0.021523349, 0.014890938, 0.049711898, -0.02444008, -0.008025615, -0.05727838, -0.0072380914, -0.051843498, 0.0726057, -0.012652287, 0.03637562, 0.0016807377, 0.063129336, 0.050409734, 0.006087917, -0.0045747776, 0.021425577, -0.0052754823, 0.025822932, 0.026247308, 0.112628415, 0.010632311, -0.008248521, 0.05079988, 0.051639877, -0.007025338, -0.03597639, 0.011205497, 0.05340572, 0.0019624971, 0.012730837, 0.00026698035, -0.0025464946, 0.0370942, -0.021169916, 0.020118998, 0.016896565, 0.09094886, -0.0053747003, -0.018783467, -0.025193986, -0.015450388, -0.037850585, 0.046982855, 0.023763128, -0.011758863, -0.01952136, 0.07359407, -0.014511048, 0.035665352, 0.054270133, 0.019129427, 0.0103736855, 0.0043508713, -0.015373453, 0.0064143566, -0.015356085, 0.014638619, -0.037434082, 0.011994311, -0.011684818, -0.07000019, -0.03225755, -0.020853084, -0.020964697, -0.035956267, 0.038823355, -0.054572154, 0.04045172, 0.018554667, -0.002388847, -0.017272966, 0.08326157, -0.007446317, 0.03631149, -0.002627828, 0.013130183, -0.039249733, -0.03630472, 0.0075685787, -0.01391273, 0.021871246, -0.08697686, -0.010776715, 0.012121322, -0.029720895, 0.057308026, 0.0016718885, 0.027670324, -0.015527755, -0.014098042, -0.0070814164, -0.012624166, 0.01914046, -0.022029897, -0.02109295, 0.012768484, 0.030905405, -0.050386015, -0.010110802, -0.049681902, -0.010734358, -0.001413241, -0.046277717, -0.033291608, -0.020307254, 0.018085431, 0.0114298845, -0.011936427, 0.0025215878, -0.007698951, -0.06462916, 0.00315237, -0.031707525, 0.032894798, -0.011889379, 0.037261542, 0.03048316, -0.034003776, -0.008068844, 0.05143359, 0.0052898885, -0.027909998, -0.00019435685, -0.0072839735, -0.008845149, 0.047368124, -0.06447635, 0.01882744, 0.021870872, 0.04285506, 0.11142176, -0.032996606, 0.003255021, -0.034666717, 0.079049796, 0.024251122, -0.06518095, 0.0057288944, 0.032137938, -0.02141678], "index": 2, "object": "embedding"}, {"embedding": [0.045865625, 0.07622113, -0.16950274, -0.081310846, 0.052390784, -0.078073665, 0.025810463, 0.008378547, -0.02523686, 0.035398427, 0.0023022576, 0.042364713, 0.08748845, 0.032818925, 0.024170242, 0.042751335, -0.036060702, 0.0034705852, -0.009120365, 0.015760707, -0.002390979, -0.060704757, 0.011624595, -0.04316562, 0.021982068, -0.0050113243, -0.023963496, 0.012236858, 0.040383197, 0.013150999, 0.058929436, -0.050610766, 0.0071003903, -0.062626585, 0.014640529, -0.0443724, 0.08416168, -0.0090332385, 0.050887298, 0.04178906, 0.052873682, -0.08139542, 0.04386531, -0.012312881, 0.0126371, -0.019926215, 0.04073359, -0.03498187, 0.038723335, -0.043131024, 0.02647113, 0.0056661977, -0.006042238, 0.015553092, 0.050334617, 0.038929157, -0.033048466, 0.050193567, 0.039259817, -0.05096593, 0.017449373, -0.02601641, -0.09060815, 0.090276495, -0.035348814, -0.04061479, -0.054994453, -0.007816087, -0.023899373, -0.07083446, 0.03938982, 0.010293441, -0.012108925, -0.004596333, -0.044028927, 0.040585443, -0.04557085, 0.002604277, -0.018604519, -0.00089964696, -0.009970112, 0.025718192, 0.04737894, -0.014851293, 0.023199929, 0.03311221, -0.024603056, -0.012786795, -0.016937561, 0.064977705, -0.015787022, 0.019510757, 0.039263014, 0.04958298, -0.020447444, 0.019113926, -0.016373772, -0.0013478167, -0.012652316, -0.024215143, -0.046730306, 0.0064599514, -0.015724674, -0.0041581453, -0.014695693, 0.06436545, 0.010356468, -0.028317323, 0.004713865, 0.056779724, -0.019336369, 0.008851532, -0.037746403, -0.04113031, -0.033960737, -0.053230572, 0.09525646, -0.021511847, 0.023824908, 0.013434529, -0.043536164, -0.025049519, 0.02807751, -0.000729636, 0.037503824, 0.026463578, -0.062144045, 0.023673669, -0.016374562, -0.047956213, -0.02212347, -0.021020235, -0.040274948, 0.02731047, 0.0027247218, 0.090522744, -0.06103326, -0.028618552, 0.028370902, 0.043395773, 0.006757714, -0.018977981, -0.0001384353, -0.025261363, 0.024765125, -0.0053177904, -0.00717078, -0.00047170112, -0.024423586, -0.006412808, 0.030265376, 0.053642534, -0.011792358, 0.038090304, 0.0045989305, 0.0035835123, -0.001533162, -0.010642198, 0.002708362, -0.026764091, 0.016090138, -0.014403844, -0.031835113, 0.0379091, 0.0077908104, -0.002678895, 0.041676052, 0.022503002, 0.009086211, 0.055493418, -0.031822745, -0.06194158, 0.019383255, -0.033329666, 0.065569535, 0.0044076545, 0.07407108, -0.039791953, -0.008066797, 0.0024557102, 0.05899362, -0.047131825, -0.007279635, -0.002159912, -0.0047311797, 0.023081603, 0.016736664, 0.023018437, 0.012787701, -0.009348883, 0.034517698, 0.07614124, -0.08993195, -0.025814688, -0.013705338, -0.026796496, 0.06982948, -0.042138904, 0.001764984, -0.026960624, -0.018959988, -0.030614382, -0.04398873, 0.063493505, -0.009449232, 0.051905975, 0.010193199, 0.037002172, -0.010176793, -0.008448639, 0.052031778, 0.006357338, 0.0272459, -0.009152464, 0.018548401, -0.0046325247, -0.015267768, -0.015034039, -0.022499528, 0.02989298, 0.06436744, -0.010100371, 0.058490183, 0.05532372, 0.026633006, -0.0077552283, -0.014629854, 0.0330289, -0.039928596, 0.0066927522, 0.006182224, -0.014827544, 0.021445494, -0.012409428, -0.03340716, 0.028496131, -0.012052117, 0.05312791, -0.01661671, 0.052265313, 0.06771773, 0.027773915, -0.018193256, -0.023581868, -0.059976816, -0.050019782, -0.022805229, -0.033398785, 0.0064177467, 0.07761807, -0.019512761, -0.00022825236, 0.023983894, 0.019923545, 0.03167583, 0.0028543093, -0.028220166, -0.015533521, 0.05059353, 0.024309577, 0.03580986, 0.00050810445, 0.017051328, -0.012080206, 0.0017084125, 0.015410581, -0.00789201, -0.037711512, -0.010248453, -0.020290952, 0.027143704, 0.011660623, 0.06072311, 0.020183807, 0.050468907, 0.03663382, -0.02106785, -0.031317316, -0.014827689, 0.018441863, 0.0049650874, -0.11231673, -0.040658925, 0.0148072075, -0.020179247, 0.041097056, 0.007550698, -0.034883548, 0.04754811, 0.011092517, 0.02856835, 0.018466176, 0.022727465, -0.010663158, -0.028384252, 0.044180565, 0.045221463, -0.022555698, 0.021223063, -0.00832296, 0.032631367, -0.003141404, 0.043444883, 0.077997535, 0.0054372516, 0.021219209, 0.0021405343, -0.01590071, -0.01215812, 0.006643029, -0.028798552, 0.01652284, -0.04692245, 0.0390974, -0.015439874, -0.041452654, 0.06111435, -0.04143891, 0.041199494, -0.046932366, 0.01763088, -0.02753955, -0.017143346, -0.07673022, -0.0063810553, 0.0015617562, 0.011534872, 0.005531398, -0.013275226, -0.016871719, 0.05210822, 0.0046906825, 0.025984034, -0.06391326, -0.076600924, -0.030112494, -0.043460526, -0.026873698, 0.050470278, -0.012995147, -0.007224429, -0.03539211, 0.015603318, -0.039778836, -0.011408504, 0.0064373487, -0.03641488, -0.07665461, 0.016908046, 0.01982947, -0.031903237, -0.055695184, -0.027116146, -0.010585974, 0.018495617, -0.0410947, 0.03113009, 0.012184742, 0.03629348, 0.010157377, 0.038516812, -0.010043369, 0.005517354, -0.042619206, -0.006546951, 0.019281307, 0.054766636, -0.02617003, -0.026638161, -0.00068354426, -0.0019847767, -0.030615069, -0.033329953, 0.046203777, -0.022726173, 0.031420015, -0.016680641, 0.015440888, -0.020044945, 0.0096177235, 0.017439777, -0.048777178, -0.0019284827, -0.011699258, -0.029502688, 0.0021390796, 0.036276024, -0.028297655, -0.02731035, 0.021467267, 0.0029552693, -0.059639275, -0.05024039, 0.023820493, 0.0035315566, -0.024958342, 0.02650918, -0.030566843, 0.026173491, 0.028343497, -0.024803063, -0.026851606, 0.016259886, 0.009964459, 0.017351653, -0.056368023, -0.035776716, -0.0059424792, -0.016064283, 0.0034419706, 0.019382205, 0.059390653, 0.019894635, -0.091240145, 0.042807423, 0.002567727, 0.0012162545, 0.0011443462, -0.031807434, 0.0068701785, -0.00069870387, 0.026643349, -0.0319709, -0.005918368, 0.016799565, 0.0063194986, 0.031021994, 0.070190795, 0.003610577, -0.10313329, 0.05083862, 0.022565365, 0.06487919, 0.007841637, 0.0041192034, -0.03462272, -0.037657592, 0.057741605, -0.0040260023, 0.039255936, 0.041596428, -0.00203494, -0.0042837327, -0.0063611795, 0.035205524, 0.023968704, 0.00064296526, 0.022437787, -0.06375935, 0.01733129, -0.0013057288, -0.0017059323, 0.057054948, 0.022362348, 0.08845077, -0.012023815, 0.0017640801, 0.016218686, 0.04752121, 0.030618306, -0.0013107351, 0.016669365, -0.07677603, 0.040041897, 0.013091549, -0.026425093, -0.035017937, -0.028204504, 0.025098689, 0.04103014, -0.0059338575, -0.009959024, 0.015573874, -0.026648724, -0.017875133, -0.038857393, -0.048116036, -0.030398928, 0.03314889, 0.046753813, 0.026439676, -0.03991884, -0.019331567, -0.09359042, 0.030084593, 0.078502335, 0.07696061, -0.03967264, -0.019088814, 0.006417124, 0.03525893, 0.04087205, 0.0017624944, 0.061793797, -0.02608101, -0.0112481965, 0.0027522782, 0.045124624, 0.008207391, 0.03102454, 0.039056454, 0.0825832, -0.0065782447, -0.0106328335, -0.012776879, -0.04891355, 0.017380519, -0.024908263, -0.070429906, -0.0039727795, -0.042244527, 0.083927706, -0.025898676, 0.024054436, -0.0036858423, -0.07182572, 0.051158924, 0.023942973, -0.04038538, -0.011396498, -0.004068075, -0.047387972, -0.015392301, -0.075402565, -0.055935964, 0.008977987, 0.021991488, -0.056592803, 0.0049542855, -0.05062444, -0.0007796706, 0.017241767, -0.058478065, 0.026303671, -0.0075802077, 0.013140604, -0.044470467, 0.0007027847, 0.027472978, 0.026212336, 0.018564506, -0.0071602887, 0.07040972, 0.021615544, 0.02320795, -0.0434501, 0.0010330095, 0.037983894, 0.013176492, -0.0540582, -0.024565622, -0.08124297, 0.04674727, -0.013450571, 0.034393243, -0.07868777, -0.011927388, -0.03604988, -0.00838591, 0.046845578, 0.0030424474, 0.015188725, 0.10424318, -0.024029475, -0.022168055, -0.037073165, -0.0075255753, 0.0024393143, 0.0123511655, 0.01735603, 0.019980183, -0.08139509, 0.025620693, 0.024248226, 0.027270027, -0.0028367925, 0.011219178, -0.019327251, -0.07775952, -0.025615595, 0.02023172, -0.03512198, 0.03514877, -0.015886405, 0.026197892, 0.038126763, -0.026359903, -0.022299495, 0.058469947, -0.0129423905, -0.016455865, -0.011521145, 0.013725556, -0.04177214, -0.0014755863, -0.017177206, -0.029187126, -0.00089669746, -0.04679756, -0.051145956, -0.008276674, 0.01638377, 0.043361653, -0.06990871, -0.004980567, 0.017190896, -0.02499177, 0.02379187, 0.010936152, 0.034476742, 0.0016980366, -0.004048134, -0.014161675, -0.01996899, -0.009272165, -0.038712684, 0.026205285, 0.017135104, -0.036943827, 0.0011137014, -0.05971564, -0.026125489, 0.0123929735, -0.084937595, -0.020547543, -0.027004108, 9.4902025e-05, -0.011984904, 0.059675895, 0.019854404, -0.026970515, 0.009025556, -0.011870736, -0.038856953, -0.022391452, 0.010643388, -0.044816572, -0.018620616, 0.03058993, 0.024191791, 0.0055579687, -0.021938702, 0.012018147, 0.008451883, 0.01442362, 0.0007380551, 0.026362145, 0.0739148, 0.050187383, 0.010099753, 0.08019272, 0.08094431, 0.0032650465, -0.025154622, -0.009687853, 0.041216947, -0.008807693, 0.010457714, 0.005808572, -0.010044642, 0.004971835, 0.018806612, 0.00039812303, -0.03833419, 0.022016553, -0.012541201, -0.02807765, -0.020690758, -0.020797638, -0.007807976, 0.02699129, -0.023330467, -0.029924236, 0.014819868, 0.057826113, 0.012810104, 0.04001299, 0.020408735, 0.019736895, 0.024470069, 0.08217144, -0.0053415056, 0.02243117, -0.033137664, -0.015848087, -0.06778636, -0.003061043, -0.031183481, -0.044324294, -0.029797638, -0.0046083154, -0.011594994, -0.010636758, 0.009792478, 0.012299124, 0.06521801, -0.03185289, -0.060189497, 0.026906323, 0.064544745, -0.0035572287, 0.00042776088, -0.0034751575, 0.0026946068, 0.004174804, -0.0068934574, 0.029737987, 0.015191734, 0.02495277, -0.05383007, -0.058605805, 0.010585224, -0.02892717, 0.050996974, 0.039946146, 0.02309885, -0.019939488, -0.036823224, -0.004409071, 0.011494051, 0.03349499, -0.02535855, -0.018557165, -0.017926157, 0.050118625, -0.044199932, 0.008763518, -0.053246863, 0.014055976, 0.01053213, -0.033797845, -0.06680657, -0.045725025, 0.060861357, -0.039263185, 0.023270598, -0.054638933, 0.0056501618, -0.06296302, -0.013256462, -0.042945493, 0.000116647054, -0.021376094, 0.02592426, 0.01672253, 0.015858483, -0.049981292, 0.0200087, 0.00665294, -0.008275648, -0.005458988, -0.034460288, -0.038575154, 0.000154465, -0.043706827, 0.029144965, 0.0065984377, 0.025463076, 0.07935039, -0.0049468726, -0.0006172732, -0.03825121, 0.090773195, 0.021838948, -0.044419754, -0.033859696, -0.0043698377, 0.0039013422], "index": 3, "object": "embedding"}, {"embedding": [0.068167195, 0.05638397, -0.15886824, -0.07533354, 0.06761004, 0.018285304, 0.06481408, 0.0041602217, 0.019033093, 0.018492429, 0.025615647, 0.046175364, 0.07755311, 0.041045617, -0.038172018, 0.052804794, 0.030597584, -0.020036122, 5.1980544e-05, -0.0024356053, 0.009383687, -0.02533187, -0.0006466455, -0.05254408, 0.032454044, -0.0031222343, -0.00042519462, -0.0032985942, 0.02537888, 0.03323404, 0.03943957, 0.019950572, -0.017879406, -0.052100573, -0.043318477, -0.0105068805, 0.05057942, -0.0013279943, 0.03792679, 0.057997912, 0.051037356, -0.0067412844, -0.00011000316, -0.009930252, 0.01831245, -0.025952, 0.039419353, 0.02523999, 0.05164508, -0.04660892, 0.057572145, -0.022390615, -0.020339256, 0.04685559, 0.09124487, -0.005500277, -0.041531026, 0.098583855, -0.0038339386, -0.08244622, 0.05335286, -0.0016510781, -0.09189567, 0.023605319, 0.011173917, -0.010850843, -0.048257083, -0.006843931, -0.010070541, -0.04089526, -0.0039702295, 0.020494806, -0.003912379, 0.050783608, -0.046292897, 0.03620807, -0.02501941, 0.0029990845, -0.039571945, 0.006794333, 0.000810882, 0.041697957, 0.041174743, -0.010655516, -0.0010607489, 0.0069113607, -0.01533248, -0.008267817, -0.046677418, 0.11145444, -0.01161565, 0.006796815, 0.059233483, 0.010160002, -0.061840724, 0.021318123, -0.027874855, -0.018570237, -0.01580622, 0.0051500825, -0.0332944, -0.0018701782, 0.0013086864, 0.024898626, -0.0026024785, 0.021318244, 0.015937205, 0.038524944, -0.0108320005, -0.00498199, -0.028387502, -0.017438976, -0.009604647, -0.029514058, -0.037073784, -0.0036061755, 0.097164236, -0.031606387, -0.023132652, 0.06324935, 0.0088684, -0.05586483, 0.01111633, -0.012823527, 0.00073428586, 0.05427092, -0.0843193, -0.011647028, -0.043579634, -0.017308582, -0.010558567, -0.051392954, -0.036799017, -0.011300483, -0.012320878, 0.052798316, 0.009218105, -0.019753542, 0.059801158, 0.027415497, 0.012705266, -0.012806361, -0.0051575126, -0.030156847, 0.019800594, -0.044342566, -0.013334847, -0.021778269, 0.0074222772, 0.0018471917, 0.028391695, 0.04280273, 0.020325989, 0.05555701, 0.0022953448, -0.017524963, -0.03714815, 0.004645348, -0.007082623, -0.024667704, 0.027097728, 0.0056476616, -0.04256488, 0.04134594, 0.01155156, 0.022212826, 0.014772389, 0.020585522, 0.03423834, 0.03955248, -0.066225946, -0.036905725, 0.048112106, -0.033555955, 0.05869627, 0.007835753, 0.05650217, -0.031020407, -0.012922351, -0.028374888, 0.011204523, -0.04318878, 0.016758146, -0.009526475, -0.022276888, -0.056873363, 0.0016264436, -0.00960232, 0.0008722679, -0.024079222, 0.034806207, 0.06743431, -0.100680545, -0.024991846, -0.0546673, -0.06982698, 0.036463823, -0.02248532, 0.0123386085, -0.032351993, -0.009282846, -0.02895021, -0.08117799, 0.05567946, -0.028191319, 0.058294512, 0.013682202, 0.024213683, -0.024474964, -0.024427986, 0.019961808, 0.013645274, 0.030143654, 0.0063454234, -0.0035734412, 0.00048024752, -0.029007116, 0.008811709, -0.000952909, 0.020856367, 0.008508956, -0.0108878035, 0.029479707, 0.019215608, 0.03501837, -0.032616798, -0.017832318, 0.016372195, -0.051930808, 0.024700608, -0.018761033, -0.055268332, 0.05016624, 0.013481829, 0.0029604316, 0.028216993, -0.026329152, 0.035872854, 0.022188844, 0.021436462, 0.021008413, 0.048056345, -0.040472835, -0.0028179763, -0.019658662, -0.016753264, 0.02193185, -0.050549988, -0.0220359, 0.040861204, -0.018075794, -0.0012239206, 0.015005455, -0.017765697, 0.033976763, -0.021249874, -0.015579415, 0.014571807, 0.043807674, 0.042383354, 0.023435036, -0.0046123913, -0.0026084725, -0.014684931, -0.011471316, -0.022081224, 0.021677975, -0.020456055, -0.018574025, 0.012800066, 0.03461107, -0.0035104319, 0.0682769, 0.08813523, 0.034185275, 0.041755814, -0.06331069, -0.04528965, 0.0164911, 0.034909256, -0.01452138, -0.10464137, -0.0109929005, -0.011916229, 0.02795906, 0.01738929, 0.002432726, -0.030780692, 0.01585458, -0.0008257233, 0.0024033044, 0.014639038, -0.0077736764, -0.010138045, 0.017125139, 0.032906126, 0.046026565, -0.029843813, 0.031698857, -0.029257705, 0.0020409904, 0.04124215, 0.035329144, 0.07099183, 0.015198496, -0.02320586, -0.01043188, 0.008639758, 0.054447863, 0.011158313, -0.036403477, -0.015666494, -0.06091378, 0.0054734447, -0.014317286, 0.026743213, 0.04163396, 0.014889719, -0.0012031415, -0.053055394, 0.040837288, -0.027433831, -0.019754425, -0.053082243, 0.017866174, 0.035081707, -0.0008230411, -0.027995514, -0.008787323, 0.006394931, 0.0778191, 0.017826263, 0.041642763, -0.022785299, -0.037619617, -0.01837127, 0.0024153416, -0.0092992885, 0.053629797, 0.016922694, 0.010660872, -0.017980633, 0.014770514, -0.04681365, -0.051927917, 0.0008884289, -0.047328293, -0.04544989, 0.04393308, 0.04503944, 0.012047819, -0.036812536, -0.030210862, -0.022561524, 0.037977852, 0.009062339, -0.0034938094, 0.051816046, 0.016007958, -0.0050867684, -0.020646578, 0.004474561, -0.01996405, -0.059059594, 0.0061308495, 0.010566386, 0.04065831, 0.0042954697, 0.0035443506, -0.009724643, 0.022838937, -0.03505436, -0.02437458, 0.06654846, -0.036930278, 0.05291209, -0.03545463, 0.037536304, -0.009781151, 0.034769498, 0.04885233, 0.007175186, -0.01353777, -0.013694571, 0.032880116, 0.027812444, 0.05852052, -0.04969085, -0.039675355, 0.021060523, 0.027464215, -0.0357438, -0.062291633, 0.02752275, -0.00089881825, -0.061413366, -0.016379096, -0.0074710813, 0.033695977, 0.09326941, -0.03728339, -0.025173895, 0.022247195, 0.0067231893, -0.027705405, -0.0050153225, -0.09458729, -0.0020604313, -0.03201044, 0.030944258, 0.00046889525, 0.06653508, -0.033299156, -0.04529049, 0.034866348, 0.014724645, 0.017298272, -0.007441318, 0.008853069, -0.057188746, 0.040298406, 0.0129237715, -0.013819406, -0.024250856, 0.033364266, 0.014915321, 0.06624418, 0.028384574, 0.013107831, -0.06658816, 0.02075983, 0.044742614, 0.035266858, -0.016969591, 0.0394048, -0.027232746, 0.0008311023, 0.04840328, -0.04794522, 0.022015454, 0.025299404, -0.06706417, -0.048983146, -0.041395858, -0.007875825, 0.034071263, 0.06982963, -0.0036140687, -0.056801118, 0.046791427, 0.026054995, -0.016619604, 0.059631635, 0.0402308, 0.07855603, -0.030699328, 0.031344958, 0.025863638, 0.03930117, -0.0024689548, 0.02447604, -0.013686714, -0.049217485, -0.0041296547, -0.015179973, 0.013217323, -0.004818906, -0.01952031, 0.020661043, 0.034824286, -0.02481092, 0.020710004, 0.022556882, 0.023014553, -0.013483154, -0.028898362, -0.0322322, 0.0036543626, 0.017347269, 0.031937156, -0.009491502, -0.031152124, 0.0288401, -0.055333924, -0.0041140695, 0.04556077, 0.039070267, -0.030326877, -0.022191584, 0.0022459803, 0.057673015, 0.050017882, 0.016769541, -0.01171168, -0.020615544, 0.024090549, 0.004728089, 0.0471542, 0.013311388, 0.001014612, 0.039022543, 0.10185306, -0.013659756, -0.030375278, -0.016365698, -0.019747484, 0.02659674, -0.088136666, -0.05139662, -0.023568563, -0.0015338593, 0.022007873, 0.031754367, 0.03127426, 0.030242324, -0.07078379, 0.04142683, 0.00602238, -0.021219844, 0.04549214, -0.004668446, -0.018223936, 0.0013680883, -0.03917921, -0.09959341, -0.035865262, 0.025503227, -0.019308418, 0.009217657, 0.0057360735, 0.0312748, 0.0576188, -0.04378777, -0.0036140531, 0.010386334, 0.02024344, -0.029212682, 0.0007288694, 0.020800771, 0.0642741, 0.026990838, -0.006276698, 0.053184852, -0.00206001, -0.024965003, -0.030824227, 0.0037910296, 0.024877444, 0.021754637, -0.11515638, 0.008190781, -0.036173437, 0.027397893, -0.03753469, 0.03176037, -0.035385225, -0.06461196, -0.029322334, 0.0053360956, -0.017560534, 0.0046238503, 0.014166234, 0.096777074, 0.007882378, -0.03251938, -0.0069739367, -0.0032286437, -0.0074848505, -0.00901611, 0.052907415, 0.023303106, -0.057778247, 0.009305868, -0.00402988, 0.03195598, -0.0148168765, -0.011455844, -0.05424647, -0.059278697, -0.068897925, 0.008067565, -0.021116138, 0.01831191, 0.030414768, 0.005218635, 0.024055243, -0.058677822, -0.03165972, 0.03451161, -0.058995973, -0.028199606, -0.024941478, 0.020584792, 0.007468159, -0.017407645, -0.006408833, -0.021117168, -0.06646617, -0.07192757, -0.024706425, 0.008904071, -0.026349714, 0.0042024637, -0.028617512, 0.008834041, 0.04306327, -0.002458302, 0.048107527, 0.040387362, 0.0054968228, -0.010845513, -0.03179676, -0.033260603, -0.019737367, -0.026564892, -0.036581434, 0.053634156, -0.012328632, -0.0710149, 0.0319483, -0.0031557279, -0.014799423, -0.002957045, -0.027199589, -0.06772137, 0.032192107, -0.023574993, -0.039648626, 0.034402646, 0.028905686, -0.049936246, 0.0006122233, -0.006082703, -0.02587842, -0.06276843, 0.0020669214, -0.013985699, -0.014368451, -0.011623935, 0.0029511224, 0.042636324, 0.017349776, -0.029824836, 0.0071991556, 0.009018644, -0.013522123, 0.017346406, 0.07459359, 0.03645976, 0.010870965, 0.062480144, 0.09017249, 0.015364286, -0.049251445, -0.021337118, 0.023829825, -0.008270684, 0.013595309, 0.0075338725, -0.0015748, -0.004082644, 0.005870709, -0.010692795, 0.016592782, 0.005630636, -0.02349018, -0.047429204, -0.005444519, -0.007951505, 0.008485024, 0.034996744, 0.013489211, -0.009288506, 0.012249358, 0.082889654, -0.0021831233, 0.024050888, 0.03584444, -0.0069353087, 0.0062561925, 0.030689836, 0.015794097, -0.018197887, -0.024826022, -0.08097385, -0.04433751, 0.0036010128, -0.048051406, -0.041597452, -0.030033503, -0.043157898, -0.029625177, -0.053177, -0.022016512, -0.03240063, -0.01438529, 0.0061203097, -0.0021489593, 0.0216482, 0.057759017, -0.028572753, 0.0031663086, 0.031523287, 0.021966908, 0.011623737, -0.027144734, 0.019058201, 0.02077314, 0.009638916, -0.07203019, -0.037546836, 0.0012566227, -0.045597322, 0.056461424, 0.033021923, -0.009073042, 0.0068995915, -0.05895138, -0.0036417989, -0.0135440165, 0.0540284, 0.035780087, -0.026422076, -0.03644378, 0.03661341, -0.048662104, 0.02417685, -0.060539264, -0.004743213, 0.01693342, -0.0095071765, -0.06818605, -0.012613894, 0.023059059, -0.009178728, -0.01896024, -0.023872804, -0.005098898, -0.04468707, -0.00050312275, -0.054696016, 0.028324677, -0.0330306, 0.04097767, 0.016368024, -0.0027385019, -0.025832666, 0.030909661, -0.025631426, -0.0554501, 0.0046622804, -0.010233966, -0.05178013, 0.03874888, -0.025585055, 0.020134017, 0.04779429, -0.009497232, 0.10689358, -0.0016635938, -0.008075462, -0.0777768, 0.06689805, 0.057294097, -0.06574554, -0.04769717, -0.0093708, 0.011148632], "index": 4, "object": "embedding"}, {"embedding": [0.04870307, 0.06401401, -0.1582164, -0.038260747, 0.11698231, -0.00018690352, 0.07951837, -0.0075067948, 0.038212027, -0.004651584, 0.042908177, 0.0013932526, 0.1050856, 0.052119628, -0.011165481, 0.039585624, -0.0085784765, -0.015313834, -0.03726809, -0.0051837983, 0.011368006, -0.014751609, -0.043459404, 0.007947677, 0.04678644, 0.015571088, -0.032872956, -0.020353682, -0.0071763983, 0.0021357841, -0.016008519, 0.018718408, -0.014989665, -0.035132475, 0.008297823, 0.013288706, 0.026490252, 0.059309717, -0.0042888676, 0.022436878, 0.05246452, 0.04814206, -0.025984807, 0.008476967, 0.02634199, -0.019020336, 0.04562709, -0.020519847, 0.05320595, -0.06050486, 0.04509685, -0.025410818, -0.015519864, 0.030012209, 0.054713234, -0.039117597, -0.006747, 0.054232836, -0.03841942, -0.048931483, 0.035078745, 0.053856876, -0.0734198, 0.040357675, 0.026299553, 0.012889913, -0.049733598, 0.025686681, -0.009808964, -0.028812297, -0.006608125, 0.013100198, 0.0015251812, 0.02396294, -0.026819473, 0.03447048, -0.038302626, -0.023818692, -0.036588036, -0.0060396506, 0.06050202, 0.02979706, 0.03867054, 0.01674761, 0.012173306, -0.03559402, -0.022109563, -0.0070369854, -0.015116695, 0.0842325, 0.023687856, -0.020908715, 0.04779805, 0.017012931, -0.036392584, 0.012794883, -0.0704846, -0.021628795, -0.056540594, 0.022467382, -0.0190275, 0.000504403, -0.0010017989, 0.008796869, -0.007581031, 0.013337139, 0.046927765, 0.023653513, 0.019755151, -0.05883244, -0.002899942, -0.0045779645, -0.00929443, 0.016842581, -0.047249813, -0.058689095, 0.09564763, -0.0017104156, 0.009376372, 0.05168618, -0.013404363, -0.027038883, -0.0068793288, -0.035219904, -0.0036846055, 0.038151175, -0.06031716, 0.0116957845, -0.07876638, -0.050061386, 0.009880543, -0.03620902, -0.006223672, -0.018190198, -0.007946173, 0.07947298, -0.0014931759, -0.026287308, 0.023060545, 0.031110834, 0.025939237, 0.0018527121, -0.0009825171, -0.016067414, 0.0051381835, -0.06159131, 0.00937253, -0.003795934, 0.0108944755, -0.0016893826, -0.0013675891, 0.042033494, 0.024415618, 0.038421106, 0.023109294, -0.07792016, -0.04178184, 0.016567359, 0.019258857, 0.017562144, 0.033782907, 0.034370907, -0.024992622, 0.024188815, -0.02846477, 0.03772214, -0.01963935, 0.0035596606, 0.011531748, 0.060735542, -0.089057826, -0.013171014, 0.019778999, -0.0013799074, 0.063879654, -0.014278753, 0.040574133, -0.041034043, -0.020203162, -0.047894135, 0.048874456, -0.04456907, 0.044931263, 0.015660483, -0.02457574, -0.06663261, 0.03882712, -0.033310845, -0.0084944, -0.021061856, 0.017569473, 0.032826837, -0.05719455, -0.022227418, -0.075874165, -0.06534835, 0.027624762, -0.025632106, 0.033478506, -0.0117728505, -0.025387641, 0.0038607102, -0.06398987, 0.0348772, -0.011253565, 0.04249318, 0.0025482145, 0.027547201, -0.014858839, -0.023250585, 0.014214528, 0.005501403, 0.016104544, -0.013639986, 0.0019374578, 0.027379947, 0.014502224, 0.015951494, -0.020881763, -0.010764878, 0.021098537, -0.026612403, -0.009189005, -0.03253754, 0.034291115, -0.06291383, -0.055434525, 0.023710575, -0.02536224, 0.017085312, 0.017728798, -0.062237687, -0.004725205, 0.015773246, 0.0046217614, 0.05634284, -0.017055118, 0.041196506, 0.00077846344, -0.009708542, -0.004701406, 0.048839662, -0.053716272, 0.01945114, -0.040958334, -0.029734157, -0.004268, -0.07128148, -0.013813481, 0.03288088, 0.012682207, 0.028861444, 0.038039837, -0.027506491, 0.013885504, -0.0415974, -0.026087366, 0.0044447086, 0.06266872, 0.05595285, 0.041630793, -0.0206805, 0.016270988, 0.0061320458, 0.0071216915, -0.031392485, 0.008246579, -0.022285873, -0.016885506, -0.010244858, 0.04117301, 0.025947746, 0.005602522, 0.051393803, 0.011217606, 0.04325376, -0.02946018, -0.04941183, -0.016809339, 0.025700567, -0.028512182, -0.025151739, -0.013107922, -0.033071287, 0.015101774, 0.043258026, -0.016109845, -0.0024213037, 0.0065360186, 0.013986928, 0.031745233, 0.0230427, -0.03282111, 0.01791293, -0.027141757, 0.033805117, 0.07322082, -0.054233048, 0.020293018, -0.029743453, 0.029239465, 0.024192346, 0.04604154, 0.060282327, -0.012461772, -0.032441933, 0.00365466, 0.026082933, 0.05430926, 0.05749293, -0.051903818, -0.0063061984, -0.05406948, -0.016328743, -0.012140797, -0.013309708, 0.031032452, 0.03369406, 0.03977923, -0.022309944, 0.070275016, -0.01325516, 0.0067165806, -0.02238218, 0.012772237, 0.053317145, -0.0029072056, -0.01655657, -0.0051404936, -0.017851984, 0.083807796, 0.024898393, 0.034192104, -0.038449712, -0.017033594, 0.02745864, 0.037991002, 0.0077468413, 0.03964423, 0.009556769, 0.056110457, -0.0073916805, 0.03892907, -0.047988184, -0.0751886, -0.0004889097, -0.041628715, -0.040495615, 0.05996494, 0.076892935, 0.0038141129, 0.0011130536, -0.033032414, -0.04059043, 0.066723585, -0.01247974, 0.014608047, 0.06998668, 0.0041669076, 0.001771421, 0.03176269, 0.00917165, 0.0020053831, -0.0494777, -0.016177893, 0.017602162, 0.034806892, -0.013775994, -0.028835403, -0.025675068, 0.04014126, -0.04150127, -0.023350308, 0.061003763, -0.055914443, 0.019777423, -0.033648867, 0.033731177, -0.04038656, -0.0049016033, 0.025142591, 0.007957981, -0.029237818, -0.0056649186, 0.027333643, 0.027264515, 0.05147737, -0.07193637, -0.028695825, 0.021819433, -0.0073648095, -0.030587232, -0.059382156, 0.024096165, 0.013989115, -0.036415137, -0.03412697, -0.042560045, 0.028598938, 0.10319142, -0.025737163, -0.0159816, 0.029639475, 0.012059878, -0.015185416, 0.014308659, -0.0841441, -0.021923114, 0.014134898, -0.002095532, 0.008703351, 0.06999104, -0.057142958, -0.04117921, 0.08336942, 0.008325714, 0.038852308, -0.013476855, 0.012629322, -0.03392075, 0.028309444, 0.010421477, 0.02277363, -0.009501118, 0.02182345, 0.05509246, 0.06826044, 0.038318265, -0.003558509, -0.04541057, 0.029114796, 0.024862757, 0.00402528, -0.03899383, -0.021139232, 0.009143313, -0.012646099, -0.0008988414, -0.016909558, 0.058814526, 0.042163413, -0.064009406, -0.04762883, -0.060179133, -0.0150333345, 0.051072456, 0.08462344, -0.007976176, -0.054646075, 0.012559594, 0.03341318, -0.005729646, 0.028787766, 0.06788624, 0.04706963, -0.030563725, -0.0048579527, 0.046552386, 0.019719135, -0.0070881215, 0.026760688, -0.023285741, -0.011880353, -0.016538734, -0.019973429, -0.010029328, 0.018420478, -0.020413144, 0.023199068, 0.057138138, -0.040051237, -0.00084316893, -0.0132690985, 0.044120874, -0.04074426, -0.009425892, -0.034902286, -0.023586214, 0.02145692, 0.06282101, -0.0032660107, -0.049299203, 0.03586914, -0.04284529, -0.0075031794, 0.011085145, 0.025682855, 0.00858561, -0.015866505, -0.022548964, 0.01529401, 0.066522636, 0.0033749947, -0.025915476, -0.00802523, 0.011955836, -0.007863688, 0.041636094, 0.04809042, 0.0018452883, 0.0077473484, 0.02489532, 0.009903749, 0.0196005, -0.002244504, 0.007286492, 0.021639284, -0.09668419, -0.03135953, 0.017706001, 0.01941275, -0.02619122, 0.013010822, 0.025694741, 0.02279736, -0.051599756, 0.008220451, 0.0012692234, -0.020772764, 0.0035623987, 0.008661787, -0.0139318295, -0.014287964, -0.030801758, -0.11769529, -0.031091006, 0.02222233, -0.020734001, 0.02183222, -0.015370543, 0.006736602, 0.09187952, -0.034764756, 0.0012366505, 0.0144016575, 0.016347164, -0.038414396, 0.010291809, 0.042659726, 0.062748656, 0.005278431, 0.023314107, 0.047963742, 0.0064801234, -0.0072562564, -0.034864385, 0.00961766, 0.028069109, 0.0059975716, -0.08586241, 0.03769166, -0.04670247, 0.011514647, -0.044935755, 0.048293617, -0.0063922573, -0.031670462, -0.028337257, -0.009437022, -0.032544, 0.016780198, -0.0006894464, 0.09170076, 0.0057308897, -0.05265787, 0.007953648, -0.01644893, -0.023533152, -0.003688191, 0.044388764, 0.031715494, -0.060608637, 0.004771407, 0.0017095312, -0.016415978, -0.010860581, -0.028150547, -0.033899073, -0.0703239, -0.061751407, 0.045179598, -0.028310332, 0.04244536, 0.01924681, 0.007550839, 0.0051099285, -0.06608143, -0.03973981, -0.014739677, -0.050062027, -0.027753554, -0.04474689, 0.012089917, -0.008855215, 0.013489873, -0.0034503778, -0.007475589, -0.05054805, -0.048986834, -0.039020237, -0.026215445, -0.038477253, 0.018518401, -0.03968392, 0.034450117, 0.06873649, 0.021227967, 0.034722183, 0.040615235, -0.010806791, -0.02267595, -0.011477893, -0.04842531, -0.030594988, -0.039465025, -0.033074252, 0.05087744, -0.041950606, -0.046216313, 0.01555504, -0.02951929, -0.029249078, 0.04297004, -0.024614615, -0.0072863437, 0.024438854, -0.02417313, -0.036572814, -0.0063341903, 0.045279037, -0.05281749, -0.021342149, -0.003935989, -0.03748181, -0.06623677, 0.0021043988, 0.015065997, -0.00026587158, -0.014328016, -0.004530733, 0.0029519035, 0.017646236, -0.0014456798, 0.07590612, 0.025353365, -0.0025975185, -0.006384561, 0.067916006, 0.041861888, -0.0067786826, 0.07395573, 0.061541166, -0.0032875137, -0.035675928, -0.02563342, 0.009759393, 0.022811407, 0.02437572, 0.0060333046, 0.013693924, 0.03162808, -0.0077821445, 0.012549957, 0.010279284, 0.011046044, -0.025062602, -0.09054736, 0.024893314, -0.00019017195, 0.024583152, 0.03685086, 0.015179614, 0.035670973, -0.009222747, 0.066124916, -0.019095846, 0.026899947, 0.050624058, 0.023331309, 0.020291882, 0.017764952, 0.010855645, 0.021152364, -0.059394132, -0.033904817, -0.04565986, 0.049077876, -0.019143475, -0.042986713, -0.03374686, -0.049314536, -0.016062515, -0.023515783, -0.06862195, -0.016716849, 0.014016309, -0.023864893, -0.008054441, -0.0064063626, 0.0056184195, -0.014390964, 0.0056431824, 0.02778499, -0.025609452, 0.03407539, -0.004350457, -0.0053434223, -0.028658202, -0.0009846399, -0.09143871, 0.0025128538, 0.029566638, -0.054303527, 0.056147154, 0.0010310565, -0.016787982, -0.029022513, -0.06785155, 0.0022230085, -0.047749825, 0.028814511, 0.00879717, -0.054989155, -0.015464429, 0.044414464, -0.009319621, -0.0062464355, -0.07588237, -0.021335991, 0.005026507, -0.042572245, -0.017462905, 1.28230195e-05, 0.052508976, -0.012122771, -0.008575127, -0.007481607, -0.037415072, -0.08950197, -0.0038632993, -0.016697286, 0.02573806, -0.056142323, 0.049672797, 0.0072032786, 0.0028816364, 0.004033598, 0.026157571, -0.020861465, -0.028528716, 0.019047868, 0.029284779, -0.04622217, 0.020171203, 0.009450614, 0.038972337, 0.021437883, -0.0028154443, 0.07732539, 0.00990034, -0.0048377556, -0.033990033, 0.043316703, 0.024327919, -0.058303185, -0.07955888, 0.0050947093, 0.020452712], "index": 5, "object": "embedding"}], "model": "nomic-embed-text:v1.5", "object": "list", "usage": {"prompt_tokens": 7341, "total_tokens": 7341}}, "input": {"input": ["-trained text and\nimage encoders. This method enables the model to generalise across various tasks without requiring\ntask-specific fine-tuning (adopted from [90]).\n11.2\nFine-tuning of multimodal models\nFor fine-tuning a Multimodal Large Language Model (MLLM), PEFT techniques such as LoRA and\nQLoRA can be utilised. The process of fine-tuning for multimodal applications is analogous to that for\nlarge language models, with the primary difference being the nature of the input data. In addition to\nLoRA, which employs matrix factorisation techniques to reduce the number of parameters, other tools\nsuch as LLM-Adapters and (IA)[91] can be effectively used. LLM-Adapters integrate various adapter\nmodules into the pre-trained models architecture, enabling parameter-efficient fine-tuning for diverse\ntasks by updating only the adapter parameters while keeping the base model parameters fixed. (IA),\nor Infused Adapters by Inhibiting and Amplifying Inner Activations, enhances performance by learn-\ning vectors to weight model parameters through activation multiplications, supporting robust few-shot\nperformance and task mixing without manual adjustments. Moreover, dynamic adaptation techniques\nlike DyLoRA[92] allow for the training of low-rank adaptation blocks across different ranks, optimising\nthe learning process by sorting the representations during training. LoRA-FA[93], a variant of LoRA,\noptimises the fine-tuning process by freezing the first low-rank matrix after initialisation and using it as a\nrandom projection while training the other, thereby reducing the number of parameters by half without\ncompromising performance.\nThe Efficient Attention Skipping (EAS)[94] module introduces a novel parameter and computation-\nefficient tuning method for MLLMs, aiming to maintain high performance while reducing parameter and\ncomputation costs for downstream tasks. However, MemVP[95] critiques this approach, noting that it\nstill increases the input length of language models. To address this, MemVP integrates visual prompts\nwith the weights of Feed Forward Networks, thereby injecting visual knowledge to decrease training time\nand inference latency, ultimately outperforming previous PEFT methods.\n11.2.1\nFull-parameter Fine-Tuning\nMethods such as those introduced by LOMO[96] and MeZO[97] provide alternative solutions by focusing\non memory efficiency.\nLOMO utilises a low-memory optimisation technique derived from Stochastic\nGradient Descent (SGD), reducing memory consumption typically associated with the ADAM optimiser.\nMeZO, on the other hand, offers a memory-efficient optimiser that requires only two forward passes\nto compute gradients, enabling comprehensive fine-tuning of large models with a memory footprint\nequivalent to inference [89].\n91\n11.2.2\nCase study of fine-tuning MLLMs for Medical domain\nThe following section provides a case study on fine-tuning MLLMs for the Visual Question Answering\n(VQA) task. In this example, we present a PEFT for fine-tuning MLLM specifically designed for Med-\nVQA applications. To ensure accurate performance measurement, human evaluations were conducted,\ndemonstrating that the model achieves an overall accuracy of 81.9% and surpasses the GPT-4v model\nby a substantial margin of 26% in absolute accuracy on closed-ended questions.\nThe model consists of three components: the vision encoder, a pre-trained Large Language Model (LLM)\nfor handling multimodal inputs and generating responses, and a single linear layer for projecting embed-\ndings from the visual encoding space to the LLM space, as shown in figure 11.3.\nThe Vision Transformer (ViT) type backbone, EVA, encodes image tokens into visual embeddings,\nwith model weights remaining frozen during the fine-tuning process. The technique from MiniGPT-v2\nis utilised, grouping four consecutive tokens into one visual embedding to efficiently reduce resource\nconsumption by concatenating on the embedding dimension.\nThese grouped visual tokens are then processed through the projection layer, resulting in embeddings\n(length 4096) in the LLM space. A multimodal prompt template integrates both visual and question\ninformation, which is input into the pre-trained LLM, LLaMA2-chat(7B), for answer generation. The\nlow-rank adaptation (LoRA) technique is applied for efficient fine-tuning, keeping the rest of the LLM\nfrozen during downstream fine-tuning. A beam search with a width of 1 is utilised.\nFigure 11.3: Overview of Med VQA architecture integrating LoRA and a pre-trained LLM with a Vision\nEncoder for medical visual question answering tasks. The architecture includes stages for processing\nimages and generating contextually relevant responses, demonstrating the integration of vision and lan-\nguage models in a medical setting (adopted from [98]).\nThe multimodal prompt includes input images, questions, and a specific token for VQA tasks, following\nthe MiniGPT-v2 template. In Figure 11.3, the image features derived from linear projection are labelled\nas ImageFeature, with the corresponding questions serving as text instructions. The special token [VQA]\nis used as the task identifier, forming the complete multimodal instructional template:\n92\n[INST]<img><ImageFeature></img>[VQA] Instruction [/INST].\nModel Training\nWeights from MiniGPT-v2, pre-trained on general domain datasets, are further fine-tuned using multi-\nmodal medical datasets in two stages. The LoRA technique is employed for efficient fine-tuning, updating\nonly a small portion of the entire model, as detailed below:\n Fine-tuning with image captioning: During this stage, the model is fine-tuned using the ROCO\nmedical image-caption dataset, which contains medical image", "VQA] Instruction [/INST].\nModel Training\nWeights from MiniGPT-v2, pre-trained on general domain datasets, are further fine-tuned using multi-\nmodal medical datasets in two stages. The LoRA technique is employed for efficient fine-tuning, updating\nonly a small portion of the entire model, as detailed below:\n Fine-tuning with image captioning: During this stage, the model is fine-tuned using the ROCO\nmedical image-caption dataset, which contains medical image-caption pairs of varying lengths. The\nprompt template used is <Img><ImageHere></Img>[caption] <instruction>, with the instruc-\ntion prompt randomly selected from a pool of four candidates, such as Briefly describe this image.\nDuring training, only the linear projection layer and the LoRA layer in the LLM are fine-tuned,\nwhile other parts of the model remain frozen.\n Fine-tuning on VQA: In the second stage, the model is fine-tuned on the Med-VQA dataset,\nVQA-RAD, which contains triplets of images, questions, and answers. Following the instruction\ntemplate proposed in MiniGPT-v2, the template used is: [INST] <img><ImageFeature></img>[VQA]\nInstruction [/INST], where the instruction prompt is: Based on the image, respond to this\nquestion with a short answer: question, with question signifying the question corresponding to\nthe given medical image. The motivation for generating short answers is to validate against the\nexisting labelled data in VQA-RAD, where the answers are typically short in both open-ended and\nclosed-ended QA pairs. Similar to the first stage, the vision encoder and the LLM remain frozen\nwhile only the linear projection and LoRA layers in the LLM are updated.\n11.3\nApplications of Multimodal models\n1. Gesture Recognition - These models interpret and recognise human gestures, which is crucial\nfor sign language translation. Multimodal models facilitate inclusive communication by processing\ngestures and converting them into text or speech.\n2. Video Summarisation - Multimodal models can summarise lengthy videos by extracting key vi-\nsual and audio elements. This capability streamlines content consumption, enables efficient content\nbrowsing, and enhances video content management platforms.\n3. DALL-E is a notable example of multimodal AI that generates images from textual descriptions.\nThis technology expands creative possibilities in content creation and visual storytelling, with\napplications in art, design, advertising, and more.\n4. Educational Tools - Multimodal models enhance learning experiences by providing interactive\neducational content that responds to both visual and verbal cues from students. They are integral\nto adaptive learning platforms that adjust content and difficulty based on student performance and\nfeedback.\n5. Virtual Assistants - Multimodal models power virtual assistants by understanding and respond-\ning to voice commands while processing visual data for comprehensive user interaction. They are\nessential for smart home automation, voice-controlled devices, and digital personal assistants.\n11.4\nAudio or Speech LLMs Or Large Audio Models\nAudio or speech LLMs are models designed to understand and generate human language based on audio\ninputs. They have applications in speech recognition, text-to-speech conversion, and natural language\nunderstanding tasks. These models are typically pre-trained on large datasets to learn generic language\npatterns, which are then fine-tuned on specific tasks or domains to enhance performance.\nAudio and Speech Large Language Models (LLMs) represent a significant advancement in the integration\nof language processing with audio signals. These models leverage a robust Large Language Model as a\nfoundational backbone, which is enhanced to handle multimodal data through the inclusion of custom\naudio tokens. This transformation allows the models to learn and operate within a shared multimodal\nspace, where both text and audio signals can be effectively processed.\n93\nUnlike text, which is inherently discrete, audio signals are continuous and need to be discretized into\nmanageable audio tokens. Techniques like HuBERT[99] and wav2vec[100] are employed for this purpose,\nconverting audio into a tokenized format that the LLM can process alongside text. The model, typically\nautoregressive and decoder-based, is pre-trained using a combination of self-supervised tasks, such as\npredicting masked tokens in interleaved text and audio, and supervised fine-tuning for specific tasks like\ntranscription or sentiment analysis. This capability to handle and generate audio and text simultane-\nously allows for a wide range of applications, from audio question answering to speech-based sentiment\ndetection, making Audio and Speech LLMs a versatile tool in multimodal AI. The figure 11.4 illustrates\nan example of a multimodal Audio LM architecture. In this setup, a prompt provides instructions in\nboth text and audio formats. The audio is tokenized using an audio tokenizer. The multimodal model\nthen combines these text and audio tokens and generates spoken speech through a vocoder (also known\nas a voice decoder).\nFigure 11.4: Multimodal Audio-Text Language Model architecture that integrates text and audio in-\nputs for advanced multimodal processing.\nThe architecture utilises text tokenizers and audio en-\ncoders/tokenizers to convert inputs into tokens, which are then processed by the audio-text LM. This\nmodel supports both discrete and continuous speech processing and enables tasks such as sentiment anal-\nysis and response generation in natural language. The audio tokens are further refined using a vocoder,\nwhile text tokens are detokenized to produce coherent text outputs (adapted from [101]).\n94\nAudio and speech LLMs like AudioPaLM[102], AudioLM[103], and various adaptations of models like\nWhisper and LLaMA, integrate capabilities for understanding and generating audio data, including\nspeech-to-text (STT), text-to-speech (", " such as sentiment anal-\nysis and response generation in natural language. The audio tokens are further refined using a vocoder,\nwhile text tokens are detokenized to produce coherent text outputs (adapted from [101]).\n94\nAudio and speech LLMs like AudioPaLM[102], AudioLM[103], and various adaptations of models like\nWhisper and LLaMA, integrate capabilities for understanding and generating audio data, including\nspeech-to-text (STT), text-to-speech (TTS), and speech-to-speech (STS) translation.\nThese models\nhave shown that LLMs, initially designed for text, can be effectively adapted for audio tasks through\nsophisticated tokenization and fine-tuning techniques.\n11.4.1\nTokenization and Preprocessing\nA key aspect of adapting LLMs for audio is the tokenization of audio data into discrete representations\nthat the model can process. For instance, AudioLM and AudioPaLM utilise a combination of acoustic\nand semantic tokens. Acoustic tokens capture the high-quality audio synthesis aspect, while semantic\ntokens help maintain long-term structural coherence in the generated audio. This dual-token approach\nallows the models to handle both the intricacies of audio waveforms and the semantic content of speech.\n11.4.2\nFine-Tuning Techniques\nFine-tuning audio and speech LLMs typically involve several key strategies:\n Full Parameter Fine-Tuning: This involves updating all the models parameters during fine-\ntuning. For instance, LauraGPT and SpeechGPT fine-tune all parameters to adapt pre-trained\ntext LLMs to various audio tasks, although this can be computationally expensive.\n Layer-Specific Fine-Tuning: Techniques like LoRA (Low-Rank Adaptation) update only spe-\ncific layers or modules of the model. This method significantly reduces computational requirements\nwhile still allowing effective adaptation. Models like Qwen-Audio leverage LoRA to fine-tune pre-\ntrained components for enhanced performance on speech recognition tasks.\n Component-Based Fine-Tuning: Recent models, such as those integrating the Whisper en-\ncoder, freeze certain parts of the model (like the speech encoder) and only fine-tune a linear\nprojector or specific adapters to align the speech and text modalities. This approach simplifies the\ntraining process and enhances efficiency[104].\n Multi-Stage Fine-Tuning: Models like AudioPaLM perform multi-stage fine-tuning, starting\nwith a text-based pre-training phase, followed by fine-tuning on a mixture of tasks that include\nboth text and audio data. This staged approach leverages the strengths of pre-trained text models\nwhile adapting them for multimodal tasks.\n11.4.3\nFine-Tuning Whisper for Automatic Speech Recognition (ASR)\nWhisper1 is an advanced Automatic Speech Recognition (ASR) model developed by OpenAI, designed\nto convert spoken language into text. Built upon the powerful Transformer architecture, Whisper excels\nat capturing and transcribing diverse speech patterns across various languages and accents.\nUnlike\ntraditional ASR models that require extensive labelled data, Whisper leverages a vast dataset and self-\nsupervised learning, enabling it to perform robustly in noisy environments and handle a wide range of\nspeech variations. Its versatility and high accuracy make it an ideal choice for applications such as voice\nassistants, transcription services, and multilingual speech recognition systems.\nWhy Fine-Tune Whisper?\nFine-tuning Whisper for specific ASR tasks can significantly enhance its performance in specialised\ndomains. Although Whisper is pre-trained on a large and diverse dataset, it might not fully capture\nthe nuances of specific vocabularies or accents present in niche applications. Fine-tuning allows Whisper\nto adapt to particular audio characteristics and terminologies, leading to more accurate and reliable\ntranscriptions. This process is especially beneficial in industries with domain-specific jargon, like medical,\nlegal, or technical fields, where the generic model might struggle with specialised vocabulary.\n1https://openai.com/index/whisper/\n95\nSteps to Fine-Tune Whisper\n Data Collection and Preparation: Gather a sizable dataset that matches the target domain or\ntask. Ensure the dataset includes diverse examples with clear transcriptions. Clean and preprocess\nthe audio files and transcripts, ensuring they are in a consistent format and aligned correctly. Tools\nlike FFmpeg2 can help standardise audio formats and sample rates.\n Data Augmentation: To improve robustness, augment the dataset with variations such as dif-\nferent noise levels, accents, or speeds. Techniques like adding background noise, altering pitch, or\nchanging the tempo can help the model generalise better to real-world conditions.\n Preprocessing: Convert the audio files into a format suitable for Whisper, typically into mel\nspectrograms or another time-frequency representation. This transformation is crucial as Whisper\nrelies on such representations to learn and transcribe speech effectively.\n Model Configuration: Initialise the Whisper model with pre-trained weights. Configure the\nmodel to accommodate the target language or domain-specific adjustments. This includes setting\nappropriate hyperparameters, like learning rate and batch size, tailored to the datasets size and\ncomplexity.\n Training: Fine-tune the Whisper model on the prepared dataset using a framework like PyTorch\nor TensorFlow. Ensure to monitor the models performance on a validation set to avoid overfitting.\nTechniques like gradient clipping, learning rate scheduling, and early stopping can help maintain\ntraining stability and efficiency.\n Evaluation and Testing: After training, evaluate the models performance on a separate test\nset to assess its accuracy and generalisability. Metrics like Word Error Rate (WER) or Character\nError Rate (CER) provide insights into how well the model transcribes audio compared to ground\ntruth transcriptions.\n11.4.4\nCase Studies and Applications\n1. Medical Transcription: Fine-tuning speech L", " rate scheduling, and early stopping can help maintain\ntraining stability and efficiency.\n Evaluation and Testing: After training, evaluate the models performance on a separate test\nset to assess its accuracy and generalisability. Metrics like Word Error Rate (WER) or Character\nError Rate (CER) provide insights into how well the model transcribes audio compared to ground\ntruth transcriptions.\n11.4.4\nCase Studies and Applications\n1. Medical Transcription: Fine-tuning speech LLMs on medical data has led to significant im-\nprovements in transcribing doctor-patient interactions. Models like Whisper have been fine-tuned\non medical terminologies, resulting in more accurate and reliable transcriptions.\n2. Legal Document Processing: Legal firms have employed fine-tuned audio LLMs to transcribe\ncourt proceedings and legal discussions.\nDomain-specific fine-tuning has enhanced the models\nability to recognise and accurately transcribe legal jargon.\n3. Customer Service Automation: Companies are using fine-tuned speech models to automate\ncustomer service interactions. These models are trained on customer support data to understand\nand respond to queries more effectively, providing a more seamless user experience.\n2https://ffmpeg.org/ffmpeg.html\n96\nChapter 12\nOpen Challenges and Research\nDirections\n12.1\nScalability Issues\nThe fine-tuning of Large Language Models (LLMs) such as GPT-4, PaLM1 , and T52 has become a critical\narea of research, presenting several significant challenges and opening up new avenues for exploration,\nparticularly in scaling these processes efficiently. This discussion focuses on the two main aspects: the\nchallenges in scaling fine-tuning processes and potential research directions for scalable solutions.\n12.1.1\nChallenges in Scaling Fine-Tuning Processes\n1. Computational Resources: Large-scale models such as GPT-3 and PaLM require enormous\ncomputational resources for fine-tuning. For instance, fine-tuning a 175-billion parameter model\nlike GPT-3 necessitates high-performance GPUs or TPUs capable of handling vast amounts of data\nand complex operations. The sheer volume of parameters translates to extensive computational\ndemands. Even a relatively smaller model, such as BERT-large with 340 million parameters, can\nbe computationally intensive to fine-tune.\n2. Memory Requirements: The memory footprint for fine-tuning LLMs is staggering. Each pa-\nrameter in the model requires storage, and during training, additional memory is needed to store\nintermediate computations, gradients, and optimiser states. For example, loading a 7 billion pa-\nrameter model (e.g., LLaMA 2) in FP32 (4 bytes per parameter) requires approximately 28 GB\nof GPU memory, while fine-tuning demands around 112 GB of GPU memory[105]. This memory\ndemand is beyond the capability of most consumer-grade hardware, making fine-tuning accessible\nprimarily to well-funded organisations or research institutions.\n3. Data Volume: LLMs typically require vast amounts of training data to achieve state-of-the-art\nperformance during fine-tuning. This data needs to be loaded, preprocessed, and fed into the model\nat high speeds to maintain efficient training. Managing large datasets can become a bottleneck,\nespecially if the data is stored in a distributed fashion across multiple systems or if it needs to be\nfetched from remote storage.\n4. Throughput and Bottlenecks: High throughput is essential to keep GPUs or TPUs fully\nutilised. However, data pipelines can become bottlenecks if not properly optimised. For exam-\nple, shuffling large datasets or loading them into memory quickly enough to keep up with the\ntraining process can be challenging. Techniques like data packing, where multiple small examples\nare combined into larger batches, help improve throughput but add complexity to data handling\nroutines.[106]\n5. Efficient Use of Resources: The financial and environmental costs of fine-tuning large models\nare significant. Large-scale fine-tuning involves not just the direct cost of computational resources\nbut also the indirect costs associated with energy consumption and infrastructure maintenance.\n1https://ai.google/discover/palm2/\n2https://huggingface.co/docs/transformers/en/model_doc/t5\n97\nTechniques such as mixed-precision training and gradient checkpointing can reduce these costs by\noptimising memory and computational efficiency.\nThe challenges in scaling the fine-tuning processes of LLMs are multifaceted and complex, involving sig-\nnificant computational, memory, and data handling constraints. Innovations in PEFT, data throughput\noptimisation, and resource-efficient training methods are critical for overcoming these challenges. As\nLLMs continue to grow in size and capability, addressing these challenges will be essential for making\nadvanced AI accessible and practical for a wider range of applications.\n12.1.2\nResearch Directions for Scalable Solutions\nAdvanced PEFT Techniques and Sparse Fine-Tuning\nRecent advancements in PEFT techniques, like LoRA and its variant, Quantised LoRA, are revolu-\ntionising the scalability of LLMs. LoRA reduces the computational burden by updating only a low-rank\napproximation of the parameters, significantly lowering memory and processing requirements. Quantised\nLoRA further optimises resource usage by applying quantisation to these low-rank matrices, maintaining\nhigh model performance while minimising the need for extensive hardware. This has enabled efficient\nfine-tuning of massive models, such as in Metas LLaMA project, where adapting a smaller set of influ-\nential parameters allowed the models to perform robustly across various tasks with less computational\nstrain.\nSparse fine-tuning techniques, such as SpIEL [107] complement these efforts by selectively updating\nonly the most impactful parameters. SpIEL fine-tunes models by only changing a small portion of the\nparameters, which", " minimising the need for extensive hardware. This has enabled efficient\nfine-tuning of massive models, such as in Metas LLaMA project, where adapting a smaller set of influ-\nential parameters allowed the models to perform robustly across various tasks with less computational\nstrain.\nSparse fine-tuning techniques, such as SpIEL [107] complement these efforts by selectively updating\nonly the most impactful parameters. SpIEL fine-tunes models by only changing a small portion of the\nparameters, which it tracks with an index. The process includes updating the parameters, removing the\nleast important ones, and adding new ones based on their gradients or estimated momentum using an\nefficient optimiser.\nData Efficient Fine-Tuning (DEFT)\nTo address the scalability challenges, recently the concept of DEFT has emerged. This novel approach\nintroduces data pruning as a mechanism to optimise the fine-tuning process by focusing on the most\ncritical data samples.\nDEFT aims to enhance the efficiency and effectiveness of fine-tuning LLMs by selectively pruning the\ntraining data to identify the most influential and representative samples. This method leverages few-shot\nlearning principles, enabling LLMs to adapt to new data with minimal samples while maintaining or even\nexceeding performance levels achieved with full datasets [108].\nKey Components of DEFT\nHigh Accuracy Through Influence Score: DEFT introduces the concept of an influence score to\nevaluate and rank the importance of each data sample in the context of LLM fine-tuning. The influence\nscore estimates how removing a specific sample would impact the overall performance of the model. This\napproach allows for the selection of a small subset of data that is highly representative and influential,\nthereby enabling the model to maintain high accuracy with significantly fewer samples.\nHigh Efficiency Through Effort Score and Surrogate Models: To address the cost and complexity\nof evaluating large datasets, DEFT employs a surrogate modela smaller, computationally less intensive\nmodelto approximate the influence scores. This surrogate model helps estimate the impact of each\nsample without the heavy computational burden associated with directly using the LLM. Additionally,\nDEFT introduces an effort score to identify and prioritise more challenging samples that may require\nspecial attention from the LLM. This dual-score system ensures that the fine-tuning process remains\nboth efficient and effective.\nPractical Implications and Use Cases\n Few-Shot Fine-Tuning for Rapid Adaptation: DEFT is particularly beneficial for applica-\ntions where models need to quickly adapt to new data with minimal samples. In scenarios such as\n98\npersonalised recommendations or adapting to sudden changes in user behaviour, DEFT allows for\nrapid fine-tuning, maintaining high performance with a fraction of the data typically required.\n Reducing Computational Costs in Large-Scale Deployments: By focusing on the most\ninfluential data samples and using surrogate models, DEFT significantly reduces the computational\nresources needed for fine-tuning. This makes it feasible to maintain high-performing LLMs even in\nlarge-scale deployments where data volumes are substantial.\nFuture Directions\nThe DEFT introduces a data pruning task for fine-tuning large language models (LLMs), setting the\nstage for new research into efficient LLM-based recommendation systems and presenting numerous op-\nportunities for future exploration. Key areas for further investigation include:\n Applying the proposed DEALRec[109] approach to a broader range of LLM-based recommender\nmodels across diverse cross-domain datasets, thereby enhancing fine-tuning performance within\nresource constraints.\n Addressing the limited context window of LLMs by selectively focusing on the most informative\nitems in user interaction sequences for fine-tuning purposes.\n12.1.3\nHardware and Algorithm Co-Design\nCo-designing hardware and algorithms tailored for LLMs can lead to significant improvements in the\nefficiency of fine-tuning processes. Custom hardware accelerators optimised for specific tasks or types of\ncomputation can drastically reduce the energy and time required for model training and fine-tuning.\n Custom Accelerators: Developing hardware accelerators specifically for the sparse and low-\nprecision computations often used in LLM fine-tuning can enhance performance. These accelerators\nare designed to efficiently handle the unique requirements of LLMs, such as the high memory\nbandwidth and extensive matrix multiplications involved in transformer architectures.\n Algorithmic Optimisation: Combining hardware innovations with algorithmic optimisation\ntechniques, such as those that minimise data movement or leverage hardware-specific features\n(e.g., tensor cores for mixed-precision calculations), can further enhance the efficiency of fine-tuning\nprocesses.\n Example: NVIDIAs TensorRT3 is an example of hardware and algorithm co-design in action.\nIt optimises deep learning models for inference by leveraging NVIDIA GPUs capabilities, signifi-\ncantly speeding up the process while reducing the resource requirements. TensorRTs optimisations\ninclude support for mixed-precision and sparse tensor operations, making it highly suitable for fine-\ntuning large models.\nAs the scale of language models continues to grow, addressing the challenges of fine-tuning them efficiently\nbecomes increasingly critical. Innovations in PEFT, sparse fine-tuning, data handling, and the integration\nof advanced hardware and algorithmic solutions present promising directions for future research. These\nscalable solutions are essential not only to make the deployment of LLMs feasible for a broader range of\napplications but also to push the boundaries of what these models can achieve.\n12.2\nEthical Considerations in Fine-Tuning LLMs\n12.2.1\nBias and Fairness\nWhen fine-tuning LLMs, the goal is often to optimise their performance for specific tasks or datasets.\nHowever, these datasets may inherently carry biases that get transferred to the model during the fine-\ntuning process. Biases can arise from various sources, including historical data, imbalanced training\n", " also to push the boundaries of what these models can achieve.\n12.2\nEthical Considerations in Fine-Tuning LLMs\n12.2.1\nBias and Fairness\nWhen fine-tuning LLMs, the goal is often to optimise their performance for specific tasks or datasets.\nHowever, these datasets may inherently carry biases that get transferred to the model during the fine-\ntuning process. Biases can arise from various sources, including historical data, imbalanced training\nsamples, and cultural prejudices embedded in language. For instance, an LLM fine-tuned on a dataset\nprimarily sourced from English-speaking countries might underperform or make biased predictions when\n3https://docs.nvidia.com/tensorrt/index.html\n99\napplied to text from other linguistic or cultural backgrounds. Google AIs Fairness Indicators tool4 is a\npractical solution that allows developers to evaluate the fairness of their models by analysing performance\nmetrics across different demographic groups. This tool can be integrated into the fine-tuning pipeline to\nmonitor and address bias in real-time.\nAddressing Bias and Fairness\n Diverse and Representative Data: Ensuring that fine-tuning datasets are diverse and repre-\nsentative of all user demographics can help mitigate bias.\n Fairness Constraints: Incorporating fairness constraints, as suggested by the FairBERTa frame-\nwork5, ensures that fine-tuned models maintain equitable performance across different groups.\n Example Application: In healthcare, an LLM fine-tuned to assist in diagnosing conditions might\ninitially be trained on data from predominantly white patients. Such a model could produce less\naccurate diagnoses for patients from other racial backgrounds. By using fairness-aware fine-tuning\ntechniques, healthcare providers can develop models that perform more equitably across diverse\npatient populations.\n12.2.2\nPrivacy Concerns\nFine-tuning often involves using sensitive or proprietary datasets, which poses significant privacy risks. If\nnot properly managed, fine-tuned models can inadvertently leak private information from their training\ndata. This issue is especially critical in domains like healthcare or finance, where data confidentiality is\nparamount.\nEnsuring Privacy During Fine-Tuning\n Differential Privacy6: Implementing differential privacy techniques during fine-tuning can pre-\nvent models from leaking sensitive information.\n Federated Learning7: Utilising federated learning frameworks allows models to be fine-tuned\nacross decentralised data sources, which enhances privacy by keeping data localised.\n Example Application: In customer service applications, companies might fine-tune LLMs using\ncustomer interaction data. Employing differential privacy ensures that the model learns from these\ninteractions without memorising and potentially leaking personal information, thus maintaining\ncustomer confidentiality.\n12.2.3\nSecurity Risks\n Security Vulnerabilities in Fine-Tuned Models: Fine-tuned LLMs are susceptible to secu-\nrity vulnerabilities, particularly from adversarial attacks. These attacks involve inputs designed to\nexploit model weaknesses, causing them to produce erroneous or harmful outputs. Such vulnera-\nbilities can be more pronounced in fine-tuned models due to their specialised training data, which\nmay not cover all possible input scenarios.\n Recent Research and Industry Practices: Microsofts Adversarial ML Threat Matrix pro-\nvides a comprehensive framework for identifying and mitigating adversarial threats during model\ndevelopment and fine-tuning. This matrix helps developers understand the potential attack vectors\nand implement defensive strategies accordingly.\n Enhancing Security in Fine-Tuning:\n Adversarial Training: Exposing models to adversarial examples during fine-tuning can\nenhance their robustness against attacks.\n Security Audits: Regularly conducting security audits on fine-tuned models can help iden-\ntify and address potential vulnerabilities.\n4https://research.google/blog/fairness-indicators-scalable-infrastructure-for-fair-ml-systems/\n5https://huggingface.co/facebook/FairBERTa\n6https://privacytools.seas.harvard.edu/differential-privacy\n7https://research.ibm.com/blog/what-is-federated-learning\n100\n12.3\nAccountability and Transparency\n12.3.1\nThe Need for Accountability and Transparency\nFine-tuning can significantly alter an LLMs behaviour, making it crucial to document and understand\nthe changes and their impacts.\nThis transparency is essential for stakeholders to trust the models\noutputs and for developers to be accountable for its performance and ethical implications.\n12.3.2\nRecent Research and Industry Practices\nMetas Responsible AI framework8 underscores the importance of documenting the fine-tuning process\nand its effects on model behaviour. This includes maintaining detailed records of the data used, the\nchanges made during fine-tuning, and the evaluation metrics applied.\n12.3.3\nPromoting Accountability and Transparency\n Comprehensive Documentation: Creating detailed documentation of the fine-tuning process\nand its impact on model performance and behaviour.\n Transparent Reporting: Utilising frameworks like Model Cards9 to report on the ethical and\noperational characteristics of fine-tuned models.\n Example Application: In content moderation systems, LLMs fine-tuned to identify and filter\nharmful content need clear documentation and reporting. This ensures that platform users and\nregulators understand how the model operates and can trust its moderation decisions.\n12.3.4\nProposed frameworks/techniques for Ethical Fine-Tuning\nFrameworks for Mitigating Bias\nBias-aware fine-tuning frameworks aim to incorporate fairness into the model training process. Fair-\nBERTa, introduced by Facebook, is an example of such a framework that integrates fairness constraints\ndirectly into the models objective function during fine-tuning. This approach ensures that the models\nperformance is balanced across different demographic groups.\nOrganisations can adopt fairness-aware frameworks to develop more equitable AI systems. For instance,\nsocial media platforms can use these"], "parameters": {"model": "nomic-embed-text:v1.5"}}, "key": "embeddings_7050152de5380867ec980d7f668f3e540bdf587b53a6fb074771d7901a4c0492_v2"}