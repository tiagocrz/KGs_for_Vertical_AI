{"result": {"data": [{"embedding": [0.026972802, 0.037076928, -0.14620557, -0.09665785, 0.060047027, 0.035190184, 0.014048739, 0.037990294, -0.032127287, 0.028836925, -0.031056788, 0.02044618, 0.08884255, -0.00020414458, -0.0020151597, -0.0068181762, 0.037543405, -0.09045811, -0.023189694, -0.036245022, 0.022609439, -0.016521756, 0.005692491, -0.027346792, 0.0012292197, 0.017207842, -0.011982584, -0.02112613, 0.051915478, -0.018226923, 0.06614232, -0.020347081, -0.036972553, -0.030694354, -0.049627572, -0.040496472, 0.034139454, 0.02680716, -0.037093636, 0.048228152, 0.10053794, 0.006438919, 0.01738094, 0.007184432, 0.015971784, -0.017756626, 0.034235187, -0.012941638, 0.030954251, -0.06227105, 0.06986204, 0.017281253, -0.008181513, 0.044887938, 0.08364111, 0.046836175, -0.042785082, 0.05350485, 0.008210453, -0.08338347, 0.07150888, 0.008163552, -0.07500812, 0.03659911, -0.019459734, 0.002533392, -0.023812067, 0.021829214, 0.019314732, -0.024463901, 0.004457805, -0.0042033135, -0.0059072073, 0.0077470136, -0.016147837, 0.05208498, -0.017007295, 0.0025320686, -0.0034978683, 0.037340954, 0.0010844372, 0.055530887, 0.06297932, -0.04948388, 0.006695683, 0.0019298804, -0.06881517, -0.052502554, -0.051731873, 0.06688181, 0.0075286645, 0.019148353, -0.014812328, 0.014729326, -0.04499098, 0.007817187, -0.07124299, 0.010544777, -0.0459266, -0.011009928, -0.028132107, -0.016592551, -0.007581006, 0.008351593, 0.014969565, 0.040501505, 0.029944783, -0.011496489, -0.026434042, -0.022951627, -0.05595067, -0.011664664, -0.028207444, -0.01956181, -0.055132855, 0.0018657092, 0.12819347, -0.015425634, -0.006827894, 0.045513056, -0.00936249, -0.033621263, 0.007197449, -0.05997397, 0.025003284, -0.002423523, -0.08074086, -0.005616011, -0.055069227, -0.0032140843, 0.012818997, 0.015163962, -0.03168117, -0.012087318, 0.037402242, 0.06674143, -0.0042509167, -0.018069806, 0.061526883, 0.012077268, 0.022613343, 0.026272938, -0.0115909325, 0.015949123, 0.020003898, -0.017286174, 0.026757741, -0.02287938, -0.014141168, 0.0023392749, 0.030211983, 0.043124795, 0.013548667, 0.04511203, 0.007652053, 0.0102143055, -0.008543073, 0.010168089, 0.023767821, -0.025363913, 0.0301149, -0.025966885, -0.01993031, 0.048223343, -0.007950214, 0.02275603, 0.019946566, -0.017166015, 0.023857754, 0.046946507, -0.08506214, -0.04463782, 0.018361393, -0.0033458436, 0.011006884, -0.017027333, 0.078273766, -0.05886357, 0.009402948, -0.040168446, 0.027955195, -0.09388863, 0.013589265, 0.00456728, -0.043307167, -0.033751063, 0.029333834, -0.010406672, 0.022375152, -0.04034701, 0.041237738, 0.04686689, -0.05640321, -0.034962166, -0.041574243, -0.048861753, 0.037481904, -0.03901331, 0.016251907, -0.006182183, -0.019463193, 0.00043608245, -0.085149154, 0.045684062, -0.06440214, 0.08249291, 0.031607714, 0.004181335, -0.005315002, -0.009371949, 0.05016734, 0.006238405, 0.02365196, 0.0002242312, 0.013088404, 0.03478914, -0.027635453, 0.019359222, 0.010301608, 0.0083296085, 0.004885281, 0.008312406, 0.030250609, 0.0073894504, 0.034487717, 0.0031677417, -0.041156463, -0.008516849, -0.03408636, -0.009671551, -0.00749799, -0.06637112, 0.03604836, -0.03713867, 0.00407541, 0.04622222, -0.00046501707, 0.019653054, -0.022152344, -0.0012153218, -0.025277833, 0.072178476, -0.05605459, 0.013223054, -0.07476915, -0.03125585, 0.019707706, -0.0016950497, -0.032105017, 0.059792917, -0.013149134, 0.012064676, 0.035110205, -0.013820695, 0.039619945, -0.030492773, -0.0249407, 0.04752291, 0.048404727, 0.037979826, 0.020138716, -0.016157169, 0.0026119691, -0.010500591, -0.041060787, -0.025634287, 0.00034954268, -0.055052217, 0.029598359, -0.003764784, 0.028644374, 0.020463848, 0.021225732, 0.047211297, 0.00095339725, 0.03985703, -0.07625447, -0.010258841, -0.016122485, 0.016927881, -0.007651039, -0.09000802, -0.0044187554, -0.0077483584, 0.045874756, 0.024163604, -0.0053937864, -0.013034875, 0.014790746, -0.04658567, 0.014926944, 0.040110342, -0.030245015, 0.00033743487, 0.010344022, 0.026897253, 0.00049087784, -0.010320871, 0.05836497, -0.046599258, 0.031654775, -0.001658609, 0.039334297, 0.040435802, -0.012677955, -0.0025913985, -0.027341468, 0.0360315, 0.037426796, -0.018399706, -0.014428566, -0.017029991, -0.09517626, -0.0013303184, -0.0006415977, 0.027264407, 0.03211964, 0.01635905, 0.03544054, -0.038891755, 0.015418288, -0.07443774, 0.013024946, -0.0474645, 0.022389274, 0.013449354, 0.021618709, -0.012371011, -0.012030005, -0.014369503, 0.08027977, 0.026212692, 0.040413838, -0.05318205, -0.04002314, 0.027663022, -0.0062175053, -0.026789872, 0.017081572, 0.022992028, 0.05822249, -0.013527151, 0.027242409, -0.023700904, -0.054240953, -0.035362512, -0.059350185, -0.05456077, 0.041394938, 0.04127997, 0.011538015, -0.004585812, -0.015151466, -0.034934677, -0.01578464, -0.017979305, 0.016872918, 0.066421606, -0.0023997542, -0.031132177, 0.014998586, 0.015603511, 0.0031377184, -0.026275959, -0.0073182224, 0.028035974, 0.036972422, -0.0015217891, 0.023438076, 0.0058626947, 0.029124899, -0.048007734, -0.04075273, 0.01094342, -0.015343839, 0.03748689, -0.05017536, 0.03546003, -0.03060319, 0.038563207, 0.042771965, -0.00432148, -0.004569985, 0.0007328899, -0.0029021264, 0.013000946, 0.037412502, -0.03316538, -0.0025453707, -0.010866749, 0.008559411, -0.037792865, -0.038388014, 0.018131506, 0.03200391, -0.00048124057, 0.016862532, -0.0025666086, -0.0117818825, 0.042981718, -0.059539523, -0.03647214, 0.025679927, 0.036149096, -0.03909087, -0.012266815, -0.040705353, -0.0243213, 0.003603145, 0.023599284, -0.0072664996, 0.090400934, -0.019087369, -0.019352619, 0.030942017, -0.012066131, 0.004286174, -0.0435366, 0.011430629, -0.030064996, 0.024449112, 0.008691074, 0.021719374, 0.015225456, 0.011767802, 0.021134514, 0.06867762, 0.05119786, -0.025777036, -0.09267917, 0.043521937, 0.02334962, 0.05971034, 0.047525078, -0.003919491, 0.0008715925, 0.03455301, 0.0078065433, -0.023209464, -0.011798012, 0.00803693, -0.048733722, -0.025022687, -0.03680945, 0.008735289, 0.054173194, 0.058834977, 0.009055421, -0.020097036, 0.03844697, 0.041014493, 0.02670605, 0.035306755, 0.052991632, 0.08456265, -0.0010174187, 0.057198517, 0.03432899, -0.0038098425, -0.014948546, 0.04821719, 0.006801045, -0.06228597, 0.00083812163, 0.0054874895, -0.04254712, -0.013942833, -0.017558845, 0.022354906, 0.06019354, -0.04561213, -0.002238912, 0.004739864, -0.008534486, -0.01729056, -0.012565809, -0.041581653, -0.002068892, 0.042667523, 0.006925263, 0.011816918, -0.019433243, -0.035240073, -0.027736895, 0.011326735, 0.024127565, 0.014935895, -0.038133003, -0.013164327, 0.061265014, -0.0043972535, 0.014145275, 0.039352212, 0.0012132248, -0.034370083, 0.040105566, 0.008061134, 0.0015466606, 0.031238908, 0.008703629, 0.049998827, 0.07256162, 0.011564209, -0.042899255, -0.024457447, -0.010955877, 0.026232786, -0.04788117, -0.045005735, -0.036193285, 0.026723597, 0.047634996, -0.023512693, 0.060370483, 0.040390752, -0.054084525, 0.048749007, 0.040466618, -0.024285542, 0.011471918, -0.01296225, -0.05859006, 0.017562773, -0.034441262, -0.073947236, 0.0062288404, 0.012416668, -0.025789235, 0.02952885, 0.025748987, 0.0122212265, 0.04738425, -0.06148854, -0.022981504, 3.523363e-05, -0.003630028, -0.053087085, 0.022665162, 0.04621835, 0.01761593, 0.023535613, -0.0010742183, 0.011161057, 0.01947722, 0.036558095, -0.039210204, 0.06285585, 0.036394633, 0.022866169, -0.08842891, 0.006991214, -0.07773814, 0.042908434, 0.013765137, 0.041684985, -0.05046669, -0.052163344, -0.03291081, -0.04038629, -0.013748891, 0.03666303, 0.017244887, 0.1158265, 0.024932915, -0.0412155, 0.019150196, -0.03406752, -0.023117183, 0.005372609, 0.04482366, 0.036164813, -0.050144125, -0.017594801, -0.009260644, 0.0073014107, -0.014811837, 0.014925858, -0.012239152, -0.019549679, -0.06866919, -0.009740092, -0.037600342, 0.07411311, 0.037166804, -0.021724785, 0.029716326, -0.08668569, -0.06731729, 0.008424445, -0.013930999, -0.0077695847, -0.029934134, 0.026126739, -0.035965856, 0.036591098, -0.021372328, -0.01417277, -0.04600809, -0.014930911, -0.057312947, -0.03491417, 0.005762225, 0.0069362666, -0.0715573, 0.028330069, 0.014048963, -0.00048300336, 0.038015097, 0.05668878, 0.0010663047, 0.03223069, -0.040620312, -0.0627538, -0.008487758, -0.016392699, -0.010596015, 0.06796387, 0.019842764, -0.02650673, 0.02615487, -0.019577892, -0.02457394, 0.053067405, -0.012296222, -0.038504496, 0.032036237, -0.057953622, 0.0031148253, -0.016364789, 0.03619745, -0.06871076, -0.02214174, -0.030696869, -0.016904265, -0.04868109, -0.0075702555, -0.0060485154, -0.017782906, 0.008428934, 0.015694076, 0.067417815, -0.010191756, -0.02906791, 0.00901871, 0.0141055435, -0.008973669, 0.028349046, 0.031682782, 0.061526746, 0.0024928548, 0.02981233, 0.08356822, 0.0028212136, -0.063837275, -0.00028184787, 0.021596443, 0.009820744, -0.0053986358, -0.01826188, 0.027447455, 0.02536411, 0.003950277, -0.024833873, -0.053249374, 0.046576183, -0.017198961, -0.010896434, 0.029387854, 0.0065230937, -0.010866085, 0.028587015, 0.02395797, 0.010804796, -0.008672102, 0.030274833, 0.0247535, 0.06161899, 0.013045005, 0.003818789, 0.013837247, 0.0035923708, -0.0011322482, -0.009706714, -0.030834831, 0.0032843915, -0.06338771, 0.024088463, -0.029304596, -0.07273092, -0.0058882073, -0.054823197, -0.025450323, -0.029586697, -0.0043472224, -0.01426743, 0.027857805, 0.003158347, -0.02559945, 0.023597568, 0.033485916, -0.019772166, 0.014153685, -0.0070039686, -0.0033613772, -0.007739626, -0.0074149338, 0.0024577524, 0.0019575723, 0.019729616, -0.04427831, -0.053748176, 0.017163556, -0.067258894, 0.044849485, 0.013072205, 0.001308601, -0.02291329, -0.040568814, -0.005597303, -0.016456867, 0.056463465, 0.032866403, -0.015109351, -0.005536584, 0.051187117, -0.0368767, 0.033304304, -0.0150913, -0.0011291938, 0.0023652322, -0.04273295, -0.05566004, -0.07199459, 0.04407144, 0.0024677056, 0.0057353326, 0.0036290532, -0.038655262, -0.04865268, -0.0067542004, -0.0465014, 0.0023498086, -0.010656398, 0.051218104, 0.012633629, -0.05628238, -0.018030383, 0.067089304, -0.008526368, -0.04961019, 0.020593809, -0.05847789, -0.045545243, 0.024601568, -0.0234851, 0.010650029, -0.013798857, 0.007443477, 0.09855314, 0.013357969, 0.011734499, -0.0044022696, 0.07268189, 0.041320942, -0.051259298, -0.028614322, -0.0093679, 0.008023977], "index": 0, "object": "embedding"}, {"embedding": [0.022967918, 0.06500962, -0.17756625, -0.101791635, 0.05137046, -0.030654142, -0.0045209266, -0.010470973, -0.003720172, -0.021266641, -0.021803834, 0.0194892, 0.046437778, -0.028380267, -0.018151553, 0.049398247, -0.0065904395, -0.07646373, -0.011783912, -0.037040144, -0.03597214, -0.049435068, -0.00020210595, -0.06011275, -0.044422485, -0.01983217, -0.0024063855, -0.0043483432, 0.0706387, 0.018930055, 0.044691898, -0.025087183, 0.006272789, -0.07838605, -0.027405666, -0.017719943, 0.026415803, -0.0068669757, -0.019069014, 0.037501574, 0.09824789, -0.04901355, 0.005064293, -0.03023136, 0.033600196, 0.019818315, 0.044208486, 0.013986815, 0.04324692, -0.042013835, 0.0519492, -0.032425173, 0.002009602, 0.04059952, 0.07845694, 0.013333918, -0.051404905, 0.07521513, -0.0065673976, -0.04704707, 0.14309451, 0.038278937, -0.043091133, 0.052036174, -0.0071132784, -0.016679905, -0.048532646, 0.016742248, 0.0021878246, -0.03767672, 0.013464734, 0.01591924, 0.025419181, -0.029327322, -0.038772304, 0.018685974, -0.020899754, 0.010035117, 0.0053200326, 0.005851669, 0.0085669635, 0.046957787, 0.08605181, -0.051544968, 0.06410946, 0.053851835, -0.005274022, -0.0011365092, -0.02228901, 0.06904694, -0.03359267, -0.0043055546, 0.06729188, 0.01511921, -0.051195197, 0.031127965, -0.011009943, 0.018457033, -0.037134755, -0.02300691, -0.026536632, -0.0054567275, -0.0374664, 0.041543618, 0.0053604282, 0.022175385, -0.0013769586, 0.018293638, 0.016586425, 0.020501459, 0.00598504, 0.019624075, -0.049740996, 0.010789511, -0.043298144, -0.050983094, 0.09182331, -0.013142078, 0.018893994, 0.025386892, -0.02546469, -0.024548527, 0.05106002, -0.041136086, 0.066725545, 0.052849945, -0.049970217, 0.018410033, 0.02512334, -0.021651922, 0.006717122, -0.004042412, -0.07722697, 0.029631998, 0.023412127, 0.08771082, -0.022742886, -0.0010541339, 0.013648377, 0.018882135, 0.009561794, 0.025591405, -0.015608692, 0.03435483, 0.031776834, -0.025817363, -0.008321911, -0.063022554, -0.014172642, 0.051373646, 0.02814406, 0.04998701, -0.004142729, 0.044525925, 0.0396763, -0.0037052522, -0.021711785, 0.0008368327, 0.03302456, 0.013916212, 0.036453705, -0.026320742, -0.0013438872, 0.00042576174, 0.005346817, -0.010407261, 0.038985822, 0.05805715, 0.028729605, 0.0031915747, -0.03299595, -0.023571212, -0.008435354, -0.0046434673, 0.057167802, -0.013290494, 0.03174115, -0.031189771, -0.0019611816, -0.032524787, 0.080481924, -0.07534603, 0.0011894533, -0.0024228916, -0.020478718, 5.092618e-05, 0.028403802, -0.04995971, 0.040042568, -0.07436711, 0.0072989287, 0.048223294, -0.04690431, -0.014007145, -0.03971717, -0.03522956, 0.08425319, 0.049648646, 0.004799236, -0.010603077, 0.006754304, 0.01590383, 0.00849478, 0.05252139, -0.008836087, 0.06799034, -0.02049428, -0.002758252, -0.008625214, -0.009102758, 0.047369033, -0.046085145, 0.023082376, -0.04026347, 0.04620164, -0.00445565, -0.008951063, 0.0058077932, -0.020563208, 0.014591342, 0.025386756, -0.0069486015, 0.07547645, -0.008049109, 0.044877462, 0.0028675264, -0.04040393, 0.022225127, -0.022805369, 0.009881338, 0.01917641, -0.054810442, 0.029122878, 0.027696451, 0.000225478, 0.05030936, -0.05841452, 0.0149818035, -0.014744366, 0.0076015056, 0.057447623, 0.02523257, -0.04169814, -0.007897837, -0.054115046, -0.042122405, -0.01343822, -0.0003517446, -0.022078812, 0.011111759, 0.009556968, -0.009849782, 0.03074667, -0.076546445, 0.015634242, -0.012875986, -0.009976591, 0.012747219, 0.022150652, 0.0096724145, -0.00716815, -0.0026557627, 0.05003857, -0.02016821, -0.02793035, -0.01305735, -0.03811697, -0.00037107436, 0.014628048, -0.009835663, 0.03507259, 0.031866584, 0.051169395, 0.07520651, 0.026694778, 0.038538232, -0.02566409, 0.0011204311, -0.013462825, 0.012110059, 0.0047607166, -0.10070247, -0.041343115, -0.03563861, 0.033238206, 0.012501823, -0.030530177, -0.015421077, 0.04502066, -0.031801768, 0.0651846, 0.039671674, -0.0347041, 0.013139607, -0.02916946, 0.0040818877, 0.04285302, 0.015374974, 0.016758436, -0.037720196, 0.03878477, 0.004659216, 0.05607827, 0.056187607, 0.00092164945, 0.042868707, -0.028433716, 0.015614689, 0.028964745, -0.015551857, -0.029656593, -0.05302486, -0.052150622, 0.026012167, -0.03101088, 0.014541289, 0.037885, -0.012314892, 0.008325485, -0.0493636, -0.024485784, -0.025522526, -0.032378655, -0.044088922, 0.005237111, 0.03099393, -0.004532633, 0.013670498, -0.016018346, -0.03421054, 0.030478243, 0.0106568355, 0.060426135, -0.064589955, -0.03742976, -0.020964203, 0.0016982164, -0.04411262, 0.017260332, 0.00607176, 0.033474587, -0.014802464, 0.0038051805, -0.03601236, -0.064382575, -0.019865267, -0.07093076, -0.06575635, 0.009634562, 0.0063679353, 0.002225568, -0.048013944, -0.011392387, -0.01623766, 0.03036877, -0.0027850608, 0.015824165, 0.06043962, 0.02640453, -0.0038244408, 0.020364545, 0.015819749, 0.059791815, -0.04201037, 0.024019035, 0.013449382, 0.060063843, -0.009518587, 0.011536804, 0.042257193, 0.012303239, -0.0026749035, -0.028596627, 0.016217487, -0.022746496, 0.017437207, -0.05075574, -0.019064005, -0.018946184, 0.021828076, -0.015158543, -0.023850854, -0.04229924, -0.046573397, -0.009040064, -8.8994246e-05, 0.06542411, -0.0017937848, -0.017423596, -0.0018184204, 0.016847912, -0.087040894, -0.056346424, 0.03686956, 0.026506707, 0.032579046, 0.037052583, -0.0425371, 0.083190486, 0.030947376, -0.01526213, -0.052673783, 0.024202814, 0.009071595, -0.02446595, 0.036755048, -0.026782654, -0.03913382, 0.0224219, -0.022576459, 0.010867779, 0.037065264, -0.02618769, -0.0365879, 0.027252406, -0.012775562, 0.029842822, 0.02149227, 0.0063208328, -0.005962903, 0.012913014, 0.0059033427, -0.007420471, -0.025346424, -0.026059138, 0.024947265, 0.027868325, 0.038670287, -0.010156011, -0.077368416, 0.03347223, 0.0056103035, 0.038920768, -0.013237182, 0.035052724, -0.04011041, 0.017965883, 0.027759153, -0.018127898, 0.019713063, 0.002446616, -0.0029137628, 0.023517441, -0.017494535, -0.014427, 0.05391774, 0.0260902, 0.0040188865, -0.04372714, -0.0030749678, 0.024216833, 0.019144315, 0.06963936, 0.0071460754, 0.052537397, 0.0053564804, 0.041503146, 0.02658646, 0.04199946, 0.030576143, 0.030735606, 0.02624423, -0.05936921, 0.00047489547, -0.012828666, -0.01873994, -0.0356868, -0.009251752, 0.03486923, 0.009467304, -0.02297452, -0.0121925995, 0.023751507, 0.011484622, -0.005106115, -0.0304302, -0.0093436465, 0.0026371125, 0.04985979, 0.0019657135, 0.048190515, 0.012425401, -0.028236302, -0.05677831, 0.028782934, 0.045324087, 0.037800763, -0.055491533, 0.0024287195, 0.0042358856, 0.03378929, 0.021937879, -0.036504924, 0.030839492, -0.017050961, 0.01215672, -0.026923284, 0.0012717017, 0.022259414, -0.008275874, 0.063786864, 0.086418256, 0.029806139, -0.069590494, -0.015893118, -0.04791317, 0.010580077, -0.022726495, -0.022501105, -0.0056288084, -0.008456666, 0.03523459, -0.038939837, 0.030626629, 0.06551821, -0.08770308, 0.009118649, 0.034595557, -0.017824275, 0.059607923, -0.026416203, -0.03255796, 0.024862591, -0.028426822, -0.065886, 0.003901788, -0.011269918, -0.012617872, -0.010245628, 0.00352942, 0.010879056, 0.047381528, -0.08389031, -0.06125523, -0.048792575, 0.003989102, -0.03604053, 0.028736468, 0.012679843, 0.06018985, -0.043932304, -0.074270464, 0.027720464, 0.03879993, 0.038332913, -0.06018061, 0.012614933, 0.0143631715, 0.043873444, -0.06475227, -0.0065912493, -0.061041445, 0.032610673, 0.017021487, 0.061252445, -0.028667394, -0.071041636, -0.010096391, -0.026246691, 0.0610129, 0.06512212, 0.027879614, 0.070634216, -0.038917273, -0.0006429577, 0.044445414, -0.00044561035, 0.008236383, -0.0017973966, 0.071479104, -0.0027307833, -0.058807403, -0.0091573885, -0.009411637, 0.038013525, -0.008785927, 0.02378938, -0.062152013, -0.029433448, -0.038332656, 0.012796186, 0.009802207, 0.053188406, 0.0032092277, 0.016091244, 0.00961775, -0.08202422, -0.07479165, 0.0351025, -0.07199512, 0.008555243, -0.0110153705, -0.0005825204, 0.00020723454, 0.012196286, 0.01202262, -0.0005170272, -0.027368933, -0.025373299, -0.057755195, -0.0040905876, 0.016239608, 0.0064966055, -0.04435395, 0.015412113, 0.025878068, -0.011940991, 0.036034945, 0.016724376, -0.033289984, 0.004241446, -0.009947129, -0.07214427, -0.0006379302, 0.02122219, -0.039058197, 0.054564446, 0.010829075, -0.028559918, -0.027625492, -0.031380445, -0.036791746, 0.0022462846, -0.0011582121, 0.0011390192, -0.0010150328, -0.024183556, -0.012441202, 0.022539666, 0.060647488, -0.022119094, 0.01117767, -0.013864286, -0.07107165, -0.073460326, 0.037324995, -0.01325886, -0.05241235, -0.0038591654, 0.0071060928, 0.042701356, -0.02923633, -0.0056190374, 0.010964482, 0.022300133, -0.0030465068, 0.0507853, 0.025842, 0.03904022, 0.02082218, 0.005274652, 0.0758351, -0.013483477, -0.059741553, -0.008307959, -0.0024040358, -0.018901872, -0.0058607156, -0.019445954, -0.014259901, 0.00031031942, 0.010720661, -0.013297316, -0.024888212, 0.04561896, -0.016037516, -0.04216158, -0.007230383, -0.053665247, -0.026187904, 0.015718745, 0.01689057, -0.011784237, -0.044641815, 0.029644204, 0.007132403, 0.024805864, 0.03705875, -0.030204136, 0.008680996, -0.007285894, -0.0070161773, -0.018638598, 0.009043723, -0.0022164513, -0.027519556, 0.044449158, -0.01702451, 0.0040666666, -0.049127817, -0.01833103, -0.029829815, -0.0034549662, 0.023903923, 0.012807035, 0.04824659, 0.026399374, -0.04035943, 0.022023072, 0.059030935, -0.030483479, 0.008427519, 0.044257194, 0.014619794, 0.02545192, -0.032357782, 0.01994965, 0.006291283, -0.008537846, -0.04690496, -0.040229063, 0.018234015, -0.008530198, 0.07285118, 0.00696439, 0.012203908, -0.043519616, -0.010861286, 0.012228032, -0.050722376, 0.055022266, 0.007048793, -0.026050722, -0.02393047, 0.047346663, -0.03777709, 0.04048265, -0.018370878, 0.010624488, -0.008531218, -0.019655854, -0.03972374, -0.023848848, 0.028532404, 0.022901488, -0.03415796, -0.010010192, -0.03702987, -0.04590312, -0.02766377, -0.06944673, -0.020386534, -0.05338072, 0.04626914, 0.01765438, -0.0039683697, -0.030385254, 0.022710608, -0.055127546, -0.043735694, 0.02224667, -0.025883615, -0.020954268, 0.007833732, -0.03291593, -0.0077467947, 0.0007164159, 0.008728285, 0.042955432, -0.014500306, -0.0074946396, -0.030227482, 0.09918674, 0.060325965, -0.024562128, -0.07921724, 0.009859852, 0.012157283], "index": 1, "object": "embedding"}, {"embedding": [0.008835218, 0.08717228, -0.15761755, -0.06875671, 0.027176024, -0.011481635, 0.022942014, -0.0063347593, -0.032423526, -0.007216117, -0.017726155, 0.032573964, 0.07835089, -0.015509764, -0.019172085, 0.053968538, 0.031652316, -0.057271104, -0.007990922, 0.0011643799, -0.063919224, -0.04130478, 0.03953534, -0.037981465, -0.0020626246, 0.018369272, -0.038083248, 0.02110568, 0.017673036, 0.011278474, 0.08376967, -0.056857113, 0.0023614366, -0.0801383, -0.0242689, -0.015169596, 0.05254494, -0.031145751, 0.017306095, 0.053239197, 0.111584954, -0.03919797, 0.034815397, -0.004922782, 0.029499168, 0.0015650825, 0.057702232, -0.043389153, 0.007493231, 0.008566872, 0.04422571, 0.004645266, 0.002263622, 0.002019598, 0.08158605, 0.042585187, -0.06690137, 0.06543738, 0.024243595, -0.07790809, 0.10912046, 0.03238603, -0.09293851, 0.008560562, -0.035535403, -0.018219657, -0.037523188, 0.028589122, -0.024012666, -0.027317425, 0.0163417, 0.032932755, 0.013233156, 0.01585408, -0.009107559, 0.019218666, -0.0022934976, -0.018470751, -0.0049131177, 0.01823338, 0.027089834, 0.05237632, 0.069959626, -0.024946695, 0.03766767, 0.023224674, -0.07894388, 0.002550989, -0.038835786, 0.08189791, 0.0044941143, 0.087787926, 0.07413024, 0.03296784, -0.02582612, 0.029214213, -0.049437936, 0.0021433227, -0.047875073, -0.012937577, -0.007955123, 0.0015279122, -0.011465243, -0.007825757, -0.009341175, 0.03140769, -0.0009227817, 0.008451677, 0.008941421, -0.003678891, -0.058899485, -0.016596844, -0.036341295, -0.023723012, -0.065338895, -0.006835847, 0.059142306, -0.026087776, 0.0061003123, 0.052512158, -0.0019010744, -0.0242343, -0.012836934, -0.028350279, 0.032585997, 0.017222106, -0.06270264, 0.00091243064, -0.014428043, 0.0038732227, 0.018205062, -0.011805588, -0.079480454, 0.011842152, 0.02166296, 0.11377092, -0.025828825, -0.0145318825, 0.04673809, 0.018728578, -0.0066755535, 0.006595296, -0.038042556, -0.017117953, 0.011520616, -0.029367993, -0.047190297, -0.042656004, -0.014141999, 0.01699282, 0.01049795, 0.06784409, 0.018208778, 0.006891368, 0.052414246, -0.020239692, 0.0013093968, -0.03982788, 0.01738168, -0.0184355, 0.021636842, -0.025973137, -0.014957396, 0.016572395, 0.017753065, -0.047851965, 0.02573414, 0.012130361, 0.048830286, 0.068419, -0.100207314, -0.029422956, 0.013494041, -0.00074820773, 0.027576156, 0.0018923142, 0.054572877, -0.019018, -0.017532736, -0.03660348, 0.08912808, -0.05963074, -0.01577259, -0.015031762, -0.0031353286, -0.01794483, 0.017789485, -0.005065624, 0.02331211, -0.02456923, -0.01111012, 0.061266035, -0.059278354, -0.036834773, -0.029219253, -0.007621707, 0.08315363, -0.024705421, -0.010869598, -0.015402043, -0.008581313, -0.027392982, -0.04191488, 0.044768963, -0.06580568, 0.05177562, -0.015064455, 0.024730543, -0.01725432, -0.013670023, 0.029056853, -0.010182825, 0.011862863, 0.021790115, 0.009900537, -0.00043722166, -0.016394999, -0.017778326, -0.017948303, 0.0071930503, 0.047549564, -0.013499179, 0.02858158, 0.015874967, 0.030881781, 0.0029879697, -0.033552874, 0.02000429, -0.039153844, 0.026251566, -0.008622755, -0.039281167, 0.04765965, -0.009102491, 0.011053484, 0.012620945, -0.039115883, 0.041289043, -0.02281706, 0.02980389, 0.027297944, 0.04444817, -0.012364988, -0.028345378, -0.06334615, -0.009464325, -0.015716983, -0.017543707, 0.0058142184, 0.034479443, -0.009122515, -0.008532443, 0.06333401, -0.030170536, 0.0075377915, -0.018919427, -0.036707036, -0.0029281978, 0.026492024, 0.0031465252, 0.04505591, -0.009234797, 0.003573192, 0.020537252, -0.04834836, -0.017465042, 0.00893809, -0.033406373, -0.006526595, -0.013401865, 0.04093023, 0.032265685, 0.04578817, 0.06892888, 0.0217275, 0.009687433, -0.051255893, -0.0044793836, 0.0031560948, 0.01541826, 0.0054360335, -0.122574456, -0.023709795, -0.03664845, 0.031620648, 0.012284972, 0.024228469, -0.048752144, 0.03703703, 0.0025940118, -0.0011959742, 0.058764845, -0.031288892, -0.02724511, -0.01602191, 0.018596638, 0.025916982, -0.0048316615, 0.018957619, -0.00018970031, 0.018369243, -0.01402246, 0.05099499, 0.045031235, -0.024509698, -0.006539005, -0.019930141, -0.008388716, 0.017518483, -0.0056947595, -0.030923309, -0.022226028, -0.038633898, -0.0119745275, 0.005486219, -0.0115608545, -0.0033643136, -0.0018256729, 0.0046322243, -0.031665727, 0.013386697, -0.033140283, -0.0033970585, -0.049180236, -0.017709184, 0.03989996, 0.040528614, -0.02026294, 0.0031888508, 0.018782858, 0.0639627, 0.018063966, 0.03570693, -0.020388395, 0.0025211899, -0.009045498, -0.041621763, -0.014778124, 0.063566625, 0.022127863, 0.0018400886, -0.010186665, 0.035409886, -0.023597471, -0.048385084, -0.00054257223, -0.04772824, -0.052174453, 0.03753082, -0.016829044, -0.006119368, -0.007409282, -0.001460777, -0.027219813, 0.018325819, -0.039915506, 0.020522758, 0.029944317, -0.012152454, 0.004622238, 0.016909588, 0.016557684, 0.022159418, -0.037914384, 0.020276513, 0.0051034223, 0.04617459, 0.005863284, 0.05959836, -0.0100895045, 0.029980514, -0.02638468, -0.022081925, 0.012600198, -0.007019557, 0.03573885, -0.05152129, -0.020475317, -0.030904267, 0.004732039, 0.0039047503, -0.00800259, 0.0025257184, -0.015790548, 0.013391657, 0.01490644, 0.07521864, 0.008251629, 0.012216812, 0.019737564, -0.012344963, -0.0812486, -0.075481564, 0.004482936, 0.0037505974, -0.06686687, 0.08192765, -0.019975694, 0.009937131, 0.023521744, -0.0450017, -0.040524572, 0.053197898, -0.0011910453, -0.00079378404, -0.024934689, -0.03336213, -0.021384638, 0.0078704255, 0.01615119, 0.006986469, 0.052955147, 0.0060729147, -0.059543874, 0.006344737, 0.002515432, -0.013398031, 0.0072426903, -0.0033365206, -0.0229066, 0.01809233, 0.0059303804, -0.0037981423, -0.0054786447, 0.02880438, 0.032960847, 0.069420174, 0.0054178364, -0.021869712, -0.080489546, 0.036944717, 0.012186416, 0.03764122, -0.017690899, 0.027104817, -0.03584014, 0.024913248, 0.031690244, 0.0019122075, 0.045507498, 0.046124812, -0.040439066, 0.007143508, -0.05729255, -0.0013394164, 0.044546865, 0.045100164, 0.00088763185, -0.06889259, 0.011557284, 0.03081515, -0.009428324, 0.034071878, 0.042544782, 0.07640375, -0.0012482128, 0.03549382, 0.065334305, 0.059479278, 0.00993064, 0.05136086, 0.02303575, -0.08313457, -0.02327471, -0.020799939, -0.039603896, -0.011928914, -0.013368802, 0.031502504, 0.032274988, -0.048399594, -0.019332694, 0.035539955, 0.027890965, -0.03629834, -0.05493883, -0.0072162617, -0.012283861, 0.05073059, 0.009904202, 0.026699357, 0.0010536225, -0.009276111, -0.049396113, 0.043529835, 0.054127194, 0.013193797, 0.0029892283, 0.0012492811, 0.0066118706, 0.011225187, 0.013125532, 0.010635515, 0.03208362, -0.043799307, 0.015033421, 0.040101092, 0.0070624934, 0.014375303, 0.013572681, 0.030138208, 0.11181962, 0.0073285154, -0.055148482, 0.0033592063, -0.0445599, 0.026751542, -0.054909244, -0.030614873, -0.019316139, -0.010699625, 0.038292166, 0.018830704, 0.016362915, 0.010765211, -0.0722282, 0.037631113, 0.031546146, -0.009985551, 0.009652895, -0.005900002, -0.07744522, 0.011803557, -0.032312423, -0.099782124, 0.019135576, 0.02371223, -0.06583773, 0.033840977, -0.007936398, 0.040264674, 0.030592654, -0.08713005, -0.008072257, -0.024947502, 0.014236267, -0.03276708, 0.025831303, 0.015924314, 0.028263712, 0.019291103, -0.00185556, 0.043482114, 0.03451245, 0.020135162, -0.03169299, 0.010424735, 0.042163342, 0.060630817, -0.080052935, 0.017916737, -0.08295756, 0.05618061, 1.9563533e-05, 0.03564457, -0.06542727, -0.056546155, -0.059146777, -0.046399064, 0.015506048, -0.008045496, 0.021318795, 0.08214782, -0.040727697, -0.028430311, 0.0012176154, -0.016517809, 0.021067105, 0.039348785, 0.017875409, 0.07816924, -0.10217973, 0.00798401, -0.0007557365, 0.03000119, -0.037623778, 0.027201435, -0.006856007, -0.020891747, -0.058303807, -0.049163837, -0.019702477, 0.036624797, 0.025113933, -0.0053733783, 0.0176579, -0.0732642, -0.046646934, 0.021388179, -0.0545972, 0.01572679, -0.065701716, 0.054237496, -0.013689731, 0.0053078216, 0.0054457635, 2.7753598e-05, 0.00053832616, -0.028035227, -0.06389808, -0.005936378, 0.035626028, 0.031580713, -0.060908787, 0.0064642546, 0.028937025, -0.006756826, 0.019866174, 0.02546357, 0.0149859935, 0.018589184, -0.0062290127, -0.033199675, 0.0037645802, -0.00037875172, -0.024255648, 0.062321797, 0.023290602, -0.030206522, -0.0022723675, -0.018016513, -0.039104547, 0.018086832, -0.04441097, -0.03591433, -0.014110129, -0.05730852, 0.017825231, 0.06306932, 0.03228624, -0.034629706, -0.027847962, -0.02978018, 0.008038218, -0.030158522, 0.026537444, -0.017849404, -0.026179172, 0.03814118, 0.032837488, 0.023973988, -0.037380464, -0.031215556, 0.009075829, 0.03262273, -0.014537055, 0.023486933, 0.044392247, 0.047774516, -0.0048321886, 0.07109486, 0.086533174, -0.00656191, -0.024022434, -0.051754754, -0.0102486545, -0.004103169, -0.04079203, -0.03671976, 0.017593063, -0.024730887, -0.008757205, 0.008364734, -0.030371562, 0.02004916, 0.0031078476, -0.06605061, 0.0152306445, -0.018757444, -0.026418667, 0.02909819, 0.016650565, -0.03790599, 0.023072422, 0.034937333, -0.010158679, 0.016244914, 0.02592568, -0.002586352, -0.00782505, -0.009584198, -0.0130892815, -0.008412776, 0.0021946086, -0.011945142, -0.046315644, 0.020429663, -0.034960967, -0.033079933, 0.010711372, -0.04336032, -0.0049781622, -0.028149799, 0.022945283, -0.038016237, 0.041376952, 0.015177163, -0.013756637, 0.02683045, 0.06358193, -0.027007107, 0.012103107, 0.004458565, 0.018134492, -0.008264195, -0.011439561, 0.0026524705, 0.03404376, 0.01351358, -0.057607826, -0.05958077, 0.032592796, -0.055620715, 0.044024043, 0.06534079, 0.04023734, -0.007937347, -0.034675535, -0.007492123, 0.011454497, 0.03429354, -0.016706873, -0.014183681, -0.0047593303, 0.05007199, -0.070889734, -0.0016411655, -0.041845176, -0.023589578, 0.019068802, -0.020898197, -0.040069904, -0.028528253, 0.03369452, 0.009339809, -0.0051728585, -0.01597811, -0.002822853, -0.024114475, 0.014075178, -0.051967815, -0.0056202817, -0.012010511, 0.03439056, -0.0011882604, -0.0028107732, -0.02491072, 0.06307908, 0.007928409, -0.014797857, -0.026399603, -0.016256068, -0.015690716, -0.0077164345, -0.047043353, -0.014279282, 0.031221967, 0.00018142344, 0.103127174, 0.015376397, 0.005593489, -0.029989246, 0.07399926, 0.039595086, -0.040065862, -0.01835524, -0.026309583, -0.014231671], "index": 2, "object": "embedding"}, {"embedding": [0.06383875, 0.0727292, -0.13383389, -0.009350504, 0.008878141, 0.029359682, 0.098735645, 0.012340604, -0.026229816, 0.013415113, -0.00071652065, 0.018382704, 0.082990505, 0.029452087, -0.03323023, -0.015077357, -0.0014718454, -0.07647857, 0.0069852043, 0.08416708, -0.049516723, 0.033607468, 0.016911115, -0.01761822, 0.03456267, 0.020014357, -0.06953849, -0.018795816, -0.02786796, 0.03481621, 0.021346362, -0.03718997, 0.00080675556, -0.037479024, -0.029496936, -0.027783247, 0.07922171, -0.004599503, 0.08239353, -0.021294566, 0.042368133, 0.008194132, 0.06181706, -0.073082455, 0.04253122, -0.031390514, 0.078021586, -0.016776243, 0.028541362, -0.07470252, 0.02174671, -0.008091717, -0.0017521584, 0.0026589993, 0.01077973, -0.027329382, -0.05404941, 0.085656, 0.009256122, -0.028532134, 0.085690886, 0.00077578187, -0.061438974, -0.009538842, 0.0037152013, 0.014583968, -0.030968718, -0.031890035, 0.016451938, 0.040677324, 0.035695426, -0.014218577, 0.037765082, 0.019062843, -0.066956386, -0.011436438, -0.0154004125, -0.027397802, 0.004276997, 0.039950926, -0.008522694, -0.0029051679, 0.040312983, -0.017871821, 0.051442675, 0.008560455, -0.029735502, 0.047493324, -0.007796159, 0.0960065, -0.018575612, 0.022234842, 0.01985651, 0.06275173, -0.06268392, 0.0050725415, -0.040855013, -0.0063620186, 0.006234545, -0.021251414, -0.016246935, -0.057509042, -0.019308284, 0.021213198, -0.034582496, 0.029849421, 0.008820193, -0.019545602, 0.0011665632, -0.025937218, -0.036918692, 0.008915239, -0.031141197, -0.015857223, -0.02162068, -0.03249661, 0.06716851, -0.052651245, 0.002965218, 0.019461486, -0.042596914, -0.0051166187, 0.015206037, -0.008004333, 0.005217387, 0.080911905, -0.04807713, 0.006040144, 0.018712986, -0.03996678, -0.012121038, -0.03424301, -0.060259476, -0.011714915, -0.019859964, 0.09833877, -0.06706346, -0.018520342, -0.0033975986, 0.008859426, 0.0039107692, 0.03907029, -0.010495564, -0.0038494528, 0.0048642927, -0.007586218, -0.027387986, 0.0059181387, -0.01498734, 0.013067932, 0.036541313, -0.003876526, -0.019282864, 0.03718608, 0.015262838, -0.024835251, 0.04584423, 0.01455755, 0.073400244, 0.0012234581, 0.026312584, -0.03510987, 0.014537488, 0.015773715, 0.017951299, 0.0056550517, 0.026068656, 0.0102881, 0.0013098248, 0.008931734, -0.044644877, -0.0347436, 0.039139926, -0.0034805571, -0.0020016397, 0.023910498, 0.052707728, -0.07781178, 0.015795425, 0.008384368, 0.058901876, -0.017595086, 0.02999357, 0.013197136, 0.044060398, 0.001511148, 0.01989287, -0.029752592, 0.039417215, -0.036212448, 0.03924397, -0.018127555, -0.082028195, 0.0006068019, -0.052780736, -0.0003290589, 0.1040381, -0.07649855, -0.027754406, -0.022205884, -0.05786343, 0.0027741855, -0.039976127, 0.01874138, 0.004805055, 0.06383517, 0.044127844, 0.03028854, -0.012184565, 0.044406876, 0.06598477, -0.011898379, -0.00068944873, 0.017377207, 0.024539793, -0.024546932, -0.030771809, 0.031454004, -0.015181201, -0.012519394, 0.041181743, -0.014659987, -0.01672036, 0.023033956, 0.044011742, 0.011396405, -0.10899923, -0.01604993, -0.0120556, 0.0049196836, -0.056254927, -0.070762426, 0.040963463, -0.024759304, 0.02326278, 0.051438335, 0.002475111, -0.02798251, -0.0074900934, 0.02008979, -0.019651078, 0.00036048013, -0.011568551, 0.010869729, -0.07350177, -0.038283262, -0.009634378, -0.0098541835, 0.00032984288, 0.0117916735, 0.0065894094, 0.035205618, 0.07815722, 0.026900431, 0.052629087, -0.011987341, -0.028789649, 0.019566039, 0.036716588, 0.034206968, 0.0047771404, 0.01210505, 0.04248572, -0.006011821, -0.039808065, 0.011280332, -0.0150710475, -0.040297456, 0.021677786, -0.014434344, 0.023034327, 0.04384867, 0.035809293, 0.023805102, 0.019647695, 0.040372577, 0.011984694, -0.010657938, -0.015783409, 0.011815223, -0.0011489972, -0.09500218, -0.046381067, -0.0042822, 0.01914894, 0.01871939, 0.063739136, -0.008285595, 0.013230165, 0.014441596, -0.0033570023, 0.027852492, -0.009157973, -0.04669511, 0.031316627, 0.029046645, 0.034782548, -0.028273568, 0.018057879, 0.021170573, 0.0013699498, 0.00045321128, 0.07289721, 0.0070287935, -0.0116003305, -0.03577031, 0.0049480316, -0.05339494, -0.020820484, -0.003019597, 0.025071558, 0.029395387, -0.038992222, 0.023347456, -0.04736402, -0.01806733, 0.037995297, 0.038108207, 0.029362997, 0.04222013, 0.009890325, -0.03793858, -0.04302779, -0.023799954, 0.024637414, 0.06572099, -0.026905578, -0.00830942, 0.015543633, -0.033474706, 0.063300125, 0.040647715, 0.01592603, -0.002509716, -0.04315841, -0.045982283, 0.0025354528, -0.005316596, 0.008221533, 0.0110201, 0.013875368, -0.050348394, -0.016878879, -0.040777374, -0.047878146, 0.016424404, 0.031481598, -0.06409647, -0.015539115, 0.0001990452, -0.0016358355, -0.040171817, 0.0177933, 0.0019436118, -0.011769006, -0.012780218, 0.024503611, -0.021955648, 0.04198486, 0.004701058, 0.021045258, 0.019131066, -0.018266996, -0.0041403403, 0.01326981, 0.0013628294, 0.038876247, -0.007777987, -0.04276861, -0.033094484, -0.022234082, -0.014600855, -0.006882568, 0.005763008, 0.004187522, 0.040429264, -0.05853961, 0.012227381, -0.0051181493, -0.027702278, 0.009577612, -0.062370487, 0.0037981467, -0.03638202, -0.022438081, 0.0067437165, -0.027216053, -0.009745645, 0.014166934, 0.057417434, 0.010487303, -0.056465603, -0.033360574, 0.010643942, 0.003341751, -0.022167766, 0.06321567, 0.04085998, -0.0020950956, 0.064055674, -0.013089618, -0.04605892, -0.0012754404, 0.021313468, -0.005344388, 0.029828178, -0.05009926, -0.036837872, -0.03594604, -0.018869637, 5.279774e-06, 0.020845523, 0.022087492, -0.033898104, 0.026060475, -0.0059065465, 0.03210625, -0.028850667, -0.011973647, 0.021660116, 0.0021781835, 0.03863482, -0.008496283, -0.021946447, -0.045165177, 0.011776616, 0.050062772, 0.04864125, 0.007227232, -0.08067978, 0.036134034, 0.042297006, 0.021740532, -0.013275234, 0.029714566, 0.01387533, 0.02869206, -0.0032791106, -0.039693065, 0.02976118, 0.021093676, -0.010081599, -0.008873271, -0.024260558, 0.034258105, 0.06487901, 0.0019512748, -0.03671795, -0.057194714, 0.01645854, 0.008992053, 0.02707164, 0.06210844, 0.035693634, 0.085712135, -0.031059967, -0.039376993, 0.034733333, 0.016763097, 0.055517145, 0.04122667, 0.0368717, -0.04768972, 0.017539158, 0.001497235, -0.016331784, 0.011589053, 0.0007245652, 0.048630036, 0.053510476, -0.055140514, 0.027128914, 0.022132734, -0.021215312, -0.054731857, -0.012001209, 0.006103733, -0.002625439, 0.055253576, -0.00027252344, -0.013611153, -0.03826414, -0.023648685, -0.030788826, -0.031105181, 0.0005642008, 0.0036867703, -0.04569124, 0.008906952, -0.07904323, 0.009764757, 0.0008348297, 0.023611497, 0.0051487866, -0.053084992, -0.017204704, 0.031560007, 0.017121274, 0.009679643, -0.03530064, -0.05247404, 0.019643253, -0.019044288, -0.04191443, 0.037080843, -0.023805136, 0.06660549, -0.089243636, -0.046569027, 0.09758663, -0.0042143306, 0.00014551869, 0.040621296, -0.006742568, -0.006906582, -0.076351516, 0.046148323, -0.009751244, 0.008207244, -0.018686432, 0.033840336, 0.022655968, -0.080686785, -0.019192712, -0.0185515, -0.005945505, -0.050143383, -0.02638951, 0.011943345, 0.010130946, 0.031874973, 0.031201826, -0.023529135, 0.015785772, -0.034410827, -0.010960768, -0.061860017, 0.062159594, 0.021127054, -0.03097276, 0.023731386, 0.0147001045, 0.050780967, 0.0010517387, 0.011164273, -0.008700112, 0.024871197, 0.0035649315, 0.0214382, -0.08032479, -0.024633925, -0.036973163, 0.027421817, -0.038062964, 0.01407861, 0.017735342, -0.0031315198, -0.052740734, -0.0064528575, 0.009510251, -0.022644678, -0.038258914, 0.039157826, 0.020466054, 0.014548929, -0.033168904, -0.0054644537, 0.013172579, 0.002553673, 0.051422596, -0.007921994, -0.08393492, 0.044788737, 0.03232976, 0.022080366, -0.039114416, 0.018622706, -0.0017501644, -0.029204799, -0.054739356, -0.010304905, -0.010959365, 0.008767033, 0.031032506, -0.0011990683, 0.044083685, -0.049939174, -0.06198874, 0.035462823, -0.005430484, 0.029267853, -0.01880249, 0.013622062, -0.029065795, -0.01815958, -0.0422429, 0.0053367717, -0.014298301, -0.044401113, -0.06937758, -0.015019024, 0.054500557, 0.03795849, -0.06915039, -0.017601999, 0.05848854, 0.026545286, 0.055952147, 0.04401404, -0.00072271476, -0.016922392, 0.017564017, -0.018926842, -0.0134669375, 0.057858944, -0.039835263, 0.015292578, 0.012880708, 0.018528616, -0.09363202, -0.02737459, 0.009584249, 0.03535434, -0.026659142, -0.0019144535, -0.034023225, -0.007444963, -0.0027078749, 0.049671303, -0.008763341, 0.014236888, 0.05417121, -0.00871179, 0.03305355, -0.040447388, 0.025304724, -0.037128024, 0.074015334, -0.013422724, 0.07562556, 0.011798162, -0.03155969, -0.009924998, 0.0012929351, 0.014795106, -0.0028173486, 0.0397875, 0.089435115, 0.04605213, 0.005127029, 0.08053861, 0.08194497, 0.020020077, -0.0667765, -0.0041894168, 0.0062299627, 0.02013214, 0.019885855, 0.010092786, -0.0020568732, -0.022375919, 0.016928276, -0.022075998, 0.008913978, 0.027351655, -0.009518182, -0.021625131, 0.009753571, -0.049830757, -0.01110167, 0.06254828, -0.03166238, -0.03295329, -0.0030661637, 0.03225186, 0.039747864, -0.0214409, 0.0005395512, -0.015988665, 0.026549364, 0.042315196, -0.0037796174, 0.026337128, -0.05201833, -0.026790781, 0.010615557, -0.03207712, -0.072522044, -0.013099984, -0.07814559, -0.04309475, -0.010028885, 0.005509934, 0.0108476, -0.030437445, 0.015966564, -0.03131875, -0.04225621, 0.04662906, 0.088798456, -0.043823462, -0.0018822134, 0.013640475, -0.0038385035, -0.028806342, -0.015378629, 0.038567994, -0.012804225, -0.03572315, -0.05670388, 0.005773246, -0.0027195385, -0.0446647, 0.060160052, 0.0071793366, 0.01286249, -0.018190503, -0.06164975, -0.0039075133, 0.017686045, 0.06546789, 0.019118207, -0.008979505, -0.028493248, 0.03945363, -0.010674295, -0.016836584, -0.06751506, -0.02642216, -0.018316865, -0.03283696, -0.03907314, -0.03203023, -0.00014665924, 0.01421743, 0.020637436, 0.023164457, -0.016253477, -0.07721553, 0.016210577, -0.036274593, -0.0006394799, 0.021995815, 0.024786143, -0.0143110305, 0.008566755, -0.008957794, 0.011521379, 0.014700872, -0.06376561, 0.0040712254, -0.027421942, -0.07446308, -0.0090160575, -0.035572033, 0.018938351, -0.008880301, 0.052619904, 0.13174568, -0.011335772, -0.0039914693, -0.04313397, 0.008211208, 0.020789025, -0.036087506, -0.057310116, -0.030135088, -0.036784876], "index": 3, "object": "embedding"}, {"embedding": [0.06526779, 0.04629174, -0.11719632, -0.044778883, 0.005178555, -0.020161778, 0.07977435, 0.021874016, 0.025242578, 0.009861209, -0.020567138, 0.022867907, 0.13459516, 0.03296744, -0.0030654958, -0.017731592, -0.01783393, -0.033644598, 0.0105948625, -0.0005441793, -0.00333603, -0.04965721, 0.032042585, 0.008258604, 0.036251336, 0.03795463, -0.05889341, -0.012136336, -0.055815246, 0.052581858, 0.06370008, -0.007861231, -0.043537375, -0.024775477, -0.03906834, -0.019010307, 0.08140548, -0.007607603, 0.034687605, 0.03869976, 0.020798972, -0.020644331, 0.07005114, -0.0003507499, 0.04837991, -0.00839798, 0.01564917, -0.03315323, 0.056103658, -0.056919165, 0.014937204, -0.0029985674, -0.006140024, -0.0234046, 0.07806546, -0.018417528, -0.07147297, 0.010160572, 0.028969135, -0.08814708, 0.09665172, 0.0301372, -0.07082482, 0.046083782, 0.024109313, 0.012102451, -0.015689708, 0.03344128, 0.014229554, -0.01913938, 0.052428104, -0.013639502, -0.03965909, 0.01837457, -0.023123167, -0.0045386036, 0.016190503, -0.03591298, 0.0013173058, 0.04500115, 0.001536708, -0.002471874, 0.04581709, -0.016265687, 0.0020531497, -0.037954833, 0.0031382935, 0.037505887, -0.012191436, 0.07489868, 0.06417523, 0.036471725, 0.029966377, 0.047371633, -0.0471381, 0.027301108, -0.03301991, 0.03331935, -0.038852606, -0.010818882, -0.029550713, 0.036252417, 0.0023639244, 0.028130518, 0.01962468, 0.058097966, 0.018386522, -0.01474836, -0.024293695, 0.019348731, -0.04095705, 0.02483912, 0.004648915, -0.02013007, -0.04880102, -0.04752797, 0.05977289, -0.039561734, 0.008233232, 0.024261765, 0.049112063, -0.01486333, -0.03176736, 0.0032460145, -0.0038914408, 0.056438427, -0.051608976, -0.0077095795, -0.01336616, -0.026883898, -0.007600604, -0.02654961, -0.023380827, 0.01562035, -0.0020880084, 0.09378998, -0.03869272, -0.09402022, -0.040790804, 0.026277257, -0.022121904, 5.6972287e-05, 0.006683505, 0.01644372, 0.012716318, -0.047524534, 0.008877534, 0.0017929876, -0.02246715, 0.032302793, 0.051700693, 0.005888208, -0.022726143, 0.016045284, 0.031329717, -0.010279313, -0.0005341963, -0.0054564537, 0.071235284, 0.016560366, 0.041155517, -0.0069070235, -0.020571, 0.031740393, 0.0037121302, -0.032811947, 0.030250665, 0.040644985, 0.017725488, 0.0028890546, -0.069046415, -0.022787761, 0.06529168, -0.015826778, 0.030770708, 0.0038797592, 0.055813633, -0.054287795, -0.00558166, -0.056730367, 0.035856135, -0.052196726, 0.044243015, 0.018682897, 0.027040176, 0.014318881, -0.024412813, -0.025633767, 0.00808651, -0.04827263, 0.03675747, -0.011519065, -0.050359856, 0.00872043, -0.069713935, 0.008869035, 0.08392695, -0.038124047, -0.013261388, -0.012844895, 0.015468356, -0.05142831, -0.06796544, 0.037717897, 0.025702946, 0.042982634, 0.010890989, 0.050228447, 0.0038285758, 0.011115388, 0.069990106, 0.06714548, -0.00019298615, 0.03259665, 0.012775974, 0.039931607, -0.010983205, 0.009908291, 0.0037760122, 0.025608517, 0.022722844, -0.023709726, 0.03264919, 0.028816331, 0.033742666, -0.01567912, -0.0576984, 0.028968329, 0.00960235, 0.0022573185, -0.0038743566, -0.061817374, 0.0630517, -0.008453489, 0.025142556, 0.026929565, 0.019446818, 0.058841612, -0.015041015, 0.052142944, -0.009016646, 0.017439423, -0.045349862, -0.0012299857, -0.025461098, -0.030746508, -0.02148085, -0.023613835, -0.0003690232, 0.00020443578, 7.179663e-05, 0.021969879, 0.031663492, 0.004742626, 0.029809916, -0.020433603, -0.048283912, -0.02623636, 0.02589462, 0.001990011, 0.021073688, 0.0151312305, 0.006752995, -0.033681806, -0.038078673, -0.023864422, -0.01061058, -0.05481008, 0.059257306, 0.01636732, 0.0068970234, 0.04783437, 0.046373103, -0.010073356, 0.03455776, 0.06367074, -0.023372278, -0.061031613, 0.023398055, -0.019084264, -0.025858875, -0.049019825, -0.05373818, -0.024085704, 0.023304671, 0.07599312, 0.04039222, -0.00076774316, 0.0053133117, 0.009009179, 0.029264105, 0.010935, -0.025196413, 0.010019496, -0.0063306526, 0.017926153, 0.06827986, -0.024298169, 0.0038097296, -0.03120553, 0.021761794, 0.043883253, 0.06844538, 0.0056787324, 0.03871415, -0.03521392, -0.060504384, -0.008862315, 0.04387323, 0.006638621, -0.006465173, 0.018771814, -0.030666644, 0.01768487, -0.029111324, -0.005151509, 0.040602345, 0.0020197965, 0.06611677, -0.04563863, 0.0001546897, -0.058714792, -0.0008019434, -0.019613173, 0.017022409, 0.027817767, 0.02345872, 0.03930003, -0.03348829, 0.003607592, 0.06593776, 0.027025117, 0.030619211, 0.022576593, -0.07958763, -0.05956416, 0.023822678, -0.0057110833, 0.01991253, 0.026974909, 0.01152565, -0.0075955023, 0.005974486, -0.056257054, -0.02021847, 0.046545178, -0.0025576465, -0.030756872, 0.022628479, 0.0115276845, -0.00019035663, -0.021648854, 0.0035274094, -0.008237521, 0.023272738, 0.004615292, 0.021083653, 0.026633726, 0.015737459, -0.014634435, 0.043130524, -0.015446781, -0.0019466347, -0.049671005, -0.01931358, 0.018474083, 0.047138564, -0.0025168778, -0.020303795, -0.031493086, 0.06419212, -0.03585684, 0.023737479, 0.034614712, -0.020440225, -0.001853829, -0.030915048, 0.018862788, -0.004559384, -0.034298193, 0.003483448, -0.026758265, -0.02830909, -0.004782409, -0.052071176, 0.031815182, 0.0010429893, -0.043881748, -0.010004764, 0.004293772, -0.0122034, -0.00869172, -0.017625714, -0.04580747, -0.019157788, -0.015520096, 0.0111548, 0.01696266, -0.004967836, 0.054130908, -0.0821875, 0.010744952, 0.00045866528, -0.0069376137, -0.005308235, -0.016730336, -0.0731589, -0.030979032, 0.008631097, 0.0029670445, -0.03149455, 0.07604219, -0.007596178, -0.06891322, 0.03984771, -0.030232452, 0.09143871, 0.0036261354, -0.0102743395, -0.02693901, -0.02927399, 0.04480444, -0.0079943035, -0.008597832, 0.004857277, 0.0407596, 0.017440258, 0.04145105, 0.02981421, -0.050877348, 0.07773498, -0.024134463, 0.016863594, 0.004127108, -0.027600141, 0.0545843, 0.03659997, 0.008138607, 0.023020046, 0.006888174, 0.044204578, -0.014266096, 0.0038618576, -0.05624358, 0.015176465, 0.050745323, 0.042825267, -0.008750104, -0.048189867, 0.0138232745, 0.026642699, 0.036864206, 0.052006695, -0.012127453, 0.09172744, -0.024401352, 0.00055804895, 0.051356632, 0.029442336, 0.03809542, 0.02597004, 0.041670132, -0.062270734, -0.011368409, -0.012576063, -0.03698074, 0.009839966, -0.045337822, 0.05870352, 0.033290546, -0.050594345, -0.044886068, -0.030028092, 0.0058530727, -0.045806993, -0.010981992, 0.031628214, -0.041402172, 0.04832855, 0.03983364, -0.013752108, -0.044539135, -0.021285104, -0.03556671, -0.06450829, 0.018932167, -0.02555006, -0.055996984, -0.014418671, -0.020698572, 0.019601919, 0.009427488, -0.028449522, 0.0021392556, -0.03229634, -0.005165832, -0.008614465, 0.041214377, 0.029839285, -0.014981943, 0.018460285, 0.03404851, 0.014113858, -0.036980797, -0.0066104103, 0.0049010017, 0.04973148, -0.06142862, -0.031165898, 0.0035434035, 0.009992207, -0.025878413, 0.05033087, 0.0009936425, 0.018442972, -0.053241666, 0.0402076, 0.0248804, -0.041841622, -0.0177806, 0.054724373, -0.015492272, -0.014962928, -0.02686379, -0.0073865415, 0.021559983, -0.024101213, -0.05916495, 0.01080576, 0.008418514, 0.042107902, -0.0013388715, -0.06313596, -0.031054668, -0.03179516, 0.0014877389, -0.06255935, -0.01661388, 0.05051272, -0.012405402, 0.004229794, -0.021854766, 0.036344945, -0.047960225, 0.002222266, -0.028642587, -0.031325534, 0.019591471, -0.02333022, -0.10379695, 0.015371668, -0.06200831, 0.0052539, -0.074360244, -0.006505401, -0.0056282785, -0.014742974, -0.076802164, -0.01866696, -0.036128443, -0.05253247, 0.012119718, 0.113430604, -0.03223099, -0.053532638, -0.017162068, -0.009568473, -0.025736216, -0.00029952265, 0.029131657, 0.019048369, -0.030131107, 0.055102944, 0.050020706, 0.02983192, 0.0042614266, -0.019789213, -0.038440637, -0.052385524, -0.08126966, 0.020377425, -0.040012255, 0.0506996, -0.0045985673, -0.006126346, -0.012276003, -0.08846927, -0.017142545, 0.0369717, -0.05595845, -0.010731805, -0.013887107, 0.02145635, -0.08452706, -0.027960533, -0.029048339, -0.015482442, -0.025278794, -0.04406327, -0.09541114, 0.031128725, 0.032834798, 0.042798597, -0.034436025, 0.016523255, 0.03375637, 0.018324092, 0.03035548, 0.005327989, 0.016063139, -0.01688289, 0.018914133, -0.004273667, 0.030383509, 0.007943385, -0.036814116, 0.067135446, -0.019755734, -0.028869472, -0.0016804612, -0.0270228, -0.017619988, 0.07148691, -0.026644036, -0.008675029, -0.025276268, -0.01830182, -0.013574153, -0.009294205, 0.037007798, -0.0365356, 0.02648494, -0.04419843, -0.055679504, -0.06336667, 0.039424032, -0.016731892, 0.03178622, -0.018267961, 0.062318835, -0.021858875, 0.013065773, -0.05617554, 0.015525844, 0.019534891, -0.007633059, 0.04972514, 0.060298774, -0.042216312, -0.004112439, 0.05880199, 0.095022224, 0.015387662, -0.036824897, -0.012529881, 0.043667406, 0.00524824, 0.006979477, 0.027741665, 0.0013718036, 0.00702983, 0.010246507, 0.036210977, 0.014179126, 0.045547955, -0.03134554, -0.03318728, 0.0063156984, -0.011894195, -0.014123776, 0.060529973, -0.0029219054, -0.012837955, 0.014186973, 0.06581427, 0.0364917, -0.02963873, -0.020656142, -0.023456398, 0.014795164, 0.019952519, -0.023898887, 0.02032881, -0.047920376, 0.0027803115, -0.0028092903, -0.025219146, -0.0048819613, -0.023404885, -0.025993932, -0.035504475, -0.03397633, 0.0057572476, -0.019001346, -0.026069237, 0.043545958, -0.015891725, -0.019771133, 0.022796202, 0.022915136, -0.03069708, 0.032131974, -0.001341973, 0.0028729814, 0.03016201, 0.004018867, 0.024577731, -0.039430603, -0.028637478, -0.06286881, 0.028462471, 0.03967618, -0.015639652, 0.03370541, -0.0002989456, 0.010426792, -0.0055143037, -0.067545496, -0.024754716, 0.018963106, 0.02987536, -0.025823401, -0.006698665, -0.015726315, 0.020193214, -0.027385863, 0.02160321, -0.058645327, -0.017143497, -0.007026398, -0.0058464, -0.011019182, -0.029050477, 0.046837512, -0.024876386, 0.009836014, -0.027971564, 0.015865887, -0.055387538, -0.013149898, -0.061436784, -0.016213104, 0.041194692, -0.015012053, 0.018090913, -0.00430085, 0.015349722, 0.066751204, 0.023698544, -0.06822272, 0.018363006, -0.0068294574, -0.06369541, -0.00014043826, -0.07300947, 0.03755807, 0.049046043, 0.021409698, 0.09781499, -0.04955612, 0.011474332, -0.03444903, 0.032969415, -0.0039271186, -0.005271081, -0.039754637, -0.060785662, -0.035523683], "index": 4, "object": "embedding"}, {"embedding": [0.011217114, 0.01981707, -0.16310175, -0.02502433, 0.09215045, -0.022785362, 0.07846818, 0.019504242, -0.011453236, -0.021963177, -0.024801686, -0.03111672, 0.07661632, 0.014625615, 0.0119140325, 0.011912017, 0.022267336, -0.09699642, 0.0048778993, 0.036599953, -0.04173664, -0.025413647, 0.022215148, -0.039990034, 0.018370744, 0.008299157, -0.039084103, 0.0038648387, -0.052257977, 0.013414049, 0.04891797, -0.011298268, 0.02228962, -0.010800352, 0.0037415728, -0.005351922, 0.027376872, -0.014279294, -0.012269476, 0.013264227, 0.06115871, -0.03653588, -0.010670919, -0.055883184, -0.0024615477, 0.0074382205, 0.060136616, -0.037749104, 0.023424447, -0.052080892, 0.008087162, 0.0136990715, 0.030203389, -0.011035708, 0.07422632, 0.016880851, -0.040833674, 0.007039087, 0.015667895, -0.07030579, 0.10081672, 0.065671206, -0.05823894, 0.022181824, 0.0036195123, -0.0057554147, -0.039146062, 0.017278519, -0.0032486122, -0.0155446, 0.03653951, 0.020748388, 0.04488198, 0.07316857, -0.04428931, 0.010114302, -0.010191108, -0.054419015, -0.041530486, 0.047278285, 0.01989392, 0.037087698, 0.0646416, -0.032647997, 0.04882493, 0.0022314154, -0.045987897, -0.006978132, -0.07631544, 0.13441429, 0.01089734, 0.052968033, 0.04924918, 0.023811983, -0.03493315, 0.018391227, -0.027768953, 0.03430233, -0.03590824, 0.033850417, -0.0071449885, 0.008767889, -0.0004348313, -0.049860775, 0.01656304, 0.05208266, 0.0045104977, 0.02710821, 0.006058056, 0.017148267, -0.07326292, 0.013627833, -0.073883526, 0.0072147707, -0.04202916, -0.010079833, 0.0478317, -0.020559901, -0.04734033, 0.060165476, -0.0450664, -0.038496844, -0.015205119, 0.0038890166, 0.01754317, 0.013260896, -0.066931926, -0.029963486, -0.023438077, 0.013791344, 0.028952278, -0.03635278, -0.06498951, 0.031262778, 0.004169202, 0.10934746, -0.027724795, 0.017722527, -0.009306052, 0.011087429, -0.014582213, 0.029312212, -0.031494375, 0.023928525, 0.045877878, -0.03672275, -0.020019177, -0.0033579196, -0.023281854, 0.010023587, 0.007131684, 0.06266423, 0.0032270346, -0.010980678, 0.039253253, -0.054847937, 0.027000649, -0.0069104494, 0.049030803, 0.03780165, 0.06767441, 9.736201e-05, -0.007618248, 0.053354472, 0.0011289943, -0.06254571, -0.030056393, -0.010644647, 0.060349558, 0.020550935, -0.059731122, -0.016806876, 0.022785934, -0.018437758, 0.033008806, 0.028988445, 0.047918957, -0.043988448, -0.032508347, -0.04799997, 0.04522418, -0.030518547, 0.0049122153, 0.029506698, -0.027670035, -0.010220624, 0.024204938, -0.020736307, -0.029389147, -0.03817527, 0.03527593, 0.040583003, -0.057272527, -0.009607875, -0.06451775, -0.011818367, 0.06576357, -0.035633564, 0.03676114, 0.01627326, -0.01240089, -0.03751408, -0.06421372, 0.06590478, -0.01895375, 0.045739885, -0.010165577, 0.009966692, -0.03457475, 0.047240693, 0.0395516, 0.02739646, 0.011542488, 0.05067137, 0.023675218, 0.02310933, 0.032781076, -0.015646564, -0.02957112, 0.032479595, 0.03184722, -0.017166408, 0.006809976, 0.0036971862, 0.025268437, 0.0057946336, -0.06260043, -0.009367013, -0.049762357, 0.039294567, 0.011637471, -0.036590166, 0.033772763, -0.0032932356, 0.027805772, -0.00040609748, -0.025352784, 0.042176448, -0.0230789, 0.04110726, 0.023128124, 0.056555055, -0.022651246, -0.0021901291, -0.087876156, -0.008919336, -0.019605868, -0.01712686, 0.03575225, 0.021450164, -0.00965159, 0.028077865, 0.067598, -0.006224471, 0.012220246, -0.019006144, -0.05274554, 0.018946936, 0.018667007, -0.003979508, 0.032723263, 0.025010072, 0.02732168, -0.010854233, -0.037189554, -0.0023397892, -0.01429713, -0.021263009, -0.0046403566, -0.030694872, 0.012185186, 0.029225448, 0.017723825, 0.040969387, -0.020615289, 0.012001619, -0.016771903, 0.00084241404, -0.015205828, 0.01488307, -0.033906613, -0.07974364, -0.022256397, -0.0036014812, 0.032036338, 0.004637718, 0.034548834, -0.03609753, 0.02036011, 0.023266677, 0.027168589, 0.007836586, -0.0037905243, -0.004213092, -0.034565534, 0.017536163, 0.021319658, -0.00018759635, 0.029417364, -0.031089904, 0.027518185, 0.02462952, 0.0328106, 0.06540779, 0.005759728, -0.06291936, -0.009678032, 0.015328652, -0.008077711, 0.02468481, -0.035424974, -0.040591303, -0.01752838, 0.022981407, -0.02749549, 0.009456421, 0.010470574, 0.033210818, 0.042361733, -0.009776044, 0.021710813, -0.04634168, -0.023075491, -0.031466417, 0.01583395, 0.068181336, 0.028621526, 0.027028456, -0.019051468, 0.020139793, 0.04010043, 0.010815543, 0.011999334, -0.0153849665, -0.015004949, -0.013376309, -0.034392603, 0.0010173596, 0.051571347, -0.01215764, 0.06267594, -0.027803242, 0.046725165, -0.042872302, 0.007899447, -0.024157317, -0.0040799924, -0.01882455, 0.025565738, 0.017952308, -0.0004098469, 0.020090979, -0.020681491, -0.017243693, 0.06451583, -0.015860643, 0.029550435, 0.03813458, -0.018621711, -0.0055793277, 0.031563796, 0.011722078, 0.014187912, -0.0017361367, 0.0011607718, 0.010028484, 0.041971404, 0.011182943, 0.036090568, 0.018113315, -0.0006643034, -0.011169709, -0.023911528, 0.020464333, -0.046640985, -0.01660952, -0.053942837, -0.0002626494, -0.0042855577, 0.0027479446, 0.047643885, -0.016166367, -0.02890239, -0.026241316, -0.045873966, -0.0018670133, 0.03712808, -0.009293688, 0.011559012, 0.013216346, 0.02486992, -0.08430313, -0.05270264, 0.0026429442, 0.023866279, -0.035199773, 0.040110115, -0.016708924, -0.011423239, 0.028537452, -0.025521485, -0.057962954, 0.01985273, 0.010036446, 0.0013608284, -0.00010778587, -0.04586269, -0.022394234, 0.02264009, 7.424645e-05, 0.028676191, 0.036361046, -0.0038755373, -0.070319265, 0.070160285, 0.0107927965, 0.035280943, -0.0037263916, -0.057596467, -0.018115455, 0.019593667, 0.031457987, 0.036211923, 0.01615235, 0.05124184, 0.03989748, 0.025831508, 0.020184202, -0.022960663, -0.11433402, 0.050502345, -0.01286964, 0.048480447, -0.015754268, -0.022761997, 0.021397462, -0.00082321797, 0.005461529, 0.016685773, 0.050561696, 0.035318628, 0.021098854, -0.0027423373, -0.048206434, -0.022906318, 0.062285837, 0.038845006, -0.02744204, -0.05431563, -0.006480738, 0.02009312, 0.024582038, 0.025378952, 0.014648082, 0.048449207, -0.002087177, 0.044414382, 0.06615179, -0.010153398, 0.0024104707, 0.022319822, 0.02838253, -0.01518946, -0.0031330646, 0.021418683, -0.011342767, -0.018054876, -0.009212531, 0.008115192, 0.05330168, -0.06714987, -0.027112154, 0.05471907, 0.06312251, -0.00062054215, -0.065492004, -0.0128720915, -0.004910133, 0.0574445, 0.039548986, -0.012668606, -0.00527685, -0.01389952, -0.045085058, -0.002680298, 0.025898052, -0.0060598785, -0.04413569, 0.010137737, -0.0073588854, 0.041484993, 0.016158964, 0.030111205, 0.024120342, -0.029498294, -0.00020973885, -0.02866732, 0.005366337, 0.01960977, -0.030846512, 0.025230933, 0.038676944, -0.017352106, -0.04305376, 0.026334127, -0.02845886, 0.036068484, -0.068267375, -0.065060936, 0.021685371, -0.007856047, 0.0195853, 0.025972454, 0.028070923, 0.0042164493, -0.036786247, 0.0048695616, -0.009767158, -0.052713882, -0.031813562, 0.053748753, -0.03074192, -0.023455117, -0.04194958, -0.064011976, 0.0011672544, 0.0110393865, -0.06526476, 0.08058157, -0.012850439, 0.05959262, 0.048191883, -0.07276887, 0.024695957, -0.056143425, -0.0122880535, -0.07793297, -0.013684165, 0.027127063, 0.014632542, -0.028846018, 0.05220966, 0.023441175, 0.026606983, 0.01977498, -0.023900922, -0.0057974067, 0.061649065, 0.03307249, -0.0796105, 0.019019999, -0.0658778, 0.024744498, -0.036213547, 0.054832593, -0.018336909, -0.04059566, -0.07315394, -0.04353643, -0.008341303, -0.02316609, -0.018771261, 0.07161999, -0.022875914, -0.005433259, -0.011175851, -0.015093031, 0.016439017, 0.0067641228, -0.0070923846, 0.034059506, -0.07092666, -0.00074223033, -0.03192034, 0.02280378, 0.018551862, 0.034887444, 0.03211418, -0.054480374, -0.07410234, -0.004843294, -2.4271805e-05, 0.05770113, 0.054773357, -0.0039273035, 0.03411051, -0.08210332, -0.013388313, 0.0073445085, -0.039311036, 0.0033735523, -0.033153117, 0.026536845, -0.06287245, -0.024693504, -0.013783598, 0.023952352, -0.024473088, -0.05794165, -0.08537455, -0.0488518, -0.0027728067, 0.044182092, -0.06918944, -0.030436805, 0.066196874, -0.009378006, 0.01087796, 0.004636654, 0.028825993, 0.007871335, -0.02645112, -0.04237055, -0.01645061, 0.005440631, -0.031951167, 0.040751398, -0.00792174, 0.018549442, -0.028397389, -0.02922112, -0.09891522, 0.057228193, -0.048564266, 0.0052900612, -0.040423185, -0.012412274, -0.010653462, 0.028264353, 0.019628253, -0.015827116, -0.018106619, -0.06651952, -0.0051792376, -0.07282714, 0.007049979, 0.017151164, -0.011805519, 0.0082904445, 0.02878094, 0.034336705, -0.025444021, -0.017869292, 0.019491162, 0.029276611, -0.017069539, 0.044874024, 0.05549272, 0.00013208402, -0.012828586, 0.06803056, 0.07772295, -0.017851077, -0.030213121, -0.018483583, 0.025182584, -0.00047095082, -0.003946796, -0.031903338, -0.016781796, -0.018222349, -0.036853813, -0.015239553, 0.005962873, 0.02075078, 0.003553613, -0.03924861, 0.028336503, -0.019155696, -0.030732846, 0.04367977, 0.026854211, -0.06659056, 0.027301611, 0.02871066, -0.0068448004, -0.030601127, 0.04239808, -0.0067062867, 0.01831498, -0.000106649335, -0.009763603, 0.02922102, -0.0253699, -0.01744343, -0.064342916, 0.008279819, 0.0049752747, -0.028542228, -0.04472529, -0.02112139, -0.014418882, -0.021553727, 0.012757673, -0.0511095, 0.036259573, 0.023662586, -0.014424895, -0.009146468, 0.055772208, 0.006891409, 0.013692632, 0.02347773, 0.021407267, -0.01317754, 0.0063954787, -0.018553061, 0.017629163, 0.0145844845, -0.03987078, 0.0036745367, 0.01775105, -0.05016125, 0.031261064, -0.007715559, 0.03021067, -0.02742919, -0.06251898, -0.0058595636, 0.008840167, 0.03366495, -0.004309621, -0.047591828, -0.0072025945, 0.02115286, -0.06433073, -0.036616065, -0.0351058, 0.023328498, 0.0062079327, 0.0016836828, -0.026882505, -0.040434927, 0.076456554, 0.026641905, -0.015572988, -0.0128542995, -0.021184256, -0.06144674, 0.0030253427, -0.047926743, -0.00422422, -0.011573141, 0.027665375, -0.007995961, 0.007438694, 0.005796426, 0.07936244, 0.012627788, -0.040344168, -0.0051214653, 0.01187209, -0.014067132, -0.012937043, -0.04169191, -0.0023670795, 0.022345029, 0.05000672, 0.14041837, 0.018955002, -0.0002734966, -0.007865343, 0.0830576, 0.0068081757, -0.08021569, -0.033609748, -0.01782364, 0.00058204], "index": 5, "object": "embedding"}], "model": "nomic-embed-text:v1.5", "object": "list", "usage": {"prompt_tokens": 7342, "total_tokens": 7342}}, "input": {"input": [" techniques expand training datasets artificially to address data scarcity and\nimprove model performance. Advanced techniques often used in NLP include:\n Word Embeddings: Using word embeddings like Word2Vec and GloVe to replace words with\ntheir semantic equivalents, thereby generating new data instances [19, 20].\n Back Translation: Translating text to another language and then back to the original language\nto create paraphrased data. This technique helps in generating diverse training samples [21]. Tools\nlike Google Translate API5 are commonly used for this purpose.\n Adversarial Attacks: Generating augmented data through adversarial examples that slightly\nmodify the original text to create new training samples while preserving the original meaning [22].\nLibraries like TextAttack6 provide frameworks for such augmentations.\n NLP-AUG7: This library offers a variety of augmenters for character, word, sentence, audio, and\nspectrogram augmentation, enhancing dataset diversity.\n3.2.3\nSynthetic Data Generation using LLMs\nLarge Language Models (LLMs) can generate synthetic data through innovative techniques such as:\n Prompt Engineering: Crafting specific prompts to guide LLMs like GPT-3 in generating relevant\nand high-quality synthetic data [23].\n Multi-Step Generation: Employing iterative generation processes where LLMs generate initial\ndata that is refined through subsequent steps [24]. This method can produce high-quality synthetic\ndata for various tasks, including summarising and bias detection.\nIt is crucial to verify the accuracy and relevance of synthetic data generated by LLMs before using\nthem for fine-tuning processes [25].\n3.3\nChallenges in Data Preparation for Fine-Tuning LLMs\nKey challenges in data preparation include:\n1. Domain Relevance: Ensuring that the data is relevant to the specific domain for accurate model\nperformance. Mismatched domain data can lead to poor generalisation and inaccurate outputs\n[26].\n2. Data Diversity: Including diverse and well-balanced data to prevent model biases and improve\ngeneralisation. A lack of diversity can cause the model to perform poorly on underrepresented\nscenarios [27].\n3. Data Size: Managing and processing large datasets, with at least 1000 samples recommended for\neffective fine-tuning. However, large datasets pose challenges in terms of storage, computational\nrequirements, and processing time.\n4. Data Cleaning and Preprocessing: Removing noise, errors, and inconsistencies are critical for\nproviding clean inputs to the model. Poorly preprocessed data can degrade model performance\nsignificantly.\n4https://aws.amazon.com/sagemaker/groundtruth/\n5https://translate.google.com/?sl=auto&tl=en&op=translate\n6https://github.com/QData/TextAttack\n7https://github.com/makcedward/nlpaug\n20\n5. Data Annotation: Ensuring precise and consistent labelling is essential for tasks requiring la-\nbelled data. Inconsistent annotation can lead to unreliable model predictions.\n6. Handling Rare Cases: Adequately representing rare but important instances in the dataset to\nensure the model can generalise to less frequent but critical scenarios.\n7. Ethical Considerations: Scrutinising data for harmful or biased content to prevent unintended\nconsequences. Ethical data handling includes removing biases and ensuring privacy [28].\n3.4\nAvailable LLM Fine-Tuning Datasets\nFor a comprehensive list of datasets suitable for fine-tuning LLMs, refer to resources like LLMXplorer,\nwhich provides domain and task-specific datasets.\n3.5\nBest Practices\n3.5.1\nHigh-Quality Data Collection\nEnsuring high-quality, diverse, and representative data is critical. Leveraging curated sources and en-\nsuring comprehensive coverage across different scenarios enhances model robustness [29].\nTools like\nDataRobot Paxata8 and KNIME Analytics Platform9 offer robust data profiling and transforma-\ntion capabilities.\n3.5.2\nEffective Data Preprocessing\nProper data preprocessing is essential for model performance. Utilising libraries like spaCy, NLTK, and\nHuggingFace Transformers can streamline preprocessing tasks. Platforms like Trifacta Wrangler\nand RapidMiner automate data cleaning tasks, improving efficiency and ensuring consistency [30].\n3.5.3\nManaging Data Imbalance\nAddressing data imbalance is crucial.\nTechniques like over-sampling, under-sampling, and SMOTE\nhelp balance datasets. Libraries like imbalanced-learn and ensemble methods in scikit-learn provide\nrobust tools for managing imbalanced datasets [31].\n3.5.4\nAugmenting and Annotating Data\nData augmentation and annotation improve model robustness. Tools like NLP-AUG, TextAttack,\nand Snorkel offer sophisticated capabilities for creating diverse and well-labelled datasets [32, 33].\n3.5.5\nEthical Data Handling\nEnsuring ethical data handling involves thorough scrutiny for biases and privacy concerns. Implement-\ning privacy-preserving techniques and filtering harmful content is critical. Services like Amazon Sage-\nMaker Ground Truth ensure scalable and secure data annotation [34].\n3.5.6\nRegular Evaluation and Iteration\nContinuous evaluation and iteration of the data preparation pipeline help maintain data quality and\nrelevance. Leveraging feedback loops and performance metrics ensures ongoing improvements and adap-\ntation to new data requirements.\nBy integrating these best practices, researchers and practitioners can enhance the effectiveness of LLM\nfine-tuning, ensuring robust and reliable model performance.\n8https://www.datarobot.com/platform/preparation/\n9https://www.knime.com/\n21\nChapter 4\nStage 2: Model Initialisation\n4.1\nSteps Involved in Model Initialisation\nFigure 4.1: Sequential steps involved in Initialising a Large Language Model (LLM), illustrating the\nprocess from setting up the environment", " practices, researchers and practitioners can enhance the effectiveness of LLM\nfine-tuning, ensuring robust and reliable model performance.\n8https://www.datarobot.com/platform/preparation/\n9https://www.knime.com/\n21\nChapter 4\nStage 2: Model Initialisation\n4.1\nSteps Involved in Model Initialisation\nFigure 4.1: Sequential steps involved in Initialising a Large Language Model (LLM), illustrating the\nprocess from setting up the environment to executing tasks. Each step is critical for ensuring that the\nLLM is correctly configured and ready for operation. This includes installing necessary dependencies,\nimporting libraries, selecting and downloading the appropriate language model from a repository, and\nfinally, loading the model to perform specific tasks.\n1. Set Up the Environment: Configure your environment, such as setting up GPU/TPU usage if\navailable, which can significantly speed up model loading and inference.\n2. Install the Dependencies: Ensure that all necessary software and libraries are installed. This\ntypically includes package managers like pip and frameworks like PyTorch or TensorFlow.\n22\n3. Import the Libraries: Import the required libraries in your script or notebook. Common libraries\ninclude transformers from Hugging Face, torch for PyTorch, and other utility libraries.\n4. Choose the Language Model: Select the appropriate pre-trained language model based on your\ntask requirements. This could be models like BERT, GPT-3, or others available on platforms like\nHugging Faces Model Hub.\n5. Download the Model from the Repository: Use the chosen frameworks functions to download\nthe pre-trained model from an online repository. For instance, using transformers, you might use\nAutoModel.from pretrained(model name).\n6. Load the Model in the Memory: Load the model into memory, ready for inference or further\nfine-tuning. This step ensures the model weights are initialised and ready for use.\n7. Execute Tasks: Perform the desired tasks using the loaded model. This could involve making\npredictions, generating text, or fine-tuning the model on a new dataset.\n4.2\nTools and Libraries for Model Initialisation\nPython offers a wide range of libraries for Initialising large language models, providing access to both\nopen and closed-source models. Here are some notable libraries:\n1. Python Library: HuggingFace\nDescription: HuggingFace is renowned for its support of numerous pre-trained large language\nmodels, ranging from Phi-3 mini to Llama-3 70B. The transformers library, part of HuggingFace,\nenables users to access these models via classes such as AutoModelForCausalLM. This library\nsupports loading fine-tuned models as well as 4-bit quantised models. Additionally, the transformers\nlibrary includes the pipeline feature, making it easy to use pre-trained models for various tasks\n[35].\n2. Python Framework: PyTorch\nDescription: PyTorch offers comprehensive tools and libraries for Initialising and fine-tuning\nlarge language models. It provides a flexible and efficient platform for building and deploying deep\nlearning models. HuggingFaces transformers library bridges the gap between PyTorch and other\nframeworks, enhancing its usability for state-of-the-art language models [36].\n3. Python Framework: TensorFlow\nDescription: TensorFlow also provides extensive tools and libraries for Initialising and fine-tuning\nlarge language models. Similar to PyTorch, it benefits from the HuggingFace transformers library,\nwhich provides a versatile and user-friendly API and interface for working with the latest advance-\nments in large language models [37].\n23\n4.3\nChallenges in Model Initialisation\nChallenge\nDescription\nAlignment with the\nTarget Task\nIts essential that the pre-trained model closely aligns with your specific\ntask or domain. This initial alignment serves as a solid foundation for\nfurther fine-tuning efforts, leading to improved efficiency and results [38].\nUnderstanding the\nPre-trained Model\nBefore making a selection, its crucial to thoroughly comprehend the\narchitecture, capabilities, limitations, and the tasks the model was orig-\ninally trained on. Without this understanding, fine-tuning efforts may\nnot yield the desired outcomes [23].\nAvailability and\nCompatibility\nCareful consideration of a models documentation, license, maintenance,\nand update frequency is necessary to avoid potential issues and ensure\nsmooth integration into your application.\nModel Architecture\nNot all models excel at every task.\nEach model architecture has its\nstrengths and weaknesses, so selecting one aligned with your specific\ntask is essential for favourable outcomes [39].\nResource Constraints\nLoading pre-trained LLMs is resource-heavy and requires more compu-\ntation. These models need high-performance CPUs and GPUs and a\nsignificant amount of disk space. For instance, the Llama 3 8B model\nrequires a minimum of 16GB of memory to load and run the inference.\nPrivacy\nPrivacy and confidentiality are crucial factors when selecting a large lan-\nguage model (LLM). Many businesses prefer not to share their data\nwith external LLM providers.\nIn such instances, hosting an LLM on\nlocal servers or using pre-trained LLMs available through private cloud\nproviders can be viable solutions. These approaches ensure that data\nremains within the companys premises, thereby preserving privacy and\nconfidentiality.\nCost and Maintenance\nHosting LLMs on local servers entails significant time and expense for\nsetup and ongoing maintenance. Conversely, utilising cloud vendors al-\nleviates concerns about resource maintenance but incurs monthly billing\ncosts. These charges are typically based on factors such as model size\nand the volume of requests per minute.\nModel Size and\nQuantisation\nutilising a pre-trained model with high memory consumption can still be\nviable by employing its quantised version. Through quantisation, pre-\ntrained weights can", " LLMs on local servers entails significant time and expense for\nsetup and ongoing maintenance. Conversely, utilising cloud vendors al-\nleviates concerns about resource maintenance but incurs monthly billing\ncosts. These charges are typically based on factors such as model size\nand the volume of requests per minute.\nModel Size and\nQuantisation\nutilising a pre-trained model with high memory consumption can still be\nviable by employing its quantised version. Through quantisation, pre-\ntrained weights can be loaded with reduced precision, typically 4-bit or\n8-bit floating point, substantially diminishing parameter volume while\nmaintaining considerable accuracy [40].\nPre-training Datasets\nExamine the datasets used for pre-training to gauge the models under-\nstanding of language. These are important as there are models available\nspecifically for performing code generation, and we do not want to use\nthose models for finance text classification [41].\nBias Awareness\nBe vigilant regarding potential biases in pre-trained models, especially if\nunbiased predictions are required. The bias awareness can be evaluated\nby testing different models and backtracking the datasets used for pre-\ntraining [42].\nTable 4.1: Comprehensive Overview of Challenges in Initialising a Large Language Model (LLM). This\ntable highlights critical considerations, such as the importance of aligning pre-trained models with specific\ntasks, understanding model architecture and compatibility, managing resource constraints, and ensuring\ndata privacy. Additionally, it discusses the challenges related to cost, maintenance, and the complexities\nof model size, quantisation, and bias awareness. Each challenge is associated with specific references to\nensure thorough understanding and proper model deployment.\n4.4\nTutorials\n1. Summarisation using Llama 3\n24\n2. HuggingFace tutorial for getting started with LLMs\n3. PyTorch tutorial for fine-tuning models\n4. TensorFlow tutorial for transformer models\n25\nChapter 5\nStage 3: Training Setup\n5.1\nSteps Involved in Training Setup\n1. Setting up the training environment: When setting up the environment for training an LLM,\nit is crucial to configure high-performance hardware, such as GPUs or TPUs, and ensure proper\ninstallation of necessary software components like CUDA, cuDNN, and deep learning frameworks\nsuch as PyTorch or TensorFlow. Verify hardware recognition and compatibility with the software to\nleverage computational power effectively, reducing training time and improving model performance.\n2. Defining the Hyper-parameters: When defining hyperparameters for fine-tuning an LLM, it is\nessential to carefully tune key parameters such as learning rate, batch size, and epochs to optimise\nthe models performance.\n3. Initialising Optimisers and Loss Functions: When initialising optimisers and loss functions\nfor fine-tuning an LLM, it is crucial to select the appropriate optimiser to efficiently update the\nmodels weights and the correct loss function to measure model performance [43].\n5.2\nSetting up Training Environment\nWhen fine-tuning a large language model (LLM), the computational environment plays a crucial role in\nensuring efficient training. To achieve optimal performance, its essential to configure the environment\nwith high-performance hardware such as GPUs (Graphics Processing Units) or TPUs (Tensor Processing\nUnits). GPUs, such as the NVIDIA A100 or V100, are widely used for training deep learning models\ndue to their parallel processing capabilities. For larger-scale operations, TPUs offered by Google Cloud\ncan provide even greater acceleration [44].\nFirst, ensure that your system or cloud environment has the necessary hardware installed. For GPUs,\nthis involves setting up CUDA1 (Compute Unified Device Architecture) and cuDNN2 (CUDA Deep Neu-\nral Network library) from NVIDIA, which are essential for enabling GPU acceleration. For TPU usage,\nyou would typically set up a Google Cloud environment with TPU instances, which includes configuring\nthe TPU runtime in your training scripts.\nVerify that your hardware is correctly recognised and utilised by your deep learning frameworks. In\nPyTorch, for instance, you can check GPU availability with torch.cuda.is available(). Properly setting\nup and testing the hardware ensures that the training process can leverage the computational power\neffectively, reducing training time and improving model performance [36].\nWhen fine-tuning an LLM, both software and hardware considerations are paramount to ensure a smooth\nand efficient training process. On the software side, you need a compatible deep learning framework like\nPyTorch or TensorFlow. These frameworks have extensive support for LLMs and provide utilities for\nefficient model training and evaluation. Installing the latest versions of these frameworks, along with\nany necessary dependencies, is crucial for leveraging the latest features and performance improvements\n1https://developer.nvidia.com/cuda-toolkit\n2https://developer.nvidia.com/cudnn\n26\n[45].\nAdditionally, use libraries like Hugging Faces transformers to simplify the process of loading pre-trained\nmodels and tokenizers. This library is particularly well-suited for working with various LLMs and offers\na user-friendly interface for model fine-tuning. Ensure that all software components, including libraries\nand dependencies, are compatible with your chosen framework and hardware setup [35].\nOn the hardware side, consider the memory requirements of the model and your dataset. LLMs typ-\nically require substantial GPU memory, so opting for GPUs with higher VRAM (e.g., 16GB or more)\ncan be beneficial. If your model is exceptionally large or if you are training with very large datasets,\ndistributed training across multiple GPUs or TPUs might be necessary. This requires a careful setup of\ndata parallelism or model parallelism techniques to efficiently utilise the available hardware [46].\nLastly, ensure robust cooling and power supply for your hardware, as training LLMs can be resource-\nintensive, generating", ", so opting for GPUs with higher VRAM (e.g., 16GB or more)\ncan be beneficial. If your model is exceptionally large or if you are training with very large datasets,\ndistributed training across multiple GPUs or TPUs might be necessary. This requires a careful setup of\ndata parallelism or model parallelism techniques to efficiently utilise the available hardware [46].\nLastly, ensure robust cooling and power supply for your hardware, as training LLMs can be resource-\nintensive, generating significant heat and requiring consistent power. Proper hardware setup not only\nenhances training performance but also prolongs the lifespan of your equipment [47].\n5.3\nDefining Hyperparameters\nKey hyperparameters like learning rate, batch size, epochs are crucial for enhancing the models perfor-\nmance and obtaining superior outcomes. This process entails adjusting hyperparameters and training\nsettings to align with your particular use case. Below are the key hyperparameters:\n1. Learning Rate: Fine-tuning an LLM involves using optimisation algorithms like stochastic gradi-\nent descent (SGD). This technique estimates the error gradient for the models current state using\nsamples from the training dataset and subsequently updates the models weights via the backprop-\nagation of errors algorithm. The learning rate dictates the speed at which the model adapts to the\nproblem. Smaller learning rates necessitate more training due to the minimal weight adjustments\nper update, while larger learning rates lead to quicker changes to weights [48].\n2. Batch Size: A batch refers to a subset of the training data used to update a models weights\nduring the training process. Batch training involves dividing the entire training set into smaller\ngroups, updating the model after processing each batch. The batch size is a hyperparameter that\ndetermines the number of samples processed before the model parameters are updated.\n3. Epochs: Epoch refers to a full pass through the entire training dataset. This involves a complete\nforward and backward pass through the dataset. The dataset can be processed as a single batch\nor divided into multiple smaller batches. An epoch is considered complete once the model has\nprocessed all batches and updated its parameters based on the calculated loss.\n5.3.1\nMethods for Hyperparameter Tuning\nLLM hyperparameter tuning involves adjusting various hyperparameters during the training process\nto identify the optimal combination that yields the best output. This process often entails significant\ntrial and error, meticulously tracking each hyperparameter adjustment, and recording the resulting\nperformance. Conducting this manually can be highly time-consuming. To address this, automated\nhyperparameter tuning methods have been developed to streamline the process. The three most common\nmethods of automated hyperparameter tuning are random search, grid search, and Bayesian optimisation:\n1. Random Search: This method randomly selects and evaluates combinations of hyperparameters\nfrom a specified range. It is a straightforward and efficient approach capable of exploring a large\nparameter space. However, it may not always find the optimal combination of hyperparameters\nand can be computationally expensive [49].\n2. Grid Search: Unlike random search, grid search exhaustively evaluates every possible combination\nof hyperparameters from a given range.\nAlthough resource-intensive, this systematic approach\nensures that the optimal set of hyperparameters is found [50].\n27\n3. Bayesian Optimisation: This method uses a probabilistic model to predict the performance of\ndifferent hyperparameters and selects the best ones accordingly. It is an efficient method that can\nhandle large parameter spaces better and is less resource-intensive than grid search. However, it is\nmore complex to set up and may be less reliable in identifying the optimal set of hyperparameters\ncompared to grid search.\n4. Automated hyperparameter tuning: This facilitates the development of multiple language\nmodels, each with a unique combination of hyperparameters. By training these models on the same\ndataset, it becomes possible to compare their outputs and determine which configuration is best\nsuited for the desired use case. Additionally, models tuned with different sets of hyperparameters\ncan be tailored to various specific applications.\n5.4\nInitialising Optimisers and Loss Functions\nChoosing the right optimiser and loss function is crucial for training and fine-tuning LLMs.\nBelow\nare descriptions of some commonly used optimisation algorithms, their advantages, disadvantages, and\nappropriate use cases:\n5.4.1\nGradient Descent\nGradient Descent is a fundamental optimisation algorithm used to minimise cost functions in machine\nlearning models. It aims to find the optimal parameters for a neural network.\nHow it Works: Gradient Descent iteratively updates model parameters in the direction of the\nnegative gradient of the cost function. It calculates gradients for each parameter and applies updates\nacross all data points until convergence. This method utilises the entire dataset to calculate gradients,\noften requiring a fixed learning rate and being sensitive to the scale of data and learning rate choice.\nPros:\n Simple and easy to implement.\n Intuitive and easy to understand.\n Converges to the global minimum for convex functions.\n Suitable for small-scale problems.\nCons:\n Computationally expensive on large datasets.\n May get stuck in local minima.\n Requires a large number of iterations.\n Sensitive to the choice of learning rate.\nWhen to Use: Gradient Descent is best used for small datasets where gradient computation is\ncheap and simplicity and clarity are preferred.\n5.4.2\nStochastic Gradient Descent (SGD)\nStochastic Gradient Descent (SGD) is a variant of Gradient Descent that focuses on reducing computation\nper iteration.\nHow it Works: SGD updates parameters using a single or few data points at each iteration, intro-\nducing randomness in updates. It reduces the computational burden per iteration and often converges\nfaster than batch Gradient Descent. However, it requires a smaller learning rate due to higher variance\nand benefits from momentum to", "2\nStochastic Gradient Descent (SGD)\nStochastic Gradient Descent (SGD) is a variant of Gradient Descent that focuses on reducing computation\nper iteration.\nHow it Works: SGD updates parameters using a single or few data points at each iteration, intro-\nducing randomness in updates. It reduces the computational burden per iteration and often converges\nfaster than batch Gradient Descent. However, it requires a smaller learning rate due to higher variance\nand benefits from momentum to stabilise updates.\nPros:\n Fast and handles large datasets well.\n Efficient memory usage.\n28\n Simple and easy to implement.\n Can escape local minima due to noise.\nCons:\n High variance in updates can lead to instability.\n Can overshoot the minimum.\n Sensitive to the choice of learning rate.\n Can be slower to converge compared to batch methods.\nWhen to Use: SGD is ideal for large datasets, incremental learning scenarios, and real-time learning\nenvironments where computational resources are limited.\n5.4.3\nMini-batch Gradient Descent\nMini-batch Gradient Descent combines the efficiency of SGD and the stability of batch Gradient Descent,\noffering a compromise between batch and stochastic approaches.\nHow it Works: It splits data into small batches and updates parameters using gradients averaged\nover each mini-batch. This reduces variance compared to SGD and is more efficient than batch Gradient\nDescent, helping in generalising the updates.\nPros:\n Balances between efficiency and stability.\n More generalisable updates.\n Reduces the variance of parameter updates.\n Provides a compromise between SGD and batch.\nCons:\n Requires tuning of batch size.\n Can still be computationally expensive for very large datasets.\n More complex implementation.\n Can require more iterations than full-batch Gradient Descent.\nWhen to Use: Mini-batch Gradient Descent is suitable for most deep learning tasks, especially\nwhen working with moderate to large datasets.\n5.4.4\nAdaGrad\nAdaptive Gradient Algorithm (AdaGrad) is designed for sparse data and high-dimensional models, ad-\njusting learning rates to improve performance on sparse data.\nHow it Works: AdaGrad adapts the learning rate for each parameter based on historical gradi-\nent information, accumulating squared gradients. This approach prevents large updates for frequent\nparameters and helps in dealing with sparse features.\nPros:\n Adapts learning rate for each parameter.\n Good for sparse data.\n No need to manually tune learning rates.\n Works well with high-dimensional data.\nCons:\n Learning rate can diminish to zero, stopping learning.\n29\n May require more tuning for convergence.\n Accumulation of squared gradients can lead to overly small learning rates.\n Can slow down significantly.\nWhen to Use: AdaGrad is useful for sparse datasets like text and images where learning rates need\nto adapt to feature frequency.\n5.4.5\nRMSprop\nRoot Mean Square Propagation (RMSprop) is an adaptive learning rate method designed to perform\nbetter on non-stationary and online problems.\nHow it Works: RMSprop modifies AdaGrad by using a moving average of squared gradients to\nadapt learning rates based on recent gradient magnitudes. It maintains a running average of squared\ngradients to help in maintaining steady learning rates.\nPros:\n Addresses the diminishing learning rate problem of AdaGrad.\n Adapts learning rate based on recent gradients.\n Effective for recurrent neural networks.\n More robust against non-stationary targets.\nCons:\n Can still get stuck in local minima on non-convex problems.\n Requires hyperparameter tuning.\n Requires careful tuning of the decay rate.\n Can be sensitive to the initial learning rate.\nWhen to Use: RMSprop is best for non-convex optimisation problems, training RNNs and LSTMs,\nand dealing with noisy or non-stationary objectives.\n5.4.6\nAdaDelta\nAdaptive Delta (AdaDelta) improves on AdaGrad and RMSprop, focusing on adaptive learning rates\nwithout diminishing too quickly.\nHow it Works: AdaDelta eliminates the need for a default learning rate by using a moving window\nof gradient updates. It adapts learning rates based on recent gradient magnitudes to ensure consistent\nupdates even with sparse gradients.\nPros:\n Eliminates the need to set a default learning rate.\n Addresses the diminishing learning rate issue.\n Does not require manual tuning of the learning rate.\n Handles gradient sparsity well.\nCons:\n More complex than RMSprop and AdaGrad.\n Can have slower convergence initially.\n Can require more iterations to converge.\n Implementation can be more complex.\nWhen to Use: AdaDelta is suitable for scenarios similar to RMSprop but is preferred when avoiding\nmanual learning rate setting.\n30\n5.4.7\nAdam\nAdaptive Moment Estimation (Adam) combines the advantages of AdaGrad and RMSprop, making it\nsuitable for problems with large datasets and high-dimensional spaces.\nHow it Works: Adam uses running averages of both gradients and their squared values to com-\npute adaptive learning rates for each parameter. It includes bias correction and often achieves faster\nconvergence than other methods.\nPros:\n Combines advantages of AdaGrad and RMSprop.\n Adaptive learning rates.\n Includes bias correction.\n Fast convergence.\n Works well with large datasets and high-dimensional spaces.\nCons:\n Requires tuning of hyperparameters (though it often works well with defaults).\n Computationally intensive.\n Can lead to overfitting if not regularised properly.\n Requires more memory.\nWhen to Use: Adam is widely used in most deep learning applications due to its efficiency and\neffectiveness, particularly in complex neural network architectures.\n5.4.8\nAdamW\nAdamW is an extension of Adam that includes weight decay regularisation to address overfitting issues\npresent in Adam.\nHow it Works", " hyperparameters (though it often works well with defaults).\n Computationally intensive.\n Can lead to overfitting if not regularised properly.\n Requires more memory.\nWhen to Use: Adam is widely used in most deep learning applications due to its efficiency and\neffectiveness, particularly in complex neural network architectures.\n5.4.8\nAdamW\nAdamW is an extension of Adam that includes weight decay regularisation to address overfitting issues\npresent in Adam.\nHow it Works: AdamW integrates L2 regularisation directly into the parameter updates, decoupling\nweight decay from the learning rate. This improves generalisation and is suitable for fine-tuning large\nmodels.\nPros:\n Includes weight decay for better regularisation.\n Combines Adams adaptive learning rate with L2 regularisation.\n Improves generalisation.\n Reduces overfitting compared to Adam.\nCons:\n Slightly more complex than Adam.\n Requires careful tuning of the weight decay parameter.\n Slightly slower than Adam due to additional computations.\n Requires more memory.\nWhen to Use: AdamW is ideal for scenarios where regularisation is needed, such as preventing\noverfitting in large models and fine-tuning pre-trained models.\nA comprehensive collection of optimisation algorithms implemented within the PyTorch library can be\nfound in here. The Hugging Face Transformers package also offers a variety of optimisers for initialising\nand fine-tuning language models, available here.\n31\n5.5\nChallenges in Training Setup\n1. Ensuring compatibility and proper configuration of high-performance hardware like GPUs or TPUs\ncan be complex and time-consuming.\n2. Managing dependencies and versions of deep learning frameworks and libraries to avoid conflicts\nand leverage the latest features.\n3. Selecting an appropriate learning rate is critical, as too high a rate can cause suboptimal conver-\ngence, while too low a rate can make the training process excessively slow.\n4. Determining the optimal batch size that balances memory constraints and training efficiency, es-\npecially given the large memory requirements of LLMs.\n5. Choosing the right number of epochs to avoid underfitting or overfitting the model, requiring careful\nmonitoring and validation.\n6. Selecting the most suitable optimiser for the specific training task to efficiently update the models\nweights.\n7. Choosing the correct loss function to accurately measure model performance and guide the opti-\nmisation process.\n5.6\nBest Practices\n Optimal Learning Rate: Use a lower learning rate, typically between 1e-4 to 2e-4, to ensure\nstable convergence. A learning rate schedule, such as learning rate warm-up followed by a linear\ndecay, can also be beneficial. This helps in initially stabilising the training and then allowing the\nmodel to converge more accurately.\n Batch Size Considerations: Opt for a batch size that balances memory constraints and training\nefficiency.\nSmaller batch sizes can help in achieving faster convergence but may require more\nfrequent updates. Conversely, larger batch sizes can be more memory-intensive but may lead to\nmore stable updates. Experiment with different batch sizes to find the optimal balance for your\nspecific use case.\n Save Checkpoints Regularly: Regularly save model weights at various intervals across 5-8\nepochs to capture optimal performance without overfitting. Implement early stopping mechanisms\nto halt training once the model performance starts to degrade on the validation set, thereby pre-\nventing overfitting [51].\n Hyperparameter Tuning: Utilise hyperparameter tuning methods like grid search, random\nsearch, and Bayesian optimisation to find the optimal set of hyperparameters.\nTools such as\nOptuna, Hyperopt, and Ray Tune can automate this process and help in efficiently exploring the\nhyperparameter space [49].\n Data Parallelism and Model Parallelism: For large-scale training, consider using data paral-\nlelism or model parallelism techniques to distribute the training workload across multiple GPUs or\nTPUs. Libraries like Horovod and DeepSpeed can facilitate efficient distributed training, helping\nto reduce training time and manage memory usage effectively [52, 53].\n Regular Monitoring and Logging: Implement robust monitoring and logging to track training\nmetrics, resource usage, and potential bottlenecks. Tools like TensorBoard, Weights & Biases, and\nMLflow can provide real-time insights into the training process, allowing for timely interventions\nand adjustments.\n Handling Overfitting and Underfitting: Ensure that your model generalises well by imple-\nmenting techniques to handle overfitting and underfitting. regularisation techniques such as L2\nregularisation, dropout, and data augmentation can help prevent overfitting. Conversely, if your\nmodel is underfitting, consider increasing the model complexity or training for more epochs.\n32\n Use Mixed Precision Training: Mixed precision training involves using both 16-bit and 32-bit\nfloating-point types to reduce memory usage and increase computational efficiency. This technique\ncan significantly speed up training and reduce the required memory footprint, especially when\nusing large models. NVIDIAs Apex and TensorFlows mixed precision API provide support for\nimplementing mixed precision training [54].\n Evaluate and Iterate: Continuously evaluate the model performance using a separate validation\nset and iterate on the training process based on the results. Regularly update your training data\nand retrain the model to keep it current with new data trends and patterns.\n Documentation and Reproducibility: Maintain thorough documentation of your training\nsetup, including the hardware configuration, software environment, and hyperparameters used.\nEnsure reproducibility by setting random seeds and providing detailed records of the training\nprocess. This practice not only aids in debugging and further development but also facilitates\ncollaboration and sharing of results with the broader research community.\n33\nChapter 6\nStage "], "parameters": {"model": "nomic-embed-text:v1.5"}}, "key": "embeddings_ee4faf580c1f24c487ac75d749c6fd92abb31890657685dc9698e4bcd40e4584_v2"}