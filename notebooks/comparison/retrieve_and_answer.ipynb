{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23a9e741",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b18802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed working dir to C:\\Users\\tiago\\Documents\\Granter ai Internship\\Implementation\\Code\\KGs_for_Vertical_AI\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys \n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../..')))\n",
    "\n",
    "from app_settings import PROJECT_ROOT\n",
    "from src.utils import read_test_queries\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print(f\"Changed working dir to {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dda2032b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"What is Granter.ai's main business activity?\",\n",
       " 'Which AI methodologies/technological features does Granter.ai use in its solution?',\n",
       " \"What is the ultimate goal of Granter.ai's project?\",\n",
       " \"What is Granter.ai's mission?\",\n",
       " \"What is the business model for Granter.ai's solution?\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = read_test_queries(os.path.join(PROJECT_ROOT, 'data', 'test_queries.txt'))\n",
    "questions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5be03bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from app_settings import (\n",
    "    AZURE_OPENAI_API_KEY,\n",
    "    AZURE_OPENAI_ENDPOINT,\n",
    "    AZURE_OPENAI_API_VERSION,\n",
    "    AZURE_DEPLOYMENT_GPT41_NANO\n",
    ")\n",
    "\n",
    "gpt41_nano = AzureChatOpenAI(\n",
    "    azure_deployment=AZURE_DEPLOYMENT_GPT41_NANO,\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50002859",
   "metadata": {},
   "source": [
    "# Vector RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "680540d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rag.rag_tests import generate_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6a22fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 1...\n",
      "Generating answer for question 2...\n",
      "Generating answer for question 3...\n",
      "Generating answer for question 4...\n",
      "Generating answer for question 5...\n",
      "Generating answer for question 6...\n",
      "Generating answer for question 7...\n",
      "Generating answer for question 8...\n",
      "Generating answer for question 9...\n",
      "Generating answer for question 10...\n",
      "Generating answer for question 11...\n",
      "Generating answer for question 12...\n",
      "Generating answer for question 13...\n",
      "Generating answer for question 14...\n",
      "Generating answer for question 15...\n",
      "Generating answer for question 16...\n",
      "Generating answer for question 17...\n",
      "Generating answer for question 18...\n",
      "Generating answer for question 19...\n",
      "Generating answer for question 20...\n"
     ]
    }
   ],
   "source": [
    "results_dir = \"results/answers/vector_rag\"\n",
    "os.makedirs(results_dir, exist_ok=True) # create/check results folder\n",
    "output_file = os.path.join(results_dir, \"rag_results.txt\")\n",
    "\n",
    "for i,question in enumerate(questions):\n",
    "    print(f'Generating answer for question {i+1}...')\n",
    "    answer = generate_response(question, k=3)\n",
    "\n",
    "    with open(output_file, 'a', encoding='utf-8') as f:\n",
    "        f.write(f\"Question {i+1}:\\n{question}\\n\")\n",
    "        f.write(f\"Answer:\\n{answer}\\n\")\n",
    "        f.write(\"\\n\" + \"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2393c800",
   "metadata": {},
   "source": [
    "# Graph RAG - rdb ontology KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab6547eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.g_retriever import retrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e898aa96",
   "metadata": {},
   "source": [
    "## Without chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfbfbbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 11...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 13...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 14...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 17...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 18...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 19...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n"
     ]
    }
   ],
   "source": [
    "results_dir = \"results/answers/rdb_ontology\"\n",
    "os.makedirs(results_dir, exist_ok=True) # create/check results folder\n",
    "output_file = os.path.join(results_dir, \"rdb_results.txt\")\n",
    "\n",
    "for i,question in enumerate(questions):\n",
    "    print(f'Generating answer for question {i+1}...')\n",
    "\n",
    "    retrieved = retrieve(\n",
    "        graph_csv='results/KGs/rdb_ontology_kg_nochunks.csv',\n",
    "        question=question,\n",
    "        embed_model='paraphrase-multilingual-MiniLM-L12-v2'\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the user question\n",
    "        - Include all information that is relevant to the question\n",
    "        - Keep your response clear and concise. Aim for 3 sentences or less, but you can include more if needed to cover all the relevant information\n",
    "        - If the answer is not in the context, say \"I don't know\"\n",
    "        - Do not add information that isn't supported by the context\n",
    "    Context:\n",
    "    {retrieved}\n",
    "\n",
    "    Question: {question}\"\"\"\n",
    "\n",
    "    answer = gpt41_nano.invoke(prompt).content\n",
    "\n",
    "    with open(output_file, 'a', encoding='utf-8') as f:\n",
    "        f.write(f\"Question {i+1}:\\n{question}\\n\")\n",
    "        f.write(f\"Answer:\\n{answer}\\n\")\n",
    "        f.write(\"\\n\" + \"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceea123f",
   "metadata": {},
   "source": [
    "## With Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73f82f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:07<00:00,  7.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:07<00:00,  7.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:07<00:00,  7.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:07<00:00,  7.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:08<00:00,  8.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:08<00:00,  8.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 11...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 13...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 14...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 17...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 18...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:04<00:00,  5.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 19...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:07<00:00,  7.04s/it]\n"
     ]
    }
   ],
   "source": [
    "results_dir = \"results/answers/rdb_ontology\"\n",
    "os.makedirs(results_dir, exist_ok=True) # create/check results folder\n",
    "output_file = os.path.join(results_dir, \"rdb_chunks_results.txt\")\n",
    "\n",
    "for i,question in enumerate(questions):\n",
    "    print(f'Generating answer for question {i+1}...')\n",
    "\n",
    "    retrieved = retrieve(\n",
    "        graph_csv='results/KGs/rdb_ontology_kg_chunks.csv',\n",
    "        question=question,\n",
    "        embed_model='paraphrase-multilingual-MiniLM-L12-v2'\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the user question\n",
    "        - Include all information that is relevant to the question\n",
    "        - Keep your response clear and concise. Aim for 3 sentences or less, but you can include more if needed to cover all the relevant information\n",
    "        - If the answer is not in the context, say \"I don't know\"\n",
    "        - Do not add information that isn't supported by the context\n",
    "    Context:\n",
    "    {retrieved}\n",
    "\n",
    "    Question: {question}\"\"\"\n",
    "\n",
    "    answer = gpt41_nano.invoke(prompt).content\n",
    "\n",
    "    with open(output_file, 'a', encoding='utf-8') as f:\n",
    "        f.write(f\"Question {i+1}:\\n{question}\\n\")\n",
    "        f.write(f\"Answer:\\n{answer}\\n\")\n",
    "        f.write(\"\\n\" + \"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deded7c",
   "metadata": {},
   "source": [
    "# Graph RAG - txt ontology KG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0055b9",
   "metadata": {},
   "source": [
    "## Without Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd96bd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 11...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 13...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 14...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 17...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:04<00:00,  4.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 18...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:04<00:00,  4.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 19...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:04<00:00,  4.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.02s/it]\n"
     ]
    }
   ],
   "source": [
    "results_dir = \"results/answers/txt_ontology\"\n",
    "os.makedirs(results_dir, exist_ok=True) # create/check results folder\n",
    "output_file = os.path.join(results_dir, \"txt_results.txt\")\n",
    "\n",
    "for i,question in enumerate(questions):\n",
    "    print(f'Generating answer for question {i+1}...')\n",
    "\n",
    "    retrieved = retrieve(\n",
    "        graph_csv='results/KGs/txt_ontology_kg_nochunks.csv',\n",
    "        question=question,\n",
    "        embed_model='paraphrase-multilingual-MiniLM-L12-v2'\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the user question\n",
    "        - Include all information that is relevant to the question\n",
    "        - Keep your response clear and concise. Aim for 3 sentences or less, but you can include more if needed to cover all the relevant information\n",
    "        - If the answer is not in the context, say \"I don't know\"\n",
    "        - Do not add information that isn't supported by the context\n",
    "    Context:\n",
    "    {retrieved}\n",
    "\n",
    "    Question: {question}\"\"\"\n",
    "\n",
    "    answer = gpt41_nano.invoke(prompt).content\n",
    "\n",
    "    with open(output_file, 'a', encoding='utf-8') as f:\n",
    "        f.write(f\"Question {i+1}:\\n{question}\\n\")\n",
    "        f.write(f\"Answer:\\n{answer}\\n\")\n",
    "        f.write(\"\\n\" + \"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ffa88b",
   "metadata": {},
   "source": [
    "## With Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef23d932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:05<00:00,  2.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:07<00:00,  3.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:07<00:00,  3.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:07<00:00,  3.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:07<00:00,  3.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:07<00:00,  3.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:06<00:00,  3.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:06<00:00,  3.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:09<00:00,  4.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:45<00:00, 22.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 11...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:12<00:00,  6.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:09<00:00,  4.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 13...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:06<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 14...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:07<00:00,  3.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:07<00:00,  3.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:07<00:00,  3.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 17...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:08<00:00,  4.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 18...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:07<00:00,  3.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 19...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:07<00:00,  3.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for question 20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:07<00:00,  3.69s/it]\n"
     ]
    }
   ],
   "source": [
    "results_dir = \"results/answers/txt_ontology\"\n",
    "os.makedirs(results_dir, exist_ok=True) # create/check results folder\n",
    "output_file = os.path.join(results_dir, \"txt_chunks_results.txt\")\n",
    "\n",
    "for i,question in enumerate(questions):\n",
    "    print(f'Generating answer for question {i+1}...')\n",
    "\n",
    "    retrieved = retrieve(\n",
    "        graph_csv='results/KGs/txt_ontology_kg_chunks.csv',\n",
    "        question=question,\n",
    "        embed_model='paraphrase-multilingual-MiniLM-L12-v2'\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the user question\n",
    "        - Include all information that is relevant to the question\n",
    "        - Keep your response clear and concise. Aim for 3 sentences or less, but you can include more if needed to cover all the relevant information\n",
    "        - If the answer is not in the context, say \"I don't know\"\n",
    "        - Do not add information that isn't supported by the context\n",
    "    Context:\n",
    "    {retrieved}\n",
    "\n",
    "    Question: {question}\"\"\"\n",
    "\n",
    "    answer = gpt41_nano.invoke(prompt).content\n",
    "\n",
    "    with open(output_file, 'a', encoding='utf-8') as f:\n",
    "        f.write(f\"Question {i+1}:\\n{question}\\n\")\n",
    "        f.write(f\"Answer:\\n{answer}\\n\")\n",
    "        f.write(\"\\n\" + \"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26362c18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
